{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0adcfd11-92f1-4a9c-8d74-f8fb1fab704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87aed591-3657-4060-a9cf-054f4899c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directories for train and validation sets\n",
    "root_dir = '/projectnb/ds340/projects/Samuolis_Parikh_Image_Data/'\n",
    "\n",
    "train_dir = root_dir +\"resized_images/train\"\n",
    "validation_dir = root_dir + \"resized_images/validation\"\n",
    "\n",
    "train_target = train_dir +\"/baldeagle\"\n",
    "train_nontarget = train_dir +\"/nonbaldeagle\"\n",
    "\n",
    "val_target = validation_dir +\"/baldeagle\"\n",
    "val_nontarget = validation_dir +\"/nonbaldeagle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f74cf91-6312-46d4-9bff-681f5f76467a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5200, 224, 224, 3) (5200,) <class 'numpy.ndarray'>\n",
      "0 255\n"
     ]
    }
   ],
   "source": [
    "def load_images_from_folders(folder1, folder2, img_size = (224,224)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Load images from the first folder\n",
    "    for filename in os.listdir(folder1):\n",
    "        img_path = os.path.join(folder1, filename)\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                img = img.resize(img_size)\n",
    "                images.append(np.array(img))  # Convert image to array\n",
    "                labels.append(1)  # Class label for folder1\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load image {filename} from {folder1}: {e}\")\n",
    "\n",
    "    # Load images from the second folder\n",
    "    for filename in os.listdir(folder2):\n",
    "        img_path = os.path.join(folder2, filename)\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                img = img.resize(img_size)\n",
    "                images.append(np.array(img))\n",
    "                labels.append(0)  # Class label for folder2\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load image {filename} from {folder2}: {e}\")\n",
    "\n",
    "    # convert lists to NumPy arrays\n",
    "    \n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "images_train, label_train = load_images_from_folders(train_target, train_nontarget)\n",
    "images_val, label_val = load_images_from_folders(val_target, val_nontarget)\n",
    "\n",
    "print(images_train.shape, label_train.shape, type(images_train))\n",
    "print(images_train.min(), images_train.max())  # expected: 0 255, later will normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6143f3dc-836b-4315-9e97-4cf579260a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_labels(labels, percentage):\n",
    "    random.seed(340)\n",
    "    label_one_indices = np.where(labels == 1)[0]\n",
    "    \n",
    "    n = int(len(label_one_indices) * (percentage / 100))\n",
    "    \n",
    "    indices_to_change = np.random.choice(label_one_indices, size=n, replace=False)\n",
    "    \n",
    "    labels[indices_to_change] = 0\n",
    "    \n",
    "    return labels, indices_to_change\n",
    "\n",
    "# for example, change 20% of label 1s to label 0\n",
    "percentage = 0  \n",
    "# changed_indices\n",
    "# label_train, changed_indices = change_labels(label_train, percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "980bc2ad-4698-4f9f-a8db-1dc3928a24ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "tf.keras.utils.set_random_seed(340)\n",
    "#tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdca8f8b-c9b4-4f46-801c-62e5525d8c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "]\n",
    "epochs = 15\n",
    "# restore best weights make the model be the one that was the best instead of last one\n",
    "# patience changed from 4-->3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bed7c8e9-7d70-49a3-a311-c02690cc9137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload Images\n",
    "images_train, label_train = load_images_from_folders(train_target, train_nontarget)\n",
    "percentage = 20  \n",
    "# changed_indices\n",
    "label_train, changed_indices = change_labels(label_train, percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88983d88-6da4-46e2-b761-5995888cbe71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 137ms/step - accuracy: 0.7777 - loss: 49.7898 - val_accuracy: 1.0000 - val_loss: 0.0458\n",
      "Epoch 2/15\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 136ms/step - accuracy: 0.9982 - loss: 0.0537 - val_accuracy: 1.0000 - val_loss: 0.0044\n",
      "Epoch 3/15\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 136ms/step - accuracy: 1.0000 - loss: 0.0097 - val_accuracy: 1.0000 - val_loss: 0.0013\n",
      "Epoch 4/15\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 136ms/step - accuracy: 1.0000 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 7.4632e-04\n",
      "Epoch 5/15\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 137ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 3.6104e-04\n",
      "Epoch 6/15\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 136ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 1.0000 - val_loss: 3.8361e-04\n",
      "Epoch 7/15\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 136ms/step - accuracy: 1.0000 - loss: 8.8517e-04 - val_accuracy: 1.0000 - val_loss: 9.1421e-05\n",
      "Epoch 8/15\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 136ms/step - accuracy: 1.0000 - loss: 8.2077e-04 - val_accuracy: 1.0000 - val_loss: 1.2030e-04\n",
      "Epoch 9/15\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 136ms/step - accuracy: 1.0000 - loss: 4.8001e-04 - val_accuracy: 1.0000 - val_loss: 1.1037e-04\n",
      "Epoch 10/15\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 136ms/step - accuracy: 1.0000 - loss: 4.0078e-04 - val_accuracy: 1.0000 - val_loss: 6.7728e-05\n",
      "Epoch 11/15\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 136ms/step - accuracy: 1.0000 - loss: 3.1357e-04 - val_accuracy: 1.0000 - val_loss: 4.7557e-05\n",
      "Epoch 12/15\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 136ms/step - accuracy: 1.0000 - loss: 2.6157e-04 - val_accuracy: 1.0000 - val_loss: 3.8973e-05\n",
      "Epoch 13/15\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 136ms/step - accuracy: 1.0000 - loss: 2.1426e-04 - val_accuracy: 1.0000 - val_loss: 3.0127e-05\n",
      "Epoch 14/15\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 136ms/step - accuracy: 1.0000 - loss: 1.8746e-04 - val_accuracy: 1.0000 - val_loss: 4.2802e-05\n",
      "Epoch 15/15\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 136ms/step - accuracy: 1.0000 - loss: 1.4404e-04 - val_accuracy: 1.0000 - val_loss: 2.4154e-05\n"
     ]
    }
   ],
   "source": [
    "#Remake Models\n",
    "#mutliple inputs taken from chat and https://pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/\n",
    "# we have full confidence if it is a 1, the lower the number the more confident you are in the 0 class\n",
    "# .999999 vs .00004\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dropout, Concatenate\n",
    "confidence_init = np.array([.35 if x<.5 else 1 for x in label_train]).reshape(5200,1)\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "\n",
    "# add new fully connected layers for binary classification\n",
    "image_input = base_model.input\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "\n",
    "additional_input = Input(shape=(1,), name=\"additional_input\") #shape is just 1 feature for the confidence \n",
    "y = Dense(64, activation='relu')(additional_input) \n",
    "y = Dropout(0.1)(y) # when .5, the additional input was too powerful, the prediction vals were always either to close to 1 or 0, we try to make the additional input less important than the images\n",
    "\n",
    "combined = Concatenate()([x, y]) # 2 channels\n",
    "combined = Dense(256, activation='relu')(combined)\n",
    "combined = Dense(1, activation='sigmoid')(combined)  # sigmoid for binary \n",
    "\n",
    "model = Model(inputs=[image_input, additional_input], outputs=combined)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'], jit_compile=False)\n",
    "\n",
    "# train model\n",
    "#val_confidence = np.array([.6 if x<.5 else 1 for x in label_val]).reshape(200,1)\n",
    "\n",
    "history = model.fit(\n",
    "    [images_train, confidence_init],\n",
    "    label_train,\n",
    "    batch_size = 32,\n",
    "    epochs=epochs,  # adjust this for more epochs as needed\n",
    "    validation_data=([images_val, label_val.reshape(-1,1)], label_val),\n",
    "    callbacks = callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b56be67c-b6ff-4cc5-b583-06a31b72a9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step\n"
     ]
    }
   ],
   "source": [
    "#accracy isnt what we care about rn, we care more about the confidence score and actual classigications\n",
    "preds = model.predict([images_train, confidence_init])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f292a4fe-98cc-4b37-8eb7-3cb9915b9ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    7,   10,   15,   18,   25,   33,   34,   36,   43,   45,\n",
       "         46,   47,   51,   53,   55,   59,   71,   72,   73,   74,   82,\n",
       "         91,   93,   95,  105,  106,  107,  120,  123,  124,  130,  134,\n",
       "        138,  145,  146,  148,  149,  150,  158,  160,  169,  193,  201,\n",
       "        204,  207,  224,  228,  235,  239,  245,  249,  255,  260,  266,\n",
       "        269,  273,  283,  290,  292,  293,  298,  304,  306,  307,  311,\n",
       "        313,  315,  322,  326,  329,  336,  340,  341,  349,  351,  352,\n",
       "        353,  357,  362,  364,  368,  370,  377,  378,  385,  386,  387,\n",
       "        388,  392,  400,  403,  409,  410,  414,  417,  423,  428,  432,\n",
       "        436,  438,  455,  458,  460,  465,  470,  477,  480,  482,  486,\n",
       "        499,  511,  530,  533,  534,  537,  546,  547,  553,  564,  566,\n",
       "        572,  573,  574,  578,  598,  601,  604,  619,  623,  634,  656,\n",
       "        665,  666,  672,  674,  675,  677,  684,  687,  688,  689,  698,\n",
       "        710,  720,  723,  724,  731,  734,  738,  743,  777,  781,  782,\n",
       "        786,  790,  794,  802,  803,  810,  811,  818,  822,  825,  829,\n",
       "        830,  833,  836,  837,  839,  841,  849,  850,  856,  858,  859,\n",
       "        861,  865,  866,  867,  874,  877,  879,  882,  884,  900,  901,\n",
       "        905,  909,  913,  918,  923,  926,  938,  945,  947,  953,  969,\n",
       "        973,  976,  986,  996, 1001, 1007, 1014, 1019, 1029, 1032, 1039,\n",
       "       1041, 1050, 1052, 1055, 1068, 1078, 1082, 1083, 1085, 1086, 1087,\n",
       "       1089, 1096, 1100, 1101, 1103, 1105, 1109, 1140, 1148, 1151, 1157,\n",
       "       1177, 1178, 1179, 1187, 1188, 1193, 1200, 1201, 1203, 1211, 1213,\n",
       "       1216, 1217, 1220, 1221, 1223, 1230, 1232, 1235, 1240, 1244, 1245,\n",
       "       1249, 1263, 1265, 1267, 1268, 1291, 1299])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changed_indices.sort()\n",
    "changed_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "011ab0fc-a0a0-4323-aae2-b5c25c026081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1040,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(label_train > .5)[0].shape #260 eagles missing, instead more eagles are getting flipped to noneagles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b5bdcce-9aa8-4637-a4bb-e3558eb1042c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1040,)\n",
      "[   0    0    0 ... 5199 5199 5199]\n"
     ]
    }
   ],
   "source": [
    "preds\n",
    "preds.reshape(5200,)\n",
    "print(np.where(preds.reshape(5200,) >.5)[0].shape)\n",
    "#find indices of those 17 indexes that were incorrectly flipped and flip them to correct label?\n",
    "misclassified_indices = np.where((label_train> .5) & (preds <= .5))[0]\n",
    "print(misclassified_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f19daf3-28c9-4d4b-9086-b960d2079ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00012634016\n",
      "6.2458086e-05\n",
      "[1.55298927e-113]\n"
     ]
    }
   ],
   "source": [
    "print(preds[changed_indices].mean()) #preds for eagles labeled as 0, could we try flipping back top/highest value n percent of these back into eagles?\n",
    "print(preds[1300:].mean()) #noneagles labeled as noneagles\n",
    "from scipy.stats import ttest_ind\n",
    "tstat, pval = ttest_ind(preds[changed_indices], preds[1300:])\n",
    "print(pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6df58171-f378-4910-86d3-6d44a9aa7348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total misclassified labels: 0\n",
      "Number of labels flipped: 0\n",
      "Indices flipped: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming:\n",
    "# - true_labels: True binary labels (0 for \"Not Eagle\", 1 for \"Eagle\")\n",
    "# - preds: Model predictions (binary, 0 for \"Not Eagle\", 1 for \"Eagle\")\n",
    "# - confidences: Confidence scores for class 1 (\"Eagle\") from the model output\n",
    "\n",
    "# 1. Identify misclassified labels (true label = 1 but predicted as 0)\n",
    "misclassified_indices = np.where((label_train== 1) & (preds == 0))[0]\n",
    "\n",
    "# 2. Extract confidence scores for these misclassified labels\n",
    "misclassified_confidences = confidence_init[misclassified_indices]\n",
    "\n",
    "# 3. Rank these misclassified labels by confidence in descending order\n",
    "ranked_indices = misclassified_indices[np.argsort(-misclassified_confidences)]  # Sort by confidence\n",
    "\n",
    "# 4. Define percentage or count for flipping\n",
    "# Use a count instead of a percentage since the total misclassifications are small\n",
    "max_flips = 20  # Define how many misclassified labels you want to flip at most\n",
    "flip_indices = ranked_indices[:max_flips]\n",
    "\n",
    "# 5. Flip the selected indices back to the correct label\n",
    "preds[flip_indices] = 1\n",
    "\n",
    "# Optional: Output for debugging or validation\n",
    "print(f\"Total misclassified labels: {len(misclassified_indices)}\")\n",
    "print(f\"Number of labels flipped: {len(flip_indices)}\")\n",
    "print(f\"Indices flipped: {flip_indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "743888a6-ca42-4f0f-ae2c-f7eac89b22f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/1335118.1.academic-gpu/ipykernel_1837666/3252721010.py:15: SmallSampleWarning: All axis-slices of one or more sample arguments are too small; all elements of returned arrays will be NaN. See documentation for sample size requirements.\n",
      "  t_stat, p_value = ttest_ind(misclassified_scores, correctly_classified_scores, equal_var=False)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Select indices to flip based on the threshold\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m flip_indices \u001b[38;5;241m=\u001b[39m \u001b[43mmisclassified_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmisclassified_scores\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Flip the selected indices back to the correct label\u001b[39;00m\n\u001b[1;32m     30\u001b[0m preds[flip_indices] \u001b[38;5;241m=\u001b[39m true_labels[flip_indices]\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Assume `scores` is an array of prediction scores for all samples\n",
    "# Higher scores indicate stronger confidence in the prediction\n",
    "# `true_labels` is the ground truth, and `preds` are the predictions\n",
    "misclassified_indices = np.where(preds != label_train)[0]\n",
    "correctly_classified_indices = np.where(preds == label_train)[0]\n",
    "\n",
    "# Scores for misclassified and correctly classified samples\n",
    "misclassified_scores = confidence_init[misclassified_indices]\n",
    "correctly_classified_scores = confidence_init[correctly_classified_indices]\n",
    "\n",
    "# Perform a two-sample t-test\n",
    "t_stat, p_value = ttest_ind(misclassified_scores, correctly_classified_scores, equal_var=False)\n",
    "\n",
    "# Set a significance level (e.g., alpha = 0.05)\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    # Find the minimum score among misclassified samples that is significant\n",
    "    threshold = np.min(misclassified_scores)\n",
    "else:\n",
    "    # No statistically significant difference; do not flip any labels\n",
    "    threshold = float('inf')\n",
    "\n",
    "# Select indices to flip based on the threshold\n",
    "flip_indices = misclassified_indices[misclassified_scores >= threshold]\n",
    "\n",
    "# Flip the selected indices back to the correct label\n",
    "preds[flip_indices] = true_labels[flip_indices]\n",
    "\n",
    "# Optional: Output for debugging or validation\n",
    "print(f\"Total misclassified labels: {len(misclassified_indices)}\")\n",
    "print(f\"Number of labels flipped: {len(flip_indices)}\")\n",
    "print(f\"Indices flipped: {flip_indices}\")\n",
    "print(f\"T-statistic: {t_stat}, P-value: {p_value}, Threshold: {threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d7e7e2-22a7-425c-96c6-d47eec821950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
