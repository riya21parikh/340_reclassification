{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aeda16a-b466-44e5-8ba3-f73264f23edf",
   "metadata": {},
   "source": [
    "# ParikhSamuolisReclassificationNN Final Project File 2\n",
    "## date last modified: Dec 3, 2024\n",
    "### how to link to github --> https://saturncloud.io/blog/how-to-add-jupyter-notebook-to-github/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb8fed7-0ecb-40d3-9f29-f6f781b9984f",
   "metadata": {},
   "source": [
    "# Loading images and necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b497b9ed-b6ac-44a6-9aff-456e09a9c144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e95280c4-13b0-45cc-a013-6ca2f30b0ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directories for train and validation sets\n",
    "root_dir = '/projectnb/ds340/projects/Samuolis_Parikh_Image_Data/'\n",
    "\n",
    "train_dir = root_dir +\"resized_images/train\"\n",
    "validation_dir = root_dir + \"resized_images/validation\"\n",
    "\n",
    "train_target = train_dir +\"/baldeagle\"\n",
    "train_nontarget = train_dir +\"/nonbaldeagle\"\n",
    "\n",
    "val_target = validation_dir +\"/baldeagle\"\n",
    "val_nontarget = validation_dir +\"/nonbaldeagle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a74d33b9-6ff2-4aac-a4b7-9576a8d52d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folders(folder1, folder2, img_size = (224,224)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # load images from the first folder\n",
    "    for filename in os.listdir(folder1):\n",
    "        img_path = os.path.join(folder1, filename)\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                img = img.resize(img_size)\n",
    "                images.append(np.array(img))  # convert image to array\n",
    "                labels.append(1)  # class label for folder1\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load image {filename} from {folder1}: {e}\")\n",
    "\n",
    "    # load images from the second folder\n",
    "    for filename in os.listdir(folder2):\n",
    "        img_path = os.path.join(folder2, filename)\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                img = img.resize(img_size)\n",
    "                images.append(np.array(img))\n",
    "                labels.append(0)  # class label for folder2\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load image {filename} from {folder2}: {e}\")\n",
    "\n",
    "    # convert lists to NumPy arrays\n",
    "    \n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "images_train, label_train = load_images_from_folders(train_target, train_nontarget)\n",
    "images_val, label_val = load_images_from_folders(val_target, val_nontarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fc677bc-cf4f-4345-9025-45a27bf0a1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5200, 224, 224, 3) (5200,) <class 'numpy.ndarray'>\n",
      "0 255\n",
      "Initial eagle count: 1300\n",
      "Initial noneagle count: 3900\n"
     ]
    }
   ],
   "source": [
    "## for debugging:\n",
    "print(images_train.shape, label_train.shape, type(images_train))\n",
    "print(images_train.min(), images_train.max())  # expected: 0 255, later will normalize\n",
    "print(f\"Initial eagle count: {np.sum(label_train == 1)}\")\n",
    "print(f\"Initial noneagle count: {np.sum(label_train == 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79236d4d-b08c-4630-a005-0da7c7d0c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_labels(labels, percentage):\n",
    "    random.seed(340)\n",
    "    label_one_indices = np.where(labels == 1)[0]\n",
    "    \n",
    "    n = int(len(label_one_indices) * (percentage / 100))\n",
    "    \n",
    "    indices_to_change = np.random.choice(label_one_indices, size=n, replace=False)\n",
    "    \n",
    "    labels[indices_to_change] = 0\n",
    "    \n",
    "    return labels, indices_to_change\n",
    "\n",
    "# for example, change 20% of label 1s to label 0\n",
    "percentage = 0  \n",
    "# changed_indices\n",
    "# label_train, changed_indices = change_labels(label_train, percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f8fe98c-89ce-4d1f-a0f3-840d49eebdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 23:19:46.374470: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 23:19:46.390590: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 23:19:46.395420: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "tf.keras.utils.set_random_seed(340)\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66d6aebd-c991-4a74-9b33-f1e5920218c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "]\n",
    "epochs = 15\n",
    "# restore best weights make the model be the one that was the best instead of last one\n",
    "# patience changed from 4-->3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e3319a3-82bf-42a0-afb7-d005da2bbfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload images\n",
    "images_train, label_train = load_images_from_folders(train_target, train_nontarget)\n",
    "percentage = 20  \n",
    "# changed_indices\n",
    "label_train, changed_indices = change_labels(label_train, percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e64eef54-df72-4075-a370-f590a2f831db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New eagle count: 1040\n",
      "New noneagle count: 4160\n"
     ]
    }
   ],
   "source": [
    "print(f\"New eagle count: {np.sum(label_train == 1)}\")\n",
    "print(f\"New noneagle count: {np.sum(label_train == 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44e5183d-4eb9-40e1-8066-a17ae72e4d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New eagle count: 1040\n",
      "New noneagle count: 4160\n",
      "Confidence values: [0.35 1.   1.   1.   1.   1.   1.   0.35 1.   1.  ]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dropout, Concatenate\n",
    "confidence_init = confidence_init = np.array([.35 if x<.5 else 1 for x in label_train]).reshape(5200,1)\n",
    "# start with all 1s for confidence\n",
    "# this doesn't work --- we don't know before hand which indices we aren't confident about, we especially don't know \n",
    "# to specifically be less confident for the labels that we changed\n",
    "# confidence_init[label_train == 0] = 0  # Set confidence to 0 for original 0 labels\n",
    "# confidence_init[changed_indices] = 0.35  # Set confidence to 0.35 for flipped labels\n",
    "# confidence_init = confidence_init.reshape(-1, 1)  # Reshape to (N, 1)\n",
    "\n",
    "# print data statistics\n",
    "print(f\"New eagle count: {np.sum(label_train == 1)}\")\n",
    "print(f\"New noneagle count: {np.sum(label_train == 0)}\")\n",
    "print(f\"Confidence values: {confidence_init[:10].flatten()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc5ea703-0063-4f68-84fa-da03d64bef30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remake models\n",
    "# mutliple inputs taken from chat and https://pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/\n",
    "# we have full confidence if it is a 1, the lower the number the more confident you are in the 0 class -- .999999 vs .00004\n",
    "\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "\n",
    "# add new fully connected layers for binary classification\n",
    "image_input = base_model.input\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "\n",
    "additional_input = Input(shape=(1,), name=\"additional_input\") # shape is just 1 feature for the confidence \n",
    "y = Dense(64, activation='relu')(additional_input) \n",
    "y = Dropout(0.1)(y) # when .5, the additional input was too powerful, the prediction vals were always either to close to 1 or 0, we try to make the additional input less important than the images\n",
    "\n",
    "combined = Concatenate()([x, y]) # 2 channels\n",
    "combined = Dense(256, activation='relu')(combined)\n",
    "combined = Dense(1, activation='sigmoid')(combined)  # sigmoid for binary \n",
    "\n",
    "model = Model(inputs=[image_input, additional_input], outputs=combined)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'], jit_compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed71e4b7-0248-443a-9690-dee283ba7c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    [images_train, confidence_init],\n",
    "    label_train,\n",
    "    batch_size = 32,\n",
    "    epochs=epochs,  # adjust this for more epochs as needed\n",
    "    validation_data=([images_val, label_val.reshape(-1,1)], label_val),\n",
    "    callbacks = callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc85507c-4510-48e0-995a-d35f93bc8fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accracy isnt what we care about rn, we care more about the confidence score and actual classigications\n",
    "preds = model.predict([images_train, confidence_init])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c5a0027-b4fa-443a-9957-1b764db26b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    7,   10,   15,   18,   25,   33,   34,   36,   43,   45,\n",
       "         46,   47,   51,   53,   55,   59,   71,   72,   73,   74,   82,\n",
       "         91,   93,   95,  105,  106,  107,  120,  123,  124,  130,  134,\n",
       "        138,  145,  146,  148,  149,  150,  158,  160,  169,  193,  201,\n",
       "        204,  207,  224,  228,  235,  239,  245,  249,  255,  260,  266,\n",
       "        269,  273,  283,  290,  292,  293,  298,  304,  306,  307,  311,\n",
       "        313,  315,  322,  326,  329,  336,  340,  341,  349,  351,  352,\n",
       "        353,  357,  362,  364,  368,  370,  377,  378,  385,  386,  387,\n",
       "        388,  392,  400,  403,  409,  410,  414,  417,  423,  428,  432,\n",
       "        436,  438,  455,  458,  460,  465,  470,  477,  480,  482,  486,\n",
       "        499,  511,  530,  533,  534,  537,  546,  547,  553,  564,  566,\n",
       "        572,  573,  574,  578,  598,  601,  604,  619,  623,  634,  656,\n",
       "        665,  666,  672,  674,  675,  677,  684,  687,  688,  689,  698,\n",
       "        710,  720,  723,  724,  731,  734,  738,  743,  777,  781,  782,\n",
       "        786,  790,  794,  802,  803,  810,  811,  818,  822,  825,  829,\n",
       "        830,  833,  836,  837,  839,  841,  849,  850,  856,  858,  859,\n",
       "        861,  865,  866,  867,  874,  877,  879,  882,  884,  900,  901,\n",
       "        905,  909,  913,  918,  923,  926,  938,  945,  947,  953,  969,\n",
       "        973,  976,  986,  996, 1001, 1007, 1014, 1019, 1029, 1032, 1039,\n",
       "       1041, 1050, 1052, 1055, 1068, 1078, 1082, 1083, 1085, 1086, 1087,\n",
       "       1089, 1096, 1100, 1101, 1103, 1105, 1109, 1140, 1148, 1151, 1157,\n",
       "       1177, 1178, 1179, 1187, 1188, 1193, 1200, 1201, 1203, 1211, 1213,\n",
       "       1216, 1217, 1220, 1221, 1223, 1230, 1232, 1235, 1240, 1244, 1245,\n",
       "       1249, 1263, 1265, 1267, 1268, 1291, 1299])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changed_indices.sort()\n",
    "changed_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "864f5896-3699-4f29-8b7d-c89288471ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1040,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(label_train > .5)[0].shape #260 eagles missing, instead more eagles are getting flipped to noneagles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bf2c9cc-fad0-49d6-8028-7d95112cb62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1040,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds\n",
    "preds.reshape(5200,)\n",
    "np.where(preds.reshape(5200,) >.5)[0].shape #find indices of those 17 indexes that were incorrectly flipped and flip them to correct label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad917216-e690-4251-a548-603ac9d890de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007381159"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[changed_indices].mean() #preds for eagles labeled as 0, could we try flipping back top/highest value n percent of these back into eagles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69af3e9b-4a68-46fd-81ab-e386ba308480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002475439"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[1300:].mean() #noneagles labeled as noneagles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25f499be-edde-4e7b-a970-d5a0e7fef88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "tstat, pval = ttest_ind(preds[changed_indices], preds[1300:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ab1ddfe-875c-46dc-ab98-bbe1b6d5dcd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.51675191e-17])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "841452da-aeab-43db-8108-232093a6af0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79a5a32-3422-4a93-96a1-44112e0e0911",
   "metadata": {},
   "source": [
    "and this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "269dee85-444c-41fb-956d-89713c2d7bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1...\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 45ms/step\n",
      "Changed Indices: [  15   25   55   73  201  235  283  315  353  368  378  410  417  428\n",
      "  432  438  455  465  530  547  578  674  684  724  802  803  829  836\n",
      "  837  849  856  861  918  973 1029 1052 1085 1086 1101 1177 1193 1211\n",
      " 1213 1221 1299]\n",
      "Predictions for Changed Indices: [0.33993948 0.301688   0.31609848 0.346323   0.31062728 0.3332955\n",
      " 0.31176782 0.31758687 0.33170742 0.30259776 0.3220533  0.3301907\n",
      " 0.3241291  0.34405994 0.31074417 0.30899304 0.31454054 0.30906466\n",
      " 0.36607313 0.30413967 0.39055392 0.31689996 0.3912428  0.34620482\n",
      " 0.30776814 0.32108304 0.30376634 0.3192135  0.30213514 0.30695465\n",
      " 0.49647298 0.3226291  0.37533212 0.31266627 0.31192333 0.3216466\n",
      " 0.36463854 0.31626424 0.35049868 0.32555488 0.37126392 0.30479458\n",
      " 0.31322291 0.31673646 0.36950284]\n",
      "Label change ratio: 0.0000\n",
      "Eagles in predictions: 1040\n",
      "Noneagles in predictions: 4160\n",
      "Labels have converged.\n",
      "After Convergence - Correctly labeled eagles: 1040\n",
      "After Convergence - Correctly labeled noneagles: 4160\n"
     ]
    }
   ],
   "source": [
    "#trash rn\n",
    "import numpy as np\n",
    "\n",
    "# Define function to update labels based on misclassifications and confidence\n",
    "def update_labels(preds, labels, changed_indices, confidence_init, threshold=0.05):\n",
    "    new_labels = labels.copy()\n",
    "\n",
    "    # Misclassified eagles: predicted as non-eagles (preds <= 0.5) but they are actually eagles (labels == 1)\n",
    "    misclassified_eagles = np.where((labels[:1300] == 1) & (preds[:1300] <= 0.5))[0]\n",
    "\n",
    "    # Adjust confidence for misclassifications within the eagle class\n",
    "    for idx in misclassified_eagles:\n",
    "        confidence_init[idx] = min(confidence_init[idx] + 0.1, 1)  # Increase confidence for eagle class\n",
    "\n",
    "    # Flip misclassified eagles to eagles (preds closer to 1) if confidently predicted\n",
    "    for idx in misclassified_eagles:\n",
    "        new_labels[idx] = 1  # Flip to eagle\n",
    "\n",
    "    return new_labels  \n",
    "\n",
    "# Iterative relabeling process\n",
    "converged = False\n",
    "iteration = 0\n",
    "threshold = 0.05  # Statistical significance level\n",
    "tolerance = 0.01  # Convergence tolerance (less than 1% labels change)\n",
    "\n",
    "\n",
    "# Start iterative process\n",
    "while not converged:\n",
    "    iteration += 1\n",
    "    print(f\"Iteration {iteration}...\")\n",
    "\n",
    "    # Retrain the model with the current labels and confidence scores\n",
    "    history = model.fit(\n",
    "        [images_train, confidence_init],  # Use updated confidence\n",
    "        label_train,\n",
    "        batch_size=32,\n",
    "        epochs=1,  # To see values after each iteration\n",
    "        validation_data=([images_val, label_val.reshape(-1, 1)], label_val),\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Predict updated confidence scores from the model\n",
    "    preds = model.predict([images_train, confidence_init]).flatten()\n",
    "\n",
    "    # Filter `changed_indices` to keep only ambiguous predictions\n",
    "    changed_indices = changed_indices[np.where((preds[changed_indices] > 0.3) & (preds[changed_indices] < 0.7))[0]]\n",
    "    changed_indices = np.sort(changed_indices)\n",
    "\n",
    "    print(f\"Changed Indices: {changed_indices}\")\n",
    "    print(f\"Predictions for Changed Indices: {preds[changed_indices]}\")\n",
    "\n",
    "    # Update labels based on misclassifications and confidence\n",
    "    new_labels = update_labels(preds, label_train, changed_indices, confidence_init, threshold)\n",
    "\n",
    "    # Update confidence scores to reflect updated predictions\n",
    "    confidence_init = preds.reshape(-1, 1)\n",
    "\n",
    "    # Check for convergence\n",
    "    label_changes = np.sum(label_train != new_labels)\n",
    "    change_ratio = label_changes / len(label_train)\n",
    "    print(f\"Label change ratio: {change_ratio:.4f}\")\n",
    "    print(f\"Eagles in predictions: {np.sum(preds > 0.5)}\")\n",
    "    print(f\"Noneagles in predictions: {np.sum(preds <= 0.5)}\")\n",
    "\n",
    "    # Check if the labels have converged by comparing the old and new labels\n",
    "    if change_ratio < tolerance:  \n",
    "        converged = True\n",
    "    else:\n",
    "        label_train = new_labels.copy()  # Update labels for the next iteration\n",
    "\n",
    "print(\"Labels have converged.\")\n",
    "\n",
    "# Final print\n",
    "eagle_count = np.sum(label_train == 1)\n",
    "noneagle_count = np.sum(label_train == 0)\n",
    "print(f\"After Convergence - Correctly labeled eagles: {eagle_count}\")\n",
    "print(f\"After Convergence - Correctly labeled noneagles: {noneagle_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba674e1-757a-4db3-b351-f36bc3c645a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### riya 12/2 + tomas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e705eac-f141-4c4b-861e-13af0781541e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Indices: 5\n"
     ]
    }
   ],
   "source": [
    "data_array = np.array([0, 1, 0, 3, 0, 5])  # The array containing 0s and other values\n",
    "index_array = np.array([0, 1, 2, 3, 4, 5])  # The array of indices\n",
    "\n",
    "# filter the index_array where the corresponding value in data_array is not 0\n",
    "filtered_indices = index_array[data_array[index_array] != 0]\n",
    "\n",
    "print(\"Filtered Indices:\", filtered_indices[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe5fc09-9739-4272-bf76-ad80741582c0",
   "metadata": {},
   "source": [
    "5, 10, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74453bdf-a5c0-4764-b238-901b1f3a8810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1...\n",
      "\u001b[1m  5/163\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 42ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 21:43:02.746316: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step\n",
      "Adjusted 208 indices, avg confidence: 0.0000\n",
      "Changed Indices: [np.int64(530), np.int64(578), np.int64(1302), np.int64(1314), np.int64(1329), np.int64(1332), np.int64(1334), np.int64(1371), np.int64(1372), np.int64(1429), np.int64(1440), np.int64(1441), np.int64(1442), np.int64(1446), np.int64(1454), np.int64(1476), np.int64(1484), np.int64(1488), np.int64(1528), np.int64(1534), np.int64(1565), np.int64(1587), np.int64(1594), np.int64(1596), np.int64(1599), np.int64(1622), np.int64(1640), np.int64(1645), np.int64(1660), np.int64(1668), np.int64(1676), np.int64(1682), np.int64(1695), np.int64(1723), np.int64(1785), np.int64(1797), np.int64(1800), np.int64(1821), np.int64(1825), np.int64(1862), np.int64(1873), np.int64(1895), np.int64(1922), np.int64(1927), np.int64(1953), np.int64(1974), np.int64(1977), np.int64(2014), np.int64(2108), np.int64(2119), np.int64(2122), np.int64(2128), np.int64(2144), np.int64(2151), np.int64(2168), np.int64(2173), np.int64(2185), np.int64(2198), np.int64(2210), np.int64(2241), np.int64(2246), np.int64(2272), np.int64(2292), np.int64(2318), np.int64(2326), np.int64(2340), np.int64(2352), np.int64(2369), np.int64(2375), np.int64(2383), np.int64(2447), np.int64(2453), np.int64(2459), np.int64(2464), np.int64(2467), np.int64(2468), np.int64(2478), np.int64(2483), np.int64(2493), np.int64(2498), np.int64(2503), np.int64(2505), np.int64(2553), np.int64(2602), np.int64(2611), np.int64(2643), np.int64(2651), np.int64(2696), np.int64(2713), np.int64(2743), np.int64(2744), np.int64(2755), np.int64(2760), np.int64(2762), np.int64(2807), np.int64(2825), np.int64(2829), np.int64(2854), np.int64(2867), np.int64(2883), np.int64(2915), np.int64(2964), np.int64(2999), np.int64(3016), np.int64(3045), np.int64(3066), np.int64(3068), np.int64(3075), np.int64(3121), np.int64(3133), np.int64(3143), np.int64(3215), np.int64(3264), np.int64(3330), np.int64(3341), np.int64(3385), np.int64(3387), np.int64(3403), np.int64(3407), np.int64(3422), np.int64(3433), np.int64(3434), np.int64(3445), np.int64(3455), np.int64(3495), np.int64(3510), np.int64(3514), np.int64(3522), np.int64(3538), np.int64(3543), np.int64(3552), np.int64(3565), np.int64(3579), np.int64(3587), np.int64(3638), np.int64(3642), np.int64(3656), np.int64(3676), np.int64(3704), np.int64(3723), np.int64(3724), np.int64(3765), np.int64(3789), np.int64(3806), np.int64(3812), np.int64(3887), np.int64(3891), np.int64(3933), np.int64(3947), np.int64(3950), np.int64(3978), np.int64(3981), np.int64(4036), np.int64(4048), np.int64(4098), np.int64(4105), np.int64(4118), np.int64(4135), np.int64(4149), np.int64(4172), np.int64(4179), np.int64(4186), np.int64(4227), np.int64(4230), np.int64(4247), np.int64(4265), np.int64(4285), np.int64(4290), np.int64(4310), np.int64(4320), np.int64(4359), np.int64(4379), np.int64(4396), np.int64(4426), np.int64(4446), np.int64(4466), np.int64(4505), np.int64(4514), np.int64(4526), np.int64(4551), np.int64(4601), np.int64(4656), np.int64(4681), np.int64(4708), np.int64(4720), np.int64(4748), np.int64(4772), np.int64(4774), np.int64(4780), np.int64(4785), np.int64(4804), np.int64(4806), np.int64(4812), np.int64(4813), np.int64(4827), np.int64(4912), np.int64(4917), np.int64(4949), np.int64(5008), np.int64(5012), np.int64(5021), np.int64(5047), np.int64(5058), np.int64(5070), np.int64(5109), np.int64(5138), np.int64(5156), np.int64(5180)]\n",
      "Predictions for Changed Indices: [8.7454914e-09 4.9178166e-08 6.5326432e-08 4.0678364e-07 4.8147109e-07\n",
      " 5.0621708e-07 6.6759429e-07 6.7791018e-07 7.8842908e-07 8.6817801e-07\n",
      " 9.4510557e-07 9.5379073e-07 1.0041176e-06 1.0419724e-06 1.0681204e-06\n",
      " 1.0805153e-06 1.1233801e-06 1.1406205e-06 1.1416716e-06 1.1538555e-06\n",
      " 1.2312950e-06 1.2460990e-06 1.2701067e-06 1.3260403e-06 1.3746859e-06\n",
      " 1.3809694e-06 1.3838802e-06 1.4656190e-06 1.4663628e-06 1.4697257e-06\n",
      " 1.4907180e-06 1.5161340e-06 1.5738259e-06 1.6306590e-06 1.6381906e-06\n",
      " 1.6707337e-06 1.6794688e-06 1.6833428e-06 1.6872096e-06 1.7464076e-06\n",
      " 1.7681029e-06 1.7860057e-06 1.7939212e-06 1.8002951e-06 1.8063902e-06\n",
      " 1.8387078e-06 1.8493561e-06 1.8686075e-06 1.8980552e-06 1.9301381e-06\n",
      " 1.9321549e-06 1.9902473e-06 1.9982961e-06 2.0077612e-06 2.0269306e-06\n",
      " 2.0293212e-06 2.0305022e-06 2.0828886e-06 2.1163069e-06 2.1365695e-06\n",
      " 2.1375988e-06 2.2414129e-06 2.2559268e-06 2.2764884e-06 2.3240752e-06\n",
      " 2.3822017e-06 2.4302801e-06 2.4582143e-06 2.4702374e-06 2.4925432e-06\n",
      " 2.4974497e-06 2.5024515e-06 2.5492150e-06 2.5683903e-06 2.5775628e-06\n",
      " 2.6030905e-06 2.6075804e-06 2.6103794e-06 2.6481055e-06 2.6506677e-06\n",
      " 2.6727209e-06 2.7427541e-06 2.7777592e-06 2.7892531e-06 2.8015857e-06\n",
      " 2.8117113e-06 2.8142358e-06 2.8840520e-06 2.9459002e-06 2.9513192e-06\n",
      " 2.9773953e-06 2.9787075e-06 3.0011997e-06 3.0243546e-06 3.0324788e-06\n",
      " 3.0498775e-06 3.0526974e-06 3.1195007e-06 3.1389282e-06 3.1441739e-06\n",
      " 3.1780889e-06 3.1792556e-06 3.2051535e-06 3.2157043e-06 3.2164680e-06\n",
      " 3.2191042e-06 3.2265175e-06 3.2400044e-06 3.2418218e-06 3.2515409e-06\n",
      " 3.2621790e-06 3.2639996e-06 3.2708704e-06 3.2716564e-06 3.2858052e-06\n",
      " 3.3104518e-06 3.4066788e-06 3.4315838e-06 3.4430045e-06 3.4556458e-06\n",
      " 3.4699585e-06 3.4725147e-06 3.5499904e-06 3.5548521e-06 3.5599921e-06\n",
      " 3.5630180e-06 3.6100064e-06 3.6245124e-06 3.6308813e-06 3.6354618e-06\n",
      " 3.6720662e-06 3.6999597e-06 3.7195912e-06 3.7213265e-06 3.7408288e-06\n",
      " 3.7522805e-06 3.7957363e-06 3.8002743e-06 3.8273633e-06 3.8289227e-06\n",
      " 3.8325247e-06 3.8495914e-06 3.8662356e-06 3.8768876e-06 3.8914750e-06\n",
      " 3.9067727e-06 3.9086844e-06 3.9196625e-06 3.9693737e-06 3.9911965e-06\n",
      " 4.0075661e-06 4.0115892e-06 4.0640157e-06 4.0646632e-06 4.0651516e-06\n",
      " 4.0680684e-06 4.0713130e-06 4.0754694e-06 4.0885884e-06 4.0919590e-06\n",
      " 4.1065523e-06 4.1078642e-06 4.1216535e-06 4.1365424e-06 4.1575076e-06\n",
      " 4.1596609e-06 4.1828962e-06 4.1857697e-06 4.2393126e-06 4.2634115e-06\n",
      " 4.2691199e-06 4.3011755e-06 4.3636578e-06 4.4020821e-06 4.4031449e-06\n",
      " 4.4152930e-06 4.4251192e-06 4.4325739e-06 4.4337366e-06 4.4343537e-06\n",
      " 4.4429466e-06 4.5272591e-06 4.5337870e-06 4.5610695e-06 4.5703964e-06\n",
      " 4.5931788e-06 4.5979777e-06 4.5996753e-06 4.5999868e-06 4.6229475e-06\n",
      " 4.6462756e-06 4.6583041e-06 4.6724927e-06 4.6834266e-06 4.6866035e-06\n",
      " 4.7071367e-06 4.7077783e-06 4.7251451e-06 4.7267854e-06 4.7300955e-06\n",
      " 4.7703411e-06 4.8065813e-06 4.8128104e-06 4.8203988e-06 4.8288648e-06\n",
      " 4.8360357e-06 4.8641073e-06 4.8952720e-06]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [np.int64(578), np.int64(530)]\n",
      "Total Number of Eagles the model believes are not eagles 2\n",
      "Total Number of images the model believes are not eagles 208\n",
      "Change ratio: 0.0400\n",
      "Iteration 2...\n",
      "\u001b[1m  5/163\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 42ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 21:43:35.428100: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step\n",
      "Adjusted 197 indices, avg confidence: 0.0000\n",
      "Changed Indices: [np.int64(731), np.int64(1307), np.int64(1339), np.int64(1400), np.int64(1463), np.int64(1468), np.int64(1487), np.int64(1502), np.int64(1513), np.int64(1521), np.int64(1545), np.int64(1570), np.int64(1611), np.int64(1623), np.int64(1665), np.int64(1666), np.int64(1681), np.int64(1688), np.int64(1696), np.int64(1711), np.int64(1715), np.int64(1719), np.int64(1768), np.int64(1773), np.int64(1778), np.int64(1780), np.int64(1795), np.int64(1859), np.int64(1897), np.int64(1899), np.int64(1965), np.int64(1971), np.int64(1976), np.int64(1984), np.int64(2061), np.int64(2073), np.int64(2079), np.int64(2083), np.int64(2100), np.int64(2103), np.int64(2114), np.int64(2148), np.int64(2160), np.int64(2229), np.int64(2290), np.int64(2296), np.int64(2299), np.int64(2307), np.int64(2313), np.int64(2327), np.int64(2348), np.int64(2357), np.int64(2389), np.int64(2412), np.int64(2423), np.int64(2446), np.int64(2449), np.int64(2499), np.int64(2508), np.int64(2514), np.int64(2519), np.int64(2540), np.int64(2556), np.int64(2587), np.int64(2608), np.int64(2630), np.int64(2632), np.int64(2637), np.int64(2670), np.int64(2742), np.int64(2754), np.int64(2774), np.int64(2805), np.int64(2808), np.int64(2836), np.int64(2922), np.int64(2923), np.int64(2939), np.int64(2965), np.int64(2972), np.int64(2991), np.int64(3000), np.int64(3020), np.int64(3069), np.int64(3095), np.int64(3119), np.int64(3131), np.int64(3132), np.int64(3149), np.int64(3159), np.int64(3198), np.int64(3200), np.int64(3220), np.int64(3235), np.int64(3246), np.int64(3251), np.int64(3262), np.int64(3268), np.int64(3302), np.int64(3307), np.int64(3373), np.int64(3376), np.int64(3420), np.int64(3438), np.int64(3464), np.int64(3544), np.int64(3547), np.int64(3566), np.int64(3585), np.int64(3617), np.int64(3618), np.int64(3623), np.int64(3627), np.int64(3634), np.int64(3696), np.int64(3761), np.int64(3796), np.int64(3804), np.int64(3805), np.int64(3813), np.int64(3833), np.int64(3837), np.int64(3851), np.int64(3860), np.int64(3876), np.int64(3914), np.int64(3923), np.int64(3958), np.int64(3977), np.int64(4011), np.int64(4025), np.int64(4051), np.int64(4079), np.int64(4089), np.int64(4112), np.int64(4156), np.int64(4175), np.int64(4192), np.int64(4207), np.int64(4219), np.int64(4232), np.int64(4243), np.int64(4262), np.int64(4272), np.int64(4282), np.int64(4288), np.int64(4297), np.int64(4342), np.int64(4363), np.int64(4367), np.int64(4382), np.int64(4404), np.int64(4433), np.int64(4443), np.int64(4479), np.int64(4485), np.int64(4503), np.int64(4530), np.int64(4586), np.int64(4650), np.int64(4651), np.int64(4660), np.int64(4666), np.int64(4683), np.int64(4697), np.int64(4757), np.int64(4761), np.int64(4782), np.int64(4786), np.int64(4788), np.int64(4791), np.int64(4809), np.int64(4818), np.int64(4837), np.int64(4840), np.int64(4844), np.int64(4848), np.int64(4852), np.int64(4853), np.int64(4856), np.int64(4857), np.int64(4863), np.int64(4875), np.int64(4889), np.int64(4971), np.int64(4980), np.int64(4993), np.int64(5002), np.int64(5010), np.int64(5026), np.int64(5043), np.int64(5073), np.int64(5084), np.int64(5113), np.int64(5123), np.int64(5128), np.int64(5162)]\n",
      "Predictions for Changed Indices: [2.1148260e-06 2.1254084e-06 2.1603637e-06 2.2529341e-06 2.2619113e-06\n",
      " 2.2627721e-06 2.2876336e-06 2.3098364e-06 2.3269870e-06 2.3394880e-06\n",
      " 2.3542752e-06 2.3573780e-06 2.3644186e-06 2.3733987e-06 2.3794498e-06\n",
      " 2.4110341e-06 2.4144347e-06 2.4266947e-06 2.4271669e-06 2.4654007e-06\n",
      " 2.4685653e-06 2.4831927e-06 2.5051065e-06 2.5120994e-06 2.5151055e-06\n",
      " 2.5273248e-06 2.5374707e-06 2.5395307e-06 2.5397171e-06 2.5503114e-06\n",
      " 2.5679251e-06 2.6290670e-06 2.6293028e-06 2.6614227e-06 2.6697737e-06\n",
      " 2.6702446e-06 2.6730397e-06 2.6737764e-06 2.6745797e-06 2.6794689e-06\n",
      " 2.6834225e-06 2.6868076e-06 2.7095234e-06 2.7102910e-06 2.7123156e-06\n",
      " 2.7230481e-06 2.7342144e-06 2.7447952e-06 2.7534757e-06 2.7566839e-06\n",
      " 2.7743467e-06 2.7810247e-06 2.7968447e-06 2.8050210e-06 2.8105774e-06\n",
      " 2.8228585e-06 2.8277191e-06 2.8410889e-06 2.8447762e-06 2.8744087e-06\n",
      " 2.8808277e-06 2.8839831e-06 2.8881889e-06 2.8923591e-06 2.9011967e-06\n",
      " 2.9107273e-06 2.9209523e-06 2.9327873e-06 2.9359858e-06 2.9440298e-06\n",
      " 2.9542082e-06 2.9816804e-06 2.9893163e-06 2.9901205e-06 3.0140091e-06\n",
      " 3.0165800e-06 3.0170088e-06 3.0200226e-06 3.0264002e-06 3.0486126e-06\n",
      " 3.0514514e-06 3.0790350e-06 3.0812819e-06 3.0907381e-06 3.0936044e-06\n",
      " 3.0948820e-06 3.0974422e-06 3.1075879e-06 3.1179213e-06 3.1182603e-06\n",
      " 3.1190752e-06 3.1199679e-06 3.1237785e-06 3.1314860e-06 3.1318950e-06\n",
      " 3.1426812e-06 3.1507354e-06 3.1603774e-06 3.1711618e-06 3.1741724e-06\n",
      " 3.1812697e-06 3.1918178e-06 3.1944915e-06 3.2009839e-06 3.2045059e-06\n",
      " 3.2132887e-06 3.2141038e-06 3.2245148e-06 3.2507442e-06 3.2549663e-06\n",
      " 3.2650673e-06 3.2656840e-06 3.2750779e-06 3.2851126e-06 3.3393162e-06\n",
      " 3.3473032e-06 3.3478555e-06 3.3544354e-06 3.3666940e-06 3.3722149e-06\n",
      " 3.3784597e-06 3.4282798e-06 3.4284469e-06 3.4378404e-06 3.4386073e-06\n",
      " 3.4497884e-06 3.4597449e-06 3.4626300e-06 3.4814377e-06 3.4883931e-06\n",
      " 3.4888890e-06 3.5036533e-06 3.5050637e-06 3.5103758e-06 3.5151797e-06\n",
      " 3.5226567e-06 3.5381372e-06 3.5385253e-06 3.5506000e-06 3.5700862e-06\n",
      " 3.5713731e-06 3.5951025e-06 3.5958979e-06 3.6147637e-06 3.6162637e-06\n",
      " 3.6189617e-06 3.6191270e-06 3.6223526e-06 3.6232507e-06 3.6232577e-06\n",
      " 3.6234305e-06 3.6407528e-06 3.6425208e-06 3.6458014e-06 3.6480274e-06\n",
      " 3.6504493e-06 3.6785157e-06 3.6842803e-06 3.6888543e-06 3.7240350e-06\n",
      " 3.7316822e-06 3.7526061e-06 3.7542061e-06 3.7813884e-06 3.8078240e-06\n",
      " 3.8131950e-06 3.8269109e-06 3.8349231e-06 3.8476605e-06 3.8594535e-06\n",
      " 3.8606759e-06 3.8859416e-06 3.8983762e-06 3.9032348e-06 3.9077490e-06\n",
      " 3.9310653e-06 3.9395659e-06 3.9400957e-06 3.9448560e-06 3.9598872e-06\n",
      " 3.9601555e-06 3.9817601e-06 3.9852171e-06 3.9859051e-06 3.9917672e-06\n",
      " 3.9917900e-06 3.9951074e-06 3.9976035e-06 3.9992433e-06 4.0013642e-06\n",
      " 4.0079867e-06 4.0148616e-06 4.0187651e-06 4.0352634e-06 4.0459872e-06\n",
      " 4.0741793e-06 4.0756172e-06]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [np.int64(731)]\n",
      "Total Number of Eagles the model believes are not eagles 3\n",
      "Total Number of images the model believes are not eagles 405\n",
      "Change ratio: 0.0379\n",
      "Iteration 3...\n",
      "\u001b[1m  5/163\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 42ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 21:44:07.944383: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step\n",
      "Adjusted 187 indices, avg confidence: 0.0000\n",
      "Changed Indices: [np.int64(1308), np.int64(1344), np.int64(1376), np.int64(1383), np.int64(1388), np.int64(1418), np.int64(1470), np.int64(1493), np.int64(1552), np.int64(1578), np.int64(1580), np.int64(1609), np.int64(1786), np.int64(1812), np.int64(1817), np.int64(1828), np.int64(1833), np.int64(1835), np.int64(1842), np.int64(1889), np.int64(1905), np.int64(1913), np.int64(1942), np.int64(1954), np.int64(1962), np.int64(1981), np.int64(1982), np.int64(1987), np.int64(1998), np.int64(2002), np.int64(2016), np.int64(2035), np.int64(2038), np.int64(2046), np.int64(2075), np.int64(2094), np.int64(2112), np.int64(2131), np.int64(2133), np.int64(2156), np.int64(2157), np.int64(2161), np.int64(2183), np.int64(2190), np.int64(2359), np.int64(2360), np.int64(2372), np.int64(2380), np.int64(2395), np.int64(2397), np.int64(2422), np.int64(2495), np.int64(2555), np.int64(2557), np.int64(2564), np.int64(2569), np.int64(2601), np.int64(2615), np.int64(2623), np.int64(2638), np.int64(2655), np.int64(2660), np.int64(2665), np.int64(2686), np.int64(2690), np.int64(2703), np.int64(2729), np.int64(2756), np.int64(2761), np.int64(2813), np.int64(2844), np.int64(2859), np.int64(2871), np.int64(2885), np.int64(2898), np.int64(2901), np.int64(2921), np.int64(2940), np.int64(2960), np.int64(2983), np.int64(2987), np.int64(2995), np.int64(3009), np.int64(3025), np.int64(3039), np.int64(3044), np.int64(3059), np.int64(3093), np.int64(3104), np.int64(3155), np.int64(3167), np.int64(3189), np.int64(3233), np.int64(3316), np.int64(3324), np.int64(3333), np.int64(3344), np.int64(3360), np.int64(3427), np.int64(3439), np.int64(3451), np.int64(3460), np.int64(3489), np.int64(3497), np.int64(3499), np.int64(3527), np.int64(3573), np.int64(3584), np.int64(3593), np.int64(3596), np.int64(3600), np.int64(3613), np.int64(3649), np.int64(3670), np.int64(3678), np.int64(3684), np.int64(3706), np.int64(3725), np.int64(3754), np.int64(3819), np.int64(3870), np.int64(3902), np.int64(3908), np.int64(3957), np.int64(3975), np.int64(3987), np.int64(4003), np.int64(4017), np.int64(4029), np.int64(4032), np.int64(4034), np.int64(4055), np.int64(4071), np.int64(4073), np.int64(4083), np.int64(4093), np.int64(4106), np.int64(4128), np.int64(4129), np.int64(4240), np.int64(4260), np.int64(4318), np.int64(4328), np.int64(4335), np.int64(4355), np.int64(4361), np.int64(4394), np.int64(4399), np.int64(4414), np.int64(4424), np.int64(4442), np.int64(4448), np.int64(4450), np.int64(4475), np.int64(4486), np.int64(4499), np.int64(4509), np.int64(4550), np.int64(4567), np.int64(4588), np.int64(4629), np.int64(4634), np.int64(4659), np.int64(4687), np.int64(4694), np.int64(4705), np.int64(4743), np.int64(4744), np.int64(4745), np.int64(4778), np.int64(4829), np.int64(4842), np.int64(4877), np.int64(4899), np.int64(4923), np.int64(4939), np.int64(4952), np.int64(4958), np.int64(4988), np.int64(4994), np.int64(4996), np.int64(5097), np.int64(5100), np.int64(5135), np.int64(5164), np.int64(5175), np.int64(5186)]\n",
      "Predictions for Changed Indices: [4.5125598e-06 4.8507331e-06 5.8275286e-06 5.8598239e-06 5.8853111e-06\n",
      " 5.8853111e-06 5.9954218e-06 6.0514517e-06 6.0841226e-06 6.1762098e-06\n",
      " 6.2049194e-06 6.2569193e-06 6.2630870e-06 6.2641616e-06 6.3346902e-06\n",
      " 6.3410366e-06 6.3420703e-06 6.3699717e-06 6.3769926e-06 6.3818284e-06\n",
      " 6.3902630e-06 6.3944635e-06 6.4547153e-06 6.4585702e-06 6.4764940e-06\n",
      " 6.4891929e-06 6.5184254e-06 6.5505365e-06 6.5757736e-06 6.5881895e-06\n",
      " 6.5974191e-06 6.6158427e-06 6.6171929e-06 6.6382272e-06 6.6760786e-06\n",
      " 6.6982461e-06 6.6994471e-06 6.7103688e-06 6.7456649e-06 6.7624442e-06\n",
      " 6.7945853e-06 6.8225122e-06 6.8535919e-06 6.8571157e-06 6.8992276e-06\n",
      " 6.9084585e-06 6.9155903e-06 6.9189018e-06 6.9354965e-06 6.9430143e-06\n",
      " 6.9560047e-06 6.9871130e-06 6.9924522e-06 6.9931193e-06 7.0116639e-06\n",
      " 7.0154160e-06 7.0214533e-06 7.0261685e-06 7.0313567e-06 7.0379988e-06\n",
      " 7.0415035e-06 7.0666297e-06 7.0933947e-06 7.0944361e-06 7.1152576e-06\n",
      " 7.1645281e-06 7.1842396e-06 7.1928780e-06 7.2081016e-06 7.2162038e-06\n",
      " 7.2527846e-06 7.2536486e-06 7.2619896e-06 7.2762837e-06 7.2790872e-06\n",
      " 7.3125957e-06 7.3305973e-06 7.3323586e-06 7.3362971e-06 7.3661831e-06\n",
      " 7.4089476e-06 7.4175232e-06 7.4396266e-06 7.4591562e-06 7.4700615e-06\n",
      " 7.4740947e-06 7.4892218e-06 7.4973400e-06 7.5088747e-06 7.5226867e-06\n",
      " 7.5422895e-06 7.5610437e-06 7.5631274e-06 7.5871831e-06 7.5882044e-06\n",
      " 7.5913167e-06 7.5953571e-06 7.6046194e-06 7.6069118e-06 7.6305041e-06\n",
      " 7.6494844e-06 7.6566512e-06 7.6593169e-06 7.6634442e-06 7.6672459e-06\n",
      " 7.6709248e-06 7.6805218e-06 7.7241530e-06 7.7291497e-06 7.7312507e-06\n",
      " 7.7752302e-06 7.7911736e-06 7.7979603e-06 7.8109106e-06 7.8332750e-06\n",
      " 7.8609191e-06 7.8836820e-06 7.8883068e-06 7.9183874e-06 7.9323472e-06\n",
      " 7.9450592e-06 7.9664087e-06 7.9862548e-06 7.9929523e-06 8.0007239e-06\n",
      " 8.0177879e-06 8.0315858e-06 8.0319687e-06 8.0341142e-06 8.0356076e-06\n",
      " 8.0519394e-06 8.0762957e-06 8.0817508e-06 8.1285843e-06 8.1380467e-06\n",
      " 8.1402977e-06 8.1472408e-06 8.1514381e-06 8.1644539e-06 8.1876824e-06\n",
      " 8.1877597e-06 8.1901180e-06 8.2095512e-06 8.2221104e-06 8.2373681e-06\n",
      " 8.2423894e-06 8.2497972e-06 8.2672032e-06 8.2706174e-06 8.3021014e-06\n",
      " 8.3026234e-06 8.3031136e-06 8.3313516e-06 8.3451714e-06 8.3549585e-06\n",
      " 8.3751329e-06 8.3769382e-06 8.3778496e-06 8.4824997e-06 8.5142128e-06\n",
      " 8.5168922e-06 8.5202073e-06 8.5322490e-06 8.5588572e-06 8.5636093e-06\n",
      " 8.6022274e-06 8.6025475e-06 8.6308728e-06 8.6412174e-06 8.6758073e-06\n",
      " 8.6764276e-06 8.6779419e-06 8.6783812e-06 8.6886494e-06 8.6942191e-06\n",
      " 8.7328590e-06 8.7407170e-06 8.7471799e-06 8.7477965e-06 8.7526205e-06\n",
      " 8.7613553e-06 8.7829558e-06 8.8273091e-06 8.8322777e-06 8.8450051e-06\n",
      " 8.8922416e-06 8.8976458e-06]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: []\n",
      "Total Number of Eagles the model believes are not eagles 3\n",
      "Total Number of images the model believes are not eagles 592\n",
      "Change ratio: 0.0360\n",
      "Iteration 4...\n",
      "\u001b[1m  5/163\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 42ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 21:44:40.673121: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step\n",
      "Adjusted 178 indices, avg confidence: 0.0000\n",
      "Changed Indices: [np.int64(1331), np.int64(1349), np.int64(1387), np.int64(1430), np.int64(1436), np.int64(1518), np.int64(1526), np.int64(1553), np.int64(1607), np.int64(1619), np.int64(1620), np.int64(1629), np.int64(1689), np.int64(1701), np.int64(1716), np.int64(1718), np.int64(1724), np.int64(1731), np.int64(1744), np.int64(1762), np.int64(1770), np.int64(1826), np.int64(1840), np.int64(1858), np.int64(1928), np.int64(1943), np.int64(1979), np.int64(1994), np.int64(1999), np.int64(2008), np.int64(2015), np.int64(2045), np.int64(2066), np.int64(2121), np.int64(2180), np.int64(2203), np.int64(2217), np.int64(2225), np.int64(2239), np.int64(2297), np.int64(2315), np.int64(2364), np.int64(2426), np.int64(2433), np.int64(2435), np.int64(2482), np.int64(2512), np.int64(2528), np.int64(2549), np.int64(2551), np.int64(2559), np.int64(2563), np.int64(2565), np.int64(2570), np.int64(2575), np.int64(2579), np.int64(2584), np.int64(2598), np.int64(2605), np.int64(2657), np.int64(2797), np.int64(2798), np.int64(2828), np.int64(2830), np.int64(2839), np.int64(2842), np.int64(2845), np.int64(2886), np.int64(2959), np.int64(2982), np.int64(2988), np.int64(3013), np.int64(3017), np.int64(3023), np.int64(3031), np.int64(3055), np.int64(3060), np.int64(3087), np.int64(3101), np.int64(3130), np.int64(3137), np.int64(3141), np.int64(3157), np.int64(3181), np.int64(3196), np.int64(3222), np.int64(3223), np.int64(3228), np.int64(3248), np.int64(3271), np.int64(3273), np.int64(3276), np.int64(3284), np.int64(3305), np.int64(3308), np.int64(3310), np.int64(3323), np.int64(3338), np.int64(3348), np.int64(3354), np.int64(3359), np.int64(3364), np.int64(3386), np.int64(3413), np.int64(3442), np.int64(3483), np.int64(3511), np.int64(3516), np.int64(3534), np.int64(3548), np.int64(3582), np.int64(3679), np.int64(3700), np.int64(3710), np.int64(3736), np.int64(3758), np.int64(3776), np.int64(3783), np.int64(3854), np.int64(3856), np.int64(3866), np.int64(3879), np.int64(3881), np.int64(3916), np.int64(3925), np.int64(3942), np.int64(3964), np.int64(3986), np.int64(4002), np.int64(4010), np.int64(4061), np.int64(4065), np.int64(4070), np.int64(4088), np.int64(4122), np.int64(4124), np.int64(4144), np.int64(4157), np.int64(4160), np.int64(4178), np.int64(4212), np.int64(4222), np.int64(4251), np.int64(4266), np.int64(4300), np.int64(4323), np.int64(4338), np.int64(4413), np.int64(4465), np.int64(4491), np.int64(4492), np.int64(4512), np.int64(4524), np.int64(4572), np.int64(4577), np.int64(4589), np.int64(4603), np.int64(4607), np.int64(4608), np.int64(4614), np.int64(4633), np.int64(4676), np.int64(4734), np.int64(4766), np.int64(4819), np.int64(4887), np.int64(4905), np.int64(4926), np.int64(4950), np.int64(4981), np.int64(5004), np.int64(5006), np.int64(5112), np.int64(5132), np.int64(5133), np.int64(5144), np.int64(5155), np.int64(5167)]\n",
      "Predictions for Changed Indices: [6.7231158e-06 6.9327989e-06 6.9700222e-06 7.0125534e-06 7.1109434e-06\n",
      " 7.1142808e-06 7.1228219e-06 7.2067405e-06 7.2206994e-06 7.2306775e-06\n",
      " 7.2588182e-06 7.2705816e-06 7.2918292e-06 7.2946118e-06 7.3113270e-06\n",
      " 7.3868796e-06 7.3924325e-06 7.3946749e-06 7.4143763e-06 7.4249833e-06\n",
      " 7.4310683e-06 7.4407694e-06 7.4439058e-06 7.4621803e-06 7.4643654e-06\n",
      " 7.4664154e-06 7.4796289e-06 7.4912714e-06 7.5173866e-06 7.5202829e-06\n",
      " 7.5305456e-06 7.5439593e-06 7.5505882e-06 7.5651396e-06 7.5961830e-06\n",
      " 7.5981393e-06 7.6170900e-06 7.6317483e-06 7.6325196e-06 7.6344268e-06\n",
      " 7.6454726e-06 7.6601282e-06 7.6653960e-06 7.6680508e-06 7.6725710e-06\n",
      " 7.6755941e-06 7.6782298e-06 7.6784563e-06 7.6875422e-06 7.7182704e-06\n",
      " 7.7227032e-06 7.7350041e-06 7.7353507e-06 7.7373797e-06 7.7477471e-06\n",
      " 7.7575878e-06 7.7645673e-06 7.7783970e-06 7.7784116e-06 7.7799177e-06\n",
      " 7.7822106e-06 7.7871546e-06 7.8009061e-06 7.8095854e-06 7.8151133e-06\n",
      " 7.8193925e-06 7.8344628e-06 7.8366975e-06 7.8439498e-06 7.8567145e-06\n",
      " 7.8714966e-06 7.8910762e-06 7.9024703e-06 7.9280744e-06 7.9332776e-06\n",
      " 7.9502279e-06 7.9523279e-06 7.9814590e-06 7.9943557e-06 8.0000909e-06\n",
      " 8.0057689e-06 8.0126738e-06 8.0236314e-06 8.0259506e-06 8.0375476e-06\n",
      " 8.0633281e-06 8.0692434e-06 8.0740538e-06 8.0913360e-06 8.0939599e-06\n",
      " 8.1013659e-06 8.1249646e-06 8.1272665e-06 8.1332055e-06 8.1424796e-06\n",
      " 8.1429844e-06 8.1577691e-06 8.1636517e-06 8.2146735e-06 8.2351062e-06\n",
      " 8.2590077e-06 8.2650668e-06 8.2679990e-06 8.2699707e-06 8.2771512e-06\n",
      " 8.2905808e-06 8.2945980e-06 8.3020368e-06 8.3035738e-06 8.3063142e-06\n",
      " 8.3067098e-06 8.3126533e-06 8.3174264e-06 8.3398963e-06 8.3558907e-06\n",
      " 8.3617579e-06 8.3644454e-06 8.3787445e-06 8.3802624e-06 8.3927553e-06\n",
      " 8.3996101e-06 8.4001540e-06 8.4023659e-06 8.4195481e-06 8.4217718e-06\n",
      " 8.4531939e-06 8.4548055e-06 8.4670955e-06 8.4704789e-06 8.4784961e-06\n",
      " 8.4815374e-06 8.4882613e-06 8.4886178e-06 8.4893618e-06 8.4985395e-06\n",
      " 8.5147485e-06 8.5155280e-06 8.5201982e-06 8.5218890e-06 8.5273605e-06\n",
      " 8.5334941e-06 8.5742822e-06 8.5756637e-06 8.5917491e-06 8.5923393e-06\n",
      " 8.5923639e-06 8.5968641e-06 8.6121099e-06 8.6145255e-06 8.6173359e-06\n",
      " 8.6219798e-06 8.6394875e-06 8.6442924e-06 8.6535629e-06 8.6672562e-06\n",
      " 8.6678929e-06 8.7008302e-06 8.7026474e-06 8.7035360e-06 8.7211329e-06\n",
      " 8.7223971e-06 8.7336175e-06 8.7603948e-06 8.7832823e-06 8.7973658e-06\n",
      " 8.8098586e-06 8.8146153e-06 8.8192319e-06 8.8736224e-06 8.8795387e-06\n",
      " 8.8827910e-06 8.8851466e-06 8.8932511e-06 8.9223049e-06 8.9468431e-06\n",
      " 8.9527666e-06 8.9593095e-06 8.9625819e-06]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: []\n",
      "Total Number of Eagles the model believes are not eagles 3\n",
      "Total Number of images the model believes are not eagles 770\n",
      "Change ratio: 0.0342\n",
      "Iteration 5...\n",
      "\u001b[1m  5/163\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 43ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 21:45:13.482331: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step\n",
      "Adjusted 169 indices, avg confidence: 0.0000\n",
      "Changed Indices: [np.int64(1301), np.int64(1311), np.int64(1322), np.int64(1361), np.int64(1369), np.int64(1370), np.int64(1424), np.int64(1432), np.int64(1478), np.int64(1480), np.int64(1482), np.int64(1507), np.int64(1548), np.int64(1549), np.int64(1568), np.int64(1575), np.int64(1635), np.int64(1643), np.int64(1739), np.int64(1754), np.int64(1771), np.int64(1776), np.int64(1849), np.int64(1854), np.int64(1881), np.int64(1894), np.int64(1907), np.int64(1912), np.int64(1921), np.int64(1931), np.int64(2004), np.int64(2025), np.int64(2036), np.int64(2154), np.int64(2226), np.int64(2230), np.int64(2236), np.int64(2249), np.int64(2301), np.int64(2306), np.int64(2343), np.int64(2347), np.int64(2393), np.int64(2466), np.int64(2506), np.int64(2521), np.int64(2525), np.int64(2538), np.int64(2554), np.int64(2617), np.int64(2652), np.int64(2679), np.int64(2683), np.int64(2685), np.int64(2746), np.int64(2796), np.int64(2809), np.int64(2876), np.int64(2893), np.int64(2903), np.int64(2908), np.int64(2912), np.int64(2926), np.int64(2971), np.int64(2993), np.int64(3004), np.int64(3008), np.int64(3026), np.int64(3027), np.int64(3028), np.int64(3038), np.int64(3042), np.int64(3049), np.int64(3063), np.int64(3076), np.int64(3086), np.int64(3089), np.int64(3108), np.int64(3117), np.int64(3129), np.int64(3162), np.int64(3174), np.int64(3205), np.int64(3213), np.int64(3224), np.int64(3231), np.int64(3303), np.int64(3365), np.int64(3487), np.int64(3502), np.int64(3508), np.int64(3528), np.int64(3537), np.int64(3549), np.int64(3572), np.int64(3601), np.int64(3608), np.int64(3622), np.int64(3681), np.int64(3733), np.int64(3756), np.int64(3767), np.int64(3769), np.int64(3770), np.int64(3807), np.int64(3846), np.int64(3869), np.int64(3872), np.int64(3899), np.int64(3937), np.int64(4050), np.int64(4068), np.int64(4099), np.int64(4138), np.int64(4150), np.int64(4162), np.int64(4190), np.int64(4193), np.int64(4199), np.int64(4201), np.int64(4213), np.int64(4239), np.int64(4246), np.int64(4279), np.int64(4283), np.int64(4287), np.int64(4289), np.int64(4304), np.int64(4337), np.int64(4346), np.int64(4372), np.int64(4417), np.int64(4419), np.int64(4420), np.int64(4430), np.int64(4451), np.int64(4495), np.int64(4496), np.int64(4541), np.int64(4544), np.int64(4564), np.int64(4613), np.int64(4636), np.int64(4646), np.int64(4685), np.int64(4724), np.int64(4727), np.int64(4735), np.int64(4739), np.int64(4741), np.int64(4758), np.int64(4793), np.int64(4794), np.int64(4797), np.int64(4799), np.int64(4807), np.int64(4808), np.int64(4810), np.int64(4824), np.int64(4943), np.int64(4946), np.int64(4955), np.int64(4959), np.int64(4962), np.int64(5005), np.int64(5019), np.int64(5102), np.int64(5116), np.int64(5146)]\n",
      "Predictions for Changed Indices: [8.59178999e-06 8.89691546e-06 8.96661732e-06 9.19071863e-06\n",
      " 9.23872631e-06 9.32460352e-06 9.48862089e-06 9.50555841e-06\n",
      " 9.50773392e-06 9.50858521e-06 9.54053212e-06 9.56180065e-06\n",
      " 9.57011162e-06 9.58398505e-06 9.61059050e-06 9.63038110e-06\n",
      " 9.63196089e-06 9.63368802e-06 9.66912376e-06 9.68476616e-06\n",
      " 9.69310986e-06 9.69799203e-06 9.69865778e-06 9.70819929e-06\n",
      " 9.72618636e-06 9.72917314e-06 9.76617139e-06 9.78153275e-06\n",
      " 9.79282595e-06 9.79532888e-06 9.80465666e-06 9.81718495e-06\n",
      " 9.81753146e-06 9.81863559e-06 9.82691745e-06 9.83117206e-06\n",
      " 9.83469818e-06 9.85905808e-06 9.89877753e-06 9.90049648e-06\n",
      " 9.90963963e-06 9.93545382e-06 9.95328264e-06 9.99102849e-06\n",
      " 1.00023062e-05 1.00032694e-05 1.00140460e-05 1.00416546e-05\n",
      " 1.00462630e-05 1.00528368e-05 1.00530870e-05 1.00695515e-05\n",
      " 1.01029100e-05 1.01034111e-05 1.01055593e-05 1.01116520e-05\n",
      " 1.01194564e-05 1.01371133e-05 1.01416481e-05 1.01429059e-05\n",
      " 1.01509859e-05 1.01582873e-05 1.01773612e-05 1.01813321e-05\n",
      " 1.01902488e-05 1.01916285e-05 1.02011190e-05 1.02038639e-05\n",
      " 1.02120594e-05 1.02181584e-05 1.02268250e-05 1.02303848e-05\n",
      " 1.02430577e-05 1.02439562e-05 1.02569475e-05 1.02679387e-05\n",
      " 1.02712484e-05 1.02779895e-05 1.02781269e-05 1.02844024e-05\n",
      " 1.02893264e-05 1.02903678e-05 1.02994773e-05 1.03088332e-05\n",
      " 1.03167895e-05 1.03271350e-05 1.03494367e-05 1.03573557e-05\n",
      " 1.03698476e-05 1.04059573e-05 1.04286983e-05 1.04410574e-05\n",
      " 1.04489663e-05 1.04778846e-05 1.04969076e-05 1.05027348e-05\n",
      " 1.05089675e-05 1.05359413e-05 1.05381923e-05 1.05494946e-05\n",
      " 1.05778017e-05 1.05778427e-05 1.06016059e-05 1.06045272e-05\n",
      " 1.06135021e-05 1.06232528e-05 1.06257967e-05 1.06437274e-05\n",
      " 1.06458092e-05 1.06688085e-05 1.06817688e-05 1.06862117e-05\n",
      " 1.06934494e-05 1.07070919e-05 1.07340729e-05 1.07378009e-05\n",
      " 1.07462629e-05 1.07996038e-05 1.08290787e-05 1.08379218e-05\n",
      " 1.08630156e-05 1.08758486e-05 1.08791573e-05 1.09144075e-05\n",
      " 1.09174580e-05 1.09816874e-05 1.09880566e-05 1.09886960e-05\n",
      " 1.10053606e-05 1.10157653e-05 1.10234687e-05 1.10250985e-05\n",
      " 1.10285573e-05 1.10298524e-05 1.10314822e-05 1.10597985e-05\n",
      " 1.10845594e-05 1.10849705e-05 1.10893270e-05 1.11078925e-05\n",
      " 1.11147801e-05 1.11603040e-05 1.11630179e-05 1.11831132e-05\n",
      " 1.11854715e-05 1.11870922e-05 1.12060461e-05 1.12098187e-05\n",
      " 1.12107700e-05 1.12397702e-05 1.12519965e-05 1.12638500e-05\n",
      " 1.12874641e-05 1.13422357e-05 1.13495498e-05 1.13503947e-05\n",
      " 1.13521373e-05 1.13651577e-05 1.13754368e-05 1.13996985e-05\n",
      " 1.14185759e-05 1.14297764e-05 1.14298209e-05 1.14465856e-05\n",
      " 1.14472950e-05 1.14923623e-05 1.14927780e-05 1.15211778e-05\n",
      " 1.15310486e-05]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: []\n",
      "Total Number of Eagles the model believes are not eagles 3\n",
      "Total Number of images the model believes are not eagles 939\n",
      "Change ratio: 0.0325\n",
      "Iteration 6...\n",
      "\u001b[1m  5/163\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 42ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 21:45:46.022273: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step\n",
      "Adjusted 161 indices, avg confidence: 0.0000\n",
      "Changed Indices: [np.int64(537), np.int64(1083), np.int64(1087), np.int64(1109), np.int64(1325), np.int64(1337), np.int64(1355), np.int64(1404), np.int64(1417), np.int64(1445), np.int64(1512), np.int64(1537), np.int64(1579), np.int64(1648), np.int64(1678), np.int64(1700), np.int64(1745), np.int64(1761), np.int64(1765), np.int64(1769), np.int64(1781), np.int64(1837), np.int64(1843), np.int64(1903), np.int64(1939), np.int64(1955), np.int64(1957), np.int64(1985), np.int64(2001), np.int64(2044), np.int64(2072), np.int64(2085), np.int64(2095), np.int64(2135), np.int64(2189), np.int64(2197), np.int64(2201), np.int64(2209), np.int64(2256), np.int64(2279), np.int64(2291), np.int64(2317), np.int64(2332), np.int64(2333), np.int64(2342), np.int64(2370), np.int64(2386), np.int64(2409), np.int64(2439), np.int64(2463), np.int64(2486), np.int64(2496), np.int64(2500), np.int64(2524), np.int64(2532), np.int64(2537), np.int64(2539), np.int64(2546), np.int64(2558), np.int64(2593), np.int64(2667), np.int64(2693), np.int64(2730), np.int64(2741), np.int64(2811), np.int64(2832), np.int64(2848), np.int64(2878), np.int64(2888), np.int64(2896), np.int64(2938), np.int64(2955), np.int64(2996), np.int64(2998), np.int64(3003), np.int64(3024), np.int64(3064), np.int64(3156), np.int64(3184), np.int64(3190), np.int64(3237), np.int64(3270), np.int64(3290), np.int64(3306), np.int64(3311), np.int64(3319), np.int64(3326), np.int64(3346), np.int64(3381), np.int64(3397), np.int64(3398), np.int64(3430), np.int64(3454), np.int64(3456), np.int64(3492), np.int64(3494), np.int64(3520), np.int64(3550), np.int64(3588), np.int64(3645), np.int64(3705), np.int64(3714), np.int64(3759), np.int64(3772), np.int64(3785), np.int64(3792), np.int64(3797), np.int64(3830), np.int64(3831), np.int64(3840), np.int64(3871), np.int64(3886), np.int64(3915), np.int64(4005), np.int64(4045), np.int64(4084), np.int64(4109), np.int64(4115), np.int64(4117), np.int64(4185), np.int64(4198), np.int64(4204), np.int64(4216), np.int64(4256), np.int64(4271), np.int64(4275), np.int64(4293), np.int64(4305), np.int64(4313), np.int64(4409), np.int64(4429), np.int64(4445), np.int64(4473), np.int64(4554), np.int64(4560), np.int64(4580), np.int64(4587), np.int64(4627), np.int64(4628), np.int64(4667), np.int64(4702), np.int64(4704), np.int64(4723), np.int64(4728), np.int64(4753), np.int64(4762), np.int64(4769), np.int64(4915), np.int64(4919), np.int64(4927), np.int64(4928), np.int64(4935), np.int64(4973), np.int64(5001), np.int64(5024), np.int64(5030), np.int64(5060), np.int64(5124), np.int64(5141), np.int64(5142), np.int64(5187)]\n",
      "Predictions for Changed Indices: [1.14848017e-05 1.17420705e-05 1.18649805e-05 1.18711259e-05\n",
      " 1.20921404e-05 1.21496587e-05 1.21828080e-05 1.22123784e-05\n",
      " 1.23041100e-05 1.24156268e-05 1.24449780e-05 1.24815151e-05\n",
      " 1.26060686e-05 1.26141867e-05 1.27177154e-05 1.27315488e-05\n",
      " 1.27516469e-05 1.27566582e-05 1.28238789e-05 1.28343636e-05\n",
      " 1.28504316e-05 1.28649253e-05 1.28974543e-05 1.29808332e-05\n",
      " 1.31835141e-05 1.32047153e-05 1.32283103e-05 1.32287132e-05\n",
      " 1.32390369e-05 1.32406039e-05 1.32583682e-05 1.32778541e-05\n",
      " 1.32804635e-05 1.33035082e-05 1.33368658e-05 1.33658205e-05\n",
      " 1.33814574e-05 1.33978519e-05 1.34147031e-05 1.34265038e-05\n",
      " 1.34350084e-05 1.34821130e-05 1.34930715e-05 1.34997645e-05\n",
      " 1.35185746e-05 1.35444334e-05 1.35865048e-05 1.36018161e-05\n",
      " 1.36239769e-05 1.36733997e-05 1.36745857e-05 1.37399193e-05\n",
      " 1.37468151e-05 1.37501829e-05 1.38028799e-05 1.38062642e-05\n",
      " 1.38152200e-05 1.38423484e-05 1.38559644e-05 1.38833711e-05\n",
      " 1.38846945e-05 1.38927871e-05 1.39538406e-05 1.39714566e-05\n",
      " 1.40121938e-05 1.40419443e-05 1.40849161e-05 1.41078635e-05\n",
      " 1.41249611e-05 1.41525479e-05 1.41613909e-05 1.41704968e-05\n",
      " 1.41789042e-05 1.42010704e-05 1.42209665e-05 1.42364888e-05\n",
      " 1.42427489e-05 1.42584195e-05 1.42722956e-05 1.42724848e-05\n",
      " 1.43072357e-05 1.43179923e-05 1.43721109e-05 1.43732632e-05\n",
      " 1.44201358e-05 1.44365631e-05 1.44437527e-05 1.44528185e-05\n",
      " 1.44617543e-05 1.44637543e-05 1.44873866e-05 1.45016102e-05\n",
      " 1.45031872e-05 1.45235617e-05 1.45271488e-05 1.45307522e-05\n",
      " 1.45367112e-05 1.45421891e-05 1.45684926e-05 1.45956519e-05\n",
      " 1.46178563e-05 1.46206294e-05 1.46372731e-05 1.46544808e-05\n",
      " 1.46574994e-05 1.46619323e-05 1.47108831e-05 1.47140809e-05\n",
      " 1.47165938e-05 1.47265200e-05 1.47516657e-05 1.47665723e-05\n",
      " 1.48020581e-05 1.48054332e-05 1.48285635e-05 1.48533045e-05\n",
      " 1.48627560e-05 1.48856489e-05 1.49128182e-05 1.49142397e-05\n",
      " 1.49297375e-05 1.49375992e-05 1.49578964e-05 1.49712678e-05\n",
      " 1.49927018e-05 1.49946882e-05 1.49997531e-05 1.50032711e-05\n",
      " 1.50254937e-05 1.50279011e-05 1.50318419e-05 1.50481937e-05\n",
      " 1.50482228e-05 1.50581127e-05 1.50600818e-05 1.50625801e-05\n",
      " 1.50715332e-05 1.50810210e-05 1.50835112e-05 1.50934675e-05\n",
      " 1.50955984e-05 1.51076356e-05 1.51308786e-05 1.51328995e-05\n",
      " 1.51750410e-05 1.51929398e-05 1.51998083e-05 1.52008661e-05\n",
      " 1.52085668e-05 1.52224093e-05 1.52308030e-05 1.52537104e-05\n",
      " 1.52593420e-05 1.52687880e-05 1.52836765e-05 1.52848716e-05\n",
      " 1.53297024e-05 1.53329929e-05 1.53336950e-05 1.53809997e-05\n",
      " 1.53838755e-05]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [np.int64(1087), np.int64(537), np.int64(1109), np.int64(1083)]\n",
      "Total Number of Eagles the model believes are not eagles 7\n",
      "Total Number of images the model believes are not eagles 1100\n",
      "Change ratio: 0.0310\n",
      "Iteration 7...\n",
      "\u001b[1m  5/163\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 42ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 21:46:19.058811: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step\n",
      "Adjusted 153 indices, avg confidence: 0.0000\n",
      "Changed Indices: [np.int64(698), np.int64(1306), np.int64(1379), np.int64(1403), np.int64(1414), np.int64(1415), np.int64(1428), np.int64(1459), np.int64(1462), np.int64(1501), np.int64(1535), np.int64(1558), np.int64(1567), np.int64(1585), np.int64(1600), np.int64(1646), np.int64(1753), np.int64(1777), np.int64(1779), np.int64(1787), np.int64(1810), np.int64(1823), np.int64(1829), np.int64(1870), np.int64(1882), np.int64(1892), np.int64(1938), np.int64(1950), np.int64(1960), np.int64(1969), np.int64(2018), np.int64(2037), np.int64(2086), np.int64(2091), np.int64(2109), np.int64(2132), np.int64(2150), np.int64(2260), np.int64(2266), np.int64(2269), np.int64(2302), np.int64(2353), np.int64(2368), np.int64(2373), np.int64(2390), np.int64(2394), np.int64(2403), np.int64(2404), np.int64(2416), np.int64(2431), np.int64(2497), np.int64(2509), np.int64(2523), np.int64(2542), np.int64(2573), np.int64(2583), np.int64(2592), np.int64(2629), np.int64(2631), np.int64(2656), np.int64(2726), np.int64(2733), np.int64(2745), np.int64(2753), np.int64(2757), np.int64(2775), np.int64(2786), np.int64(2801), np.int64(2826), np.int64(2835), np.int64(2857), np.int64(2941), np.int64(2947), np.int64(2953), np.int64(3018), np.int64(3021), np.int64(3081), np.int64(3083), np.int64(3120), np.int64(3153), np.int64(3154), np.int64(3164), np.int64(3165), np.int64(3210), np.int64(3236), np.int64(3269), np.int64(3272), np.int64(3294), np.int64(3299), np.int64(3322), np.int64(3389), np.int64(3411), np.int64(3412), np.int64(3418), np.int64(3448), np.int64(3462), np.int64(3478), np.int64(3539), np.int64(3603), np.int64(3743), np.int64(3745), np.int64(3773), np.int64(3787), np.int64(3853), np.int64(3883), np.int64(3905), np.int64(3921), np.int64(3932), np.int64(3974), np.int64(4062), np.int64(4067), np.int64(4111), np.int64(4164), np.int64(4203), np.int64(4217), np.int64(4261), np.int64(4280), np.int64(4312), np.int64(4319), np.int64(4329), np.int64(4341), np.int64(4347), np.int64(4351), np.int64(4410), np.int64(4416), np.int64(4437), np.int64(4490), np.int64(4537), np.int64(4578), np.int64(4592), np.int64(4632), np.int64(4654), np.int64(4688), np.int64(4691), np.int64(4726), np.int64(4800), np.int64(4828), np.int64(4833), np.int64(4835), np.int64(4838), np.int64(4839), np.int64(4878), np.int64(4920), np.int64(4932), np.int64(4953), np.int64(5000), np.int64(5064), np.int64(5107), np.int64(5119), np.int64(5149), np.int64(5157), np.int64(5170), np.int64(5177)]\n",
      "Predictions for Changed Indices: [8.78103765e-06 1.01535898e-05 1.02010308e-05 1.02348740e-05\n",
      " 1.02764698e-05 1.02900231e-05 1.03100911e-05 1.03491702e-05\n",
      " 1.03582643e-05 1.03987559e-05 1.04417650e-05 1.04541496e-05\n",
      " 1.04558949e-05 1.05248137e-05 1.05358195e-05 1.05471390e-05\n",
      " 1.05540321e-05 1.05966928e-05 1.06081388e-05 1.07125052e-05\n",
      " 1.07174101e-05 1.07423994e-05 1.07434753e-05 1.07552223e-05\n",
      " 1.07811529e-05 1.07816668e-05 1.08004378e-05 1.08031372e-05\n",
      " 1.08214290e-05 1.08348222e-05 1.08390705e-05 1.08625918e-05\n",
      " 1.09004995e-05 1.09036391e-05 1.09098082e-05 1.09186440e-05\n",
      " 1.09208731e-05 1.09411694e-05 1.09668363e-05 1.09774564e-05\n",
      " 1.09825251e-05 1.09831844e-05 1.09986140e-05 1.10055389e-05\n",
      " 1.10254341e-05 1.10346291e-05 1.10511837e-05 1.10602305e-05\n",
      " 1.10698429e-05 1.10729798e-05 1.10743949e-05 1.10789888e-05\n",
      " 1.11173131e-05 1.11251620e-05 1.11315094e-05 1.11955142e-05\n",
      " 1.12317130e-05 1.12327198e-05 1.12393955e-05 1.12557964e-05\n",
      " 1.12705966e-05 1.12853859e-05 1.12917696e-05 1.13075785e-05\n",
      " 1.13210654e-05 1.13525057e-05 1.13557426e-05 1.13602382e-05\n",
      " 1.13652450e-05 1.13875294e-05 1.13910701e-05 1.14167469e-05\n",
      " 1.14297318e-05 1.14502973e-05 1.14604136e-05 1.14654858e-05\n",
      " 1.14895665e-05 1.14895893e-05 1.14941040e-05 1.15090870e-05\n",
      " 1.15115572e-05 1.15184203e-05 1.15573939e-05 1.15611856e-05\n",
      " 1.15751309e-05 1.15927296e-05 1.15949069e-05 1.16318324e-05\n",
      " 1.16441179e-05 1.16621986e-05 1.16642568e-05 1.16666924e-05\n",
      " 1.16964147e-05 1.16970177e-05 1.16981792e-05 1.17264817e-05\n",
      " 1.17476829e-05 1.17487361e-05 1.17587333e-05 1.17728714e-05\n",
      " 1.17875434e-05 1.17876789e-05 1.17892305e-05 1.17964164e-05\n",
      " 1.18010175e-05 1.18244270e-05 1.18324142e-05 1.18336093e-05\n",
      " 1.18768439e-05 1.18919743e-05 1.18937096e-05 1.19028791e-05\n",
      " 1.19034348e-05 1.19243741e-05 1.19285260e-05 1.19459546e-05\n",
      " 1.19612305e-05 1.19944252e-05 1.20046570e-05 1.20049881e-05\n",
      " 1.20078394e-05 1.20204659e-05 1.20222539e-05 1.20263348e-05\n",
      " 1.20382001e-05 1.20385675e-05 1.20519844e-05 1.20539044e-05\n",
      " 1.20586747e-05 1.20675913e-05 1.20765599e-05 1.20806153e-05\n",
      " 1.20841178e-05 1.21036319e-05 1.21085623e-05 1.21165085e-05\n",
      " 1.21418407e-05 1.21630374e-05 1.21825515e-05 1.22042620e-05\n",
      " 1.22145912e-05 1.22317379e-05 1.22546699e-05 1.22613328e-05\n",
      " 1.23066920e-05 1.23170239e-05 1.23469899e-05 1.23476502e-05\n",
      " 1.23536456e-05 1.23680147e-05 1.23752707e-05 1.23762029e-05\n",
      " 1.24125145e-05]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [np.int64(698)]\n",
      "Total Number of Eagles the model believes are not eagles 8\n",
      "Total Number of images the model believes are not eagles 1253\n",
      "Change ratio: 0.0294\n",
      "Iteration 8...\n",
      "\u001b[1m  5/163\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 42ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 21:46:51.889847: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step\n",
      "Adjusted 145 indices, avg confidence: 0.0000\n",
      "Changed Indices: [np.int64(1177), np.int64(1327), np.int64(1341), np.int64(1363), np.int64(1373), np.int64(1391), np.int64(1397), np.int64(1425), np.int64(1448), np.int64(1450), np.int64(1551), np.int64(1576), np.int64(1636), np.int64(1642), np.int64(1673), np.int64(1706), np.int64(1722), np.int64(1796), np.int64(1802), np.int64(1876), np.int64(1906), np.int64(1911), np.int64(1932), np.int64(1958), np.int64(1993), np.int64(2010), np.int64(2022), np.int64(2076), np.int64(2078), np.int64(2107), np.int64(2165), np.int64(2167), np.int64(2176), np.int64(2188), np.int64(2194), np.int64(2216), np.int64(2224), np.int64(2278), np.int64(2282), np.int64(2303), np.int64(2443), np.int64(2448), np.int64(2462), np.int64(2473), np.int64(2475), np.int64(2501), np.int64(2502), np.int64(2520), np.int64(2527), np.int64(2545), np.int64(2585), np.int64(2612), np.int64(2614), np.int64(2641), np.int64(2659), np.int64(2671), np.int64(2694), np.int64(2706), np.int64(2717), np.int64(2740), np.int64(2782), np.int64(2791), np.int64(2792), np.int64(2880), np.int64(2933), np.int64(2944), np.int64(2992), np.int64(2994), np.int64(3002), np.int64(3043), np.int64(3046), np.int64(3105), np.int64(3115), np.int64(3142), np.int64(3171), np.int64(3176), np.int64(3177), np.int64(3186), np.int64(3211), np.int64(3230), np.int64(3255), np.int64(3279), np.int64(3289), np.int64(3295), np.int64(3339), np.int64(3342), np.int64(3400), np.int64(3410), np.int64(3459), np.int64(3507), np.int64(3595), np.int64(3744), np.int64(3790), np.int64(3814), np.int64(3880), np.int64(3922), np.int64(3956), np.int64(3973), np.int64(3991), np.int64(3998), np.int64(4006), np.int64(4021), np.int64(4030), np.int64(4049), np.int64(4107), np.int64(4147), np.int64(4159), np.int64(4208), np.int64(4214), np.int64(4236), np.int64(4252), np.int64(4366), np.int64(4421), np.int64(4440), np.int64(4441), np.int64(4444), np.int64(4452), np.int64(4456), np.int64(4482), np.int64(4497), np.int64(4502), np.int64(4510), np.int64(4520), np.int64(4575), np.int64(4597), np.int64(4624), np.int64(4661), np.int64(4692), np.int64(4714), np.int64(4722), np.int64(4729), np.int64(4773), np.int64(4781), np.int64(4820), np.int64(4913), np.int64(4957), np.int64(5027), np.int64(5042), np.int64(5048), np.int64(5056), np.int64(5061), np.int64(5063), np.int64(5095), np.int64(5115), np.int64(5197)]\n",
      "Predictions for Changed Indices: [7.0286819e-06 7.1067334e-06 7.1765153e-06 7.6293318e-06 7.6298129e-06\n",
      " 7.6800970e-06 7.6839360e-06 7.6923743e-06 7.7486629e-06 7.7961759e-06\n",
      " 7.7992772e-06 7.8256144e-06 7.8350231e-06 7.8647136e-06 7.9336724e-06\n",
      " 7.9850442e-06 7.9990159e-06 8.0050668e-06 8.0146910e-06 8.0499594e-06\n",
      " 8.0753171e-06 8.0825903e-06 8.1395374e-06 8.1906965e-06 8.2022416e-06\n",
      " 8.2714305e-06 8.2800088e-06 8.2991164e-06 8.3369550e-06 8.3520026e-06\n",
      " 8.3847881e-06 8.3874265e-06 8.4057156e-06 8.4119874e-06 8.4292842e-06\n",
      " 8.4404655e-06 8.4483418e-06 8.4602098e-06 8.4802750e-06 8.4834137e-06\n",
      " 8.5587108e-06 8.5947659e-06 8.5973725e-06 8.6331293e-06 8.6521850e-06\n",
      " 8.6612908e-06 8.6784967e-06 8.6898099e-06 8.7261251e-06 8.7288627e-06\n",
      " 8.7386416e-06 8.7391172e-06 8.7617227e-06 8.7678418e-06 8.7776798e-06\n",
      " 8.7904882e-06 8.8077750e-06 8.8246325e-06 8.8263077e-06 8.8263496e-06\n",
      " 8.8278312e-06 8.8289844e-06 8.8302904e-06 8.8310308e-06 8.8363558e-06\n",
      " 8.8551169e-06 8.8946927e-06 8.9153473e-06 8.9397308e-06 8.9411633e-06\n",
      " 8.9535351e-06 8.9688319e-06 8.9747537e-06 8.9832647e-06 8.9933965e-06\n",
      " 8.9971718e-06 9.0080830e-06 9.0147869e-06 9.0148906e-06 9.0218737e-06\n",
      " 9.0439089e-06 9.0708327e-06 9.0708754e-06 9.0745443e-06 9.0790718e-06\n",
      " 9.0855856e-06 9.0935164e-06 9.0936992e-06 9.0972899e-06 9.1331476e-06\n",
      " 9.1376523e-06 9.1700567e-06 9.1719285e-06 9.1793927e-06 9.1904467e-06\n",
      " 9.1967595e-06 9.2032078e-06 9.2265136e-06 9.2343662e-06 9.2527898e-06\n",
      " 9.2636938e-06 9.2664595e-06 9.2664686e-06 9.2702257e-06 9.2718165e-06\n",
      " 9.2957926e-06 9.3001818e-06 9.3172703e-06 9.3217495e-06 9.3311692e-06\n",
      " 9.3344797e-06 9.3443750e-06 9.3521048e-06 9.3597682e-06 9.3767258e-06\n",
      " 9.4242760e-06 9.4250754e-06 9.4441593e-06 9.4703328e-06 9.4746320e-06\n",
      " 9.4840516e-06 9.4854631e-06 9.4992047e-06 9.5036903e-06 9.5149626e-06\n",
      " 9.5151263e-06 9.5242867e-06 9.5261030e-06 9.5278747e-06 9.5420146e-06\n",
      " 9.5529685e-06 9.5586274e-06 9.5607693e-06 9.5772693e-06 9.5912346e-06\n",
      " 9.5913083e-06 9.6078975e-06 9.6360309e-06 9.6526956e-06 9.6554222e-06\n",
      " 9.6623298e-06 9.6633166e-06 9.6679896e-06 9.6834729e-06 9.6857548e-06]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [np.int64(1177)]\n",
      "Total Number of Eagles the model believes are not eagles 9\n",
      "Total Number of images the model believes are not eagles 1398\n",
      "Change ratio: 0.0279\n",
      "Iteration 9...\n",
      "\u001b[1m  5/163\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 43ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 21:47:24.775223: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step\n",
      "Adjusted 138 indices, avg confidence: 0.0000\n",
      "Changed Indices: [np.int64(298), np.int64(684), np.int64(1312), np.int64(1348), np.int64(1380), np.int64(1412), np.int64(1426), np.int64(1455), np.int64(1472), np.int64(1499), np.int64(1519), np.int64(1522), np.int64(1602), np.int64(1618), np.int64(1626), np.int64(1691), np.int64(1702), np.int64(1712), np.int64(1729), np.int64(1791), np.int64(1827), np.int64(1838), np.int64(1877), np.int64(1887), np.int64(1902), np.int64(1910), np.int64(1973), np.int64(1990), np.int64(2067), np.int64(2163), np.int64(2268), np.int64(2276), np.int64(2288), np.int64(2295), np.int64(2319), np.int64(2355), np.int64(2385), np.int64(2387), np.int64(2401), np.int64(2413), np.int64(2456), np.int64(2562), np.int64(2568), np.int64(2618), np.int64(2669), np.int64(2678), np.int64(2682), np.int64(2714), np.int64(2721), np.int64(2768), np.int64(2788), np.int64(2802), np.int64(2820), np.int64(2858), np.int64(2870), np.int64(2914), np.int64(2920), np.int64(2957), np.int64(2980), np.int64(3070), np.int64(3084), np.int64(3092), np.int64(3110), np.int64(3125), np.int64(3138), np.int64(3175), np.int64(3183), np.int64(3334), np.int64(3336), np.int64(3372), np.int64(3374), np.int64(3395), np.int64(3419), np.int64(3446), np.int64(3470), np.int64(3474), np.int64(3484), np.int64(3551), np.int64(3558), np.int64(3563), np.int64(3615), np.int64(3626), np.int64(3628), np.int64(3644), np.int64(3650), np.int64(3652), np.int64(3677), np.int64(3686), np.int64(3718), np.int64(3732), np.int64(3755), np.int64(3800), np.int64(3829), np.int64(3884), np.int64(3930), np.int64(3946), np.int64(3963), np.int64(4012), np.int64(4040), np.int64(4057), np.int64(4103), np.int64(4143), np.int64(4188), np.int64(4237), np.int64(4255), np.int64(4291), np.int64(4292), np.int64(4316), np.int64(4331), np.int64(4386), np.int64(4390), np.int64(4418), np.int64(4432), np.int64(4462), np.int64(4563), np.int64(4593), np.int64(4625), np.int64(4638), np.int64(4673), np.int64(4677), np.int64(4699), np.int64(4700), np.int64(4732), np.int64(4777), np.int64(4815), np.int64(4850), np.int64(4910), np.int64(4911), np.int64(4938), np.int64(4940), np.int64(4968), np.int64(4978), np.int64(5017), np.int64(5071), np.int64(5072), np.int64(5078), np.int64(5080), np.int64(5193)]\n",
      "Predictions for Changed Indices: [1.13968845e-05 1.15183320e-05 1.15353496e-05 1.15650673e-05\n",
      " 1.17540021e-05 1.17796881e-05 1.17957534e-05 1.18017952e-05\n",
      " 1.19118722e-05 1.19482802e-05 1.20036721e-05 1.21586772e-05\n",
      " 1.21757112e-05 1.21868397e-05 1.21917928e-05 1.22090933e-05\n",
      " 1.22416805e-05 1.22450674e-05 1.22800093e-05 1.22870961e-05\n",
      " 1.23091568e-05 1.23094378e-05 1.23215350e-05 1.23373493e-05\n",
      " 1.23492864e-05 1.23496166e-05 1.23506998e-05 1.23518657e-05\n",
      " 1.23862992e-05 1.23998316e-05 1.24103490e-05 1.24189437e-05\n",
      " 1.24295475e-05 1.24363414e-05 1.24428070e-05 1.24870521e-05\n",
      " 1.25090064e-05 1.25125980e-05 1.25145789e-05 1.25151882e-05\n",
      " 1.25262204e-05 1.25543247e-05 1.25650204e-05 1.25665429e-05\n",
      " 1.25900779e-05 1.25985334e-05 1.26570849e-05 1.26606583e-05\n",
      " 1.26664672e-05 1.26919795e-05 1.26955510e-05 1.27122712e-05\n",
      " 1.27393578e-05 1.27426874e-05 1.27489966e-05 1.27970989e-05\n",
      " 1.27972708e-05 1.27979047e-05 1.27998819e-05 1.28012134e-05\n",
      " 1.28116790e-05 1.28135980e-05 1.28198435e-05 1.28365791e-05\n",
      " 1.28402044e-05 1.28404972e-05 1.28468910e-05 1.28593201e-05\n",
      " 1.28607307e-05 1.28747815e-05 1.28780721e-05 1.28841166e-05\n",
      " 1.28862048e-05 1.28896590e-05 1.29069285e-05 1.29317059e-05\n",
      " 1.29433292e-05 1.29652199e-05 1.29666287e-05 1.29718483e-05\n",
      " 1.29812779e-05 1.30043254e-05 1.30137669e-05 1.30343587e-05\n",
      " 1.30430517e-05 1.30510898e-05 1.30534181e-05 1.30657099e-05\n",
      " 1.30974468e-05 1.31014194e-05 1.31082179e-05 1.31292109e-05\n",
      " 1.31448578e-05 1.31452980e-05 1.31694396e-05 1.31824563e-05\n",
      " 1.31889965e-05 1.31903680e-05 1.32108880e-05 1.32203013e-05\n",
      " 1.32260902e-05 1.32355663e-05 1.32578507e-05 1.32624027e-05\n",
      " 1.32883306e-05 1.32970272e-05 1.33042704e-05 1.33163157e-05\n",
      " 1.33484336e-05 1.33836393e-05 1.33848635e-05 1.33876983e-05\n",
      " 1.33926151e-05 1.34433531e-05 1.34475831e-05 1.34487373e-05\n",
      " 1.34646898e-05 1.34651900e-05 1.34855454e-05 1.34892380e-05\n",
      " 1.34950660e-05 1.35300661e-05 1.35336149e-05 1.35352784e-05\n",
      " 1.35471983e-05 1.35476903e-05 1.35606551e-05 1.35829814e-05\n",
      " 1.35901719e-05 1.36177541e-05 1.36313984e-05 1.36376129e-05\n",
      " 1.36525250e-05 1.36750423e-05 1.36772860e-05 1.36851922e-05\n",
      " 1.36953240e-05 1.37060115e-05]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [np.int64(684), np.int64(298)]\n",
      "Total Number of Eagles the model believes are not eagles 11\n",
      "Total Number of images the model believes are not eagles 1536\n",
      "Change ratio: 0.0265\n",
      "Iteration 10...\n",
      "\u001b[1m  5/163\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 42ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 21:47:57.458237: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step\n",
      "Adjusted 131 indices, avg confidence: 0.0000\n",
      "Changed Indices: [np.int64(36), np.int64(91), np.int64(918), np.int64(1300), np.int64(1377), np.int64(1408), np.int64(1437), np.int64(1559), np.int64(1586), np.int64(1616), np.int64(1617), np.int64(1693), np.int64(1704), np.int64(1727), np.int64(1782), np.int64(1799), np.int64(1836), np.int64(1844), np.int64(1890), np.int64(1937), np.int64(1949), np.int64(1952), np.int64(2056), np.int64(2116), np.int64(2124), np.int64(2136), np.int64(2170), np.int64(2172), np.int64(2179), np.int64(2206), np.int64(2227), np.int64(2264), np.int64(2287), np.int64(2300), np.int64(2312), np.int64(2351), np.int64(2361), np.int64(2378), np.int64(2407), np.int64(2424), np.int64(2434), np.int64(2504), np.int64(2510), np.int64(2571), np.int64(2603), np.int64(2633), np.int64(2662), np.int64(2674), np.int64(2676), np.int64(2677), np.int64(2699), np.int64(2708), np.int64(2711), np.int64(2719), np.int64(2727), np.int64(2738), np.int64(2751), np.int64(2752), np.int64(2764), np.int64(2765), np.int64(2855), np.int64(2865), np.int64(2882), np.int64(2956), np.int64(2962), np.int64(2978), np.int64(3056), np.int64(3113), np.int64(3144), np.int64(3209), np.int64(3216), np.int64(3375), np.int64(3401), np.int64(3402), np.int64(3404), np.int64(3513), np.int64(3523), np.int64(3562), np.int64(3594), np.int64(3599), np.int64(3697), np.int64(3711), np.int64(3735), np.int64(3779), np.int64(3793), np.int64(3825), np.int64(3838), np.int64(3890), np.int64(3939), np.int64(3944), np.int64(4022), np.int64(4075), np.int64(4101), np.int64(4235), np.int64(4263), np.int64(4268), np.int64(4357), np.int64(4388), np.int64(4425), np.int64(4427), np.int64(4449), np.int64(4504), np.int64(4519), np.int64(4538), np.int64(4557), np.int64(4566), np.int64(4582), np.int64(4615), np.int64(4621), np.int64(4711), np.int64(4713), np.int64(4719), np.int64(4768), np.int64(4795), np.int64(4832), np.int64(4859), np.int64(4868), np.int64(4871), np.int64(4882), np.int64(4893), np.int64(4925), np.int64(4997), np.int64(5033), np.int64(5053), np.int64(5057), np.int64(5075), np.int64(5085), np.int64(5096), np.int64(5104), np.int64(5118), np.int64(5125)]\n",
      "Predictions for Changed Indices: [9.90501940e-06 1.11469117e-05 1.24427370e-05 1.29364171e-05\n",
      " 1.30190801e-05 1.33531057e-05 1.34752618e-05 1.36843837e-05\n",
      " 1.39387985e-05 1.41674427e-05 1.41906321e-05 1.41915798e-05\n",
      " 1.42268937e-05 1.42596155e-05 1.42608260e-05 1.43096240e-05\n",
      " 1.44090691e-05 1.44584856e-05 1.45116410e-05 1.46015964e-05\n",
      " 1.46166421e-05 1.48119998e-05 1.48211002e-05 1.48961017e-05\n",
      " 1.50337064e-05 1.51649283e-05 1.52169941e-05 1.52677549e-05\n",
      " 1.52790435e-05 1.52917091e-05 1.53251567e-05 1.53735946e-05\n",
      " 1.53763522e-05 1.54125391e-05 1.55189246e-05 1.56381921e-05\n",
      " 1.57048780e-05 1.57079048e-05 1.57913219e-05 1.58212897e-05\n",
      " 1.58214407e-05 1.58417934e-05 1.58448765e-05 1.58947732e-05\n",
      " 1.59012779e-05 1.59091342e-05 1.59216434e-05 1.59235551e-05\n",
      " 1.59285664e-05 1.59485699e-05 1.59572410e-05 1.59756964e-05\n",
      " 1.60003055e-05 1.60298132e-05 1.60325035e-05 1.60655018e-05\n",
      " 1.60892814e-05 1.61009011e-05 1.61262724e-05 1.61263488e-05\n",
      " 1.61876105e-05 1.62361739e-05 1.62427405e-05 1.62526721e-05\n",
      " 1.62535871e-05 1.62802226e-05 1.62888427e-05 1.62978085e-05\n",
      " 1.63038239e-05 1.63090008e-05 1.63126733e-05 1.63128898e-05\n",
      " 1.63206714e-05 1.63467466e-05 1.63878904e-05 1.64133853e-05\n",
      " 1.64362555e-05 1.64636749e-05 1.64756930e-05 1.64764460e-05\n",
      " 1.65022193e-05 1.65194451e-05 1.65440870e-05 1.65449073e-05\n",
      " 1.65631409e-05 1.66169193e-05 1.66239715e-05 1.66529935e-05\n",
      " 1.66626978e-05 1.66747486e-05 1.67263825e-05 1.67390044e-05\n",
      " 1.67483613e-05 1.67537128e-05 1.67613198e-05 1.67634626e-05\n",
      " 1.67781764e-05 1.68105926e-05 1.68312054e-05 1.68316092e-05\n",
      " 1.68423503e-05 1.68499810e-05 1.68621664e-05 1.68878178e-05\n",
      " 1.69042687e-05 1.69270643e-05 1.69273535e-05 1.69297437e-05\n",
      " 1.69331670e-05 1.69339892e-05 1.69590076e-05 1.69703326e-05\n",
      " 1.69872856e-05 1.69908180e-05 1.69937830e-05 1.70004132e-05\n",
      " 1.70169587e-05 1.70174444e-05 1.70242765e-05 1.70507446e-05\n",
      " 1.70598869e-05 1.70785406e-05 1.71281918e-05 1.71350839e-05\n",
      " 1.71656211e-05 1.71820830e-05 1.71909651e-05 1.72017244e-05\n",
      " 1.72261680e-05 1.72375221e-05 1.72441323e-05]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [np.int64(918), np.int64(91), np.int64(36)]\n",
      "Total Number of Eagles the model believes are not eagles 14\n",
      "Total Number of images the model believes are not eagles 1667\n",
      "Change ratio: 0.0252\n"
     ]
    }
   ],
   "source": [
    "n_percentage = 5  # % of least confident eagle predictions to adjust\n",
    "max_iterations = 10 # has biggest numbers from above output  \n",
    "convergence_tolerance = 0  # stop if change ratio is this\n",
    "high_conf_threshold = 0.8  # threshold for confident eagle flips\n",
    "confidence_init = confidence_init = np.array([.35 if x<.5 else 1 for x in label_train]).reshape(5200,1)\n",
    "\n",
    "num_incorrectly_modified = 0\n",
    "num_modified = 0\n",
    "for iteration in range(max_iterations):\n",
    "    print(f\"Iteration {iteration + 1}...\")\n",
    "\n",
    "    # step 1: train model\n",
    "    history = model.fit(\n",
    "        [images_train, confidence_init],\n",
    "        label_train,\n",
    "        batch_size=100, # ok this might seem crazy but im wondering if w batch=32 it wasn't encountering enough wrong labels \n",
    "        epochs=1,\n",
    "        # validation_data=([images_val, np.ones((len(images_val), 1))], label_val), # this was a line from chat, replaced w ours instead below\n",
    "        validation_data=([images_val, label_val.reshape(-1,1)], label_val),\n",
    "        callbacks=callbacks,\n",
    "        shuffle = True,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # step 2: predict probabilities\n",
    "    preds = model.predict([images_train, confidence_init]).flatten()\n",
    "\n",
    "    # step 3: identify least confident eagle predictions\n",
    "    low_confidence_indices = np.where((label_train == 0) & (preds < 0.5) & (preds != 0))[0] # grabbing indices where label_train is 0 (noneagle), focusing in on the misclassified\n",
    "    filtered_indices = low_confidence_indices[confidence_init.flatten()[low_confidence_indices]!= 0]\n",
    "    sorted_indices = filtered_indices[np.argsort(preds[filtered_indices])] # sorts the preds low to high\n",
    "    to_adjust = sorted_indices[:int(len(sorted_indices) * (n_percentage / 100))] # only grabbing 5% rn of the bottom\n",
    "    \n",
    "\n",
    "    # step 4: update confidence for least confident predictions\n",
    "    if len(to_adjust) > 0:\n",
    "        confidence_init[to_adjust] = 0  # reduce confidence to 0 for the indices we picked by %\n",
    "        avg_confidence = np.mean(preds[to_adjust])\n",
    "        print(f\"Adjusted {len(to_adjust)} indices, avg confidence: {avg_confidence:.4f}\")\n",
    "    else:\n",
    "        print(\"No indices to adjust in this iteration.\")\n",
    "        \n",
    "    print(f\"Changed Indices: {sorted(to_adjust)}\")\n",
    "    print(f\"Predictions for Changed Indices: {preds[to_adjust]}\")\n",
    "    wrongly_switched = [x for x in changed_indices if x in to_adjust]\n",
    "    print(\"FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES:\", wrongly_switched)\n",
    "    num_incorrectly_modified += len(wrongly_switched)\n",
    "    num_modified += len(to_adjust)\n",
    "    print(\"Total Number of Eagles the model believes are not eagles\", num_incorrectly_modified)\n",
    "    print(\"Total Number of images the model believes are not eagles\", num_modified)\n",
    "\n",
    "\n",
    "    # step 5: check for convergence\n",
    "    if len(to_adjust) > 0:\n",
    "            change_ratio = len(to_adjust) / len(label_train)  \n",
    "    else:\n",
    "            change_ratio=0\n",
    "    print(f\"Change ratio: {change_ratio:.4f}\")\n",
    "\n",
    "    if change_ratio < convergence_tolerance:\n",
    "        print(\"Convergence reached.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "deb04844-8b14-4c62-836d-b47fadb743f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many actual eagles in top 10% is: 101\n",
      "How many total are in the top 10% is: 416\n"
     ]
    }
   ],
   "source": [
    "def top_n_percent_indices(predictions, n_percent):\n",
    "\n",
    "    target_indices = np.where(label_train == 0)[0]\n",
    "    filtered_preds = predictions[target_indices]\n",
    "    \n",
    "    # calculate the number of top elements to select\n",
    "    num_top_elements = int(np.ceil(len(filtered_preds) * n_percent / 100))\n",
    "    \n",
    "    # get the indices of the sorted values (descending order)\n",
    "    sorted_indices = np.argsort(filtered_preds)[::-1]\n",
    "    \n",
    "    # select the top n_percent indices\n",
    "    top_indices = sorted_indices[:num_top_elements]\n",
    "\n",
    "    top_original_indices = target_indices[top_indices]\n",
    "    \n",
    "    return top_original_indices\n",
    "n_percent = 10\n",
    "high_confidence_indices = top_n_percent_indices(preds,n_percent)\n",
    "high_confidence_indices\n",
    "actually_eagles = [x for x in high_confidence_indices if x in changed_indices]\n",
    "# print(\"The least confident not-eagles that are actually eagles:\", actually_eagles)\n",
    "print(f\"How many actual eagles in top {n_percent}% is: {len(actually_eagles)}\")\n",
    "print(f\"How many total are in the top {n_percent}% is: {len(high_confidence_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b058189-0d36-42ac-b896-628b8cd73b2d",
   "metadata": {},
   "source": [
    "The best I can seem to do is roughly approach a ratio of 25% being in a top n%\n",
    "    - If I have to guess why this is, it would be that as you do more runs or more epochs or take a larger/smaller percent of the top or bottom, the total number of eagles that get the \"0\" confidence stays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adacf22-c6e5-412b-bff9-05cc226e5b90",
   "metadata": {},
   "source": [
    "### riya and tomas 12/3 - playing around w diff paramaters\n",
    "### gonna try to do some sort of grid search for n_percentage, max_iterations, n_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "540183b1-651e-4892-8469-e8f8f8283d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1733286000.400430  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286000.445881  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286000.497832  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286000.510470  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286000.538291  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286000.564854  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286000.585577  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286000.624863  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286000.642509  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286001.569166  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286001.574266  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286001.580908  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286001.607206  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286001.625936  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286001.660102  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286001.670805  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286001.689827  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286001.712164  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286001.755115  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286001.767765  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286001.793480  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286001.827573  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286001.857844  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286001.877702  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286001.916666  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286001.965491  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  4/163\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 29ms/step  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1733286025.069175  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286025.072429  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286025.080163  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286025.089614  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286025.097400  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286025.104684  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286025.110613  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286025.119936  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286025.124869  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1733286031.689859  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286031.692640  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286031.697256  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286031.702152  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286031.706419  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286031.710123  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286031.713856  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286031.718506  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
      "W0000 00:00:1733286031.722301  774541 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted 832 indices, avg confidence: 0.1463\n",
      "Changed Indices: [15, 73, 91, 120, 158, 201, 249, 266, 306, 326, 329, 336, 351, 357, 368, 370, 378, 388, 428, 455, 465, 533, 534, 564, 578, 666, 672, 674, 684, 689, 698, 710, 786, 803, 822, 829, 830, 837, 839, 865, 882, 905, 913, 918, 926, 945, 947, 953, 973, 986, 1007, 1014, 1019, 1029, 1086, 1089, 1109, 1148, 1151, 1157, 1177, 1187, 1193, 1217, 1244, 1245, 1265, 1291, 1299, 1300, 1302, 1321, 1325, 1327, 1330, 1332, 1334, 1340, 1346, 1349, 1355, 1371, 1372, 1376, 1378, 1381, 1382, 1388, 1389, 1391, 1397, 1401, 1406, 1412, 1414, 1418, 1422, 1430, 1432, 1446, 1454, 1455, 1456, 1457, 1460, 1463, 1468, 1472, 1477, 1484, 1487, 1488, 1498, 1524, 1528, 1534, 1540, 1545, 1549, 1559, 1562, 1569, 1595, 1597, 1616, 1621, 1623, 1624, 1627, 1636, 1638, 1639, 1640, 1642, 1646, 1658, 1664, 1665, 1666, 1668, 1671, 1672, 1682, 1685, 1689, 1691, 1704, 1708, 1710, 1712, 1715, 1716, 1717, 1719, 1722, 1728, 1730, 1739, 1743, 1747, 1763, 1767, 1776, 1778, 1785, 1792, 1800, 1802, 1810, 1812, 1815, 1821, 1822, 1830, 1831, 1837, 1840, 1842, 1847, 1857, 1862, 1870, 1873, 1874, 1880, 1882, 1884, 1892, 1898, 1909, 1911, 1922, 1932, 1934, 1935, 1936, 1937, 1938, 1939, 1951, 1953, 1960, 1969, 1971, 1977, 1982, 1986, 1999, 2006, 2008, 2017, 2034, 2036, 2037, 2040, 2047, 2055, 2066, 2075, 2093, 2095, 2097, 2108, 2114, 2116, 2119, 2122, 2126, 2128, 2130, 2136, 2138, 2145, 2146, 2151, 2153, 2154, 2156, 2157, 2170, 2171, 2174, 2179, 2198, 2201, 2206, 2209, 2214, 2218, 2224, 2228, 2236, 2246, 2248, 2253, 2258, 2262, 2266, 2267, 2269, 2271, 2284, 2286, 2287, 2292, 2294, 2300, 2301, 2303, 2315, 2324, 2325, 2329, 2338, 2339, 2340, 2344, 2350, 2351, 2352, 2359, 2367, 2372, 2383, 2386, 2395, 2403, 2404, 2406, 2410, 2416, 2417, 2424, 2433, 2435, 2439, 2443, 2446, 2447, 2450, 2458, 2459, 2463, 2471, 2475, 2478, 2479, 2485, 2488, 2498, 2499, 2505, 2506, 2509, 2516, 2519, 2524, 2526, 2528, 2535, 2550, 2553, 2556, 2557, 2558, 2570, 2586, 2593, 2602, 2607, 2609, 2611, 2616, 2617, 2619, 2625, 2637, 2650, 2651, 2657, 2659, 2660, 2661, 2678, 2681, 2687, 2696, 2704, 2705, 2707, 2713, 2715, 2716, 2718, 2719, 2720, 2730, 2731, 2739, 2740, 2741, 2742, 2743, 2744, 2752, 2760, 2761, 2768, 2777, 2778, 2782, 2796, 2798, 2799, 2801, 2804, 2805, 2807, 2811, 2822, 2823, 2825, 2829, 2833, 2839, 2844, 2850, 2855, 2857, 2877, 2884, 2890, 2891, 2903, 2905, 2909, 2913, 2915, 2920, 2923, 2945, 2949, 2953, 2956, 2960, 2964, 2969, 2970, 2971, 2974, 2975, 2983, 2987, 2990, 2993, 2995, 2999, 3026, 3032, 3034, 3043, 3045, 3046, 3047, 3050, 3056, 3059, 3061, 3069, 3075, 3081, 3082, 3083, 3094, 3104, 3113, 3125, 3130, 3132, 3135, 3141, 3145, 3154, 3155, 3163, 3170, 3178, 3183, 3190, 3191, 3198, 3202, 3204, 3220, 3230, 3231, 3235, 3241, 3242, 3261, 3268, 3296, 3299, 3308, 3309, 3312, 3315, 3316, 3317, 3318, 3322, 3323, 3324, 3325, 3330, 3338, 3339, 3343, 3344, 3351, 3356, 3362, 3373, 3377, 3381, 3387, 3389, 3395, 3398, 3400, 3403, 3411, 3418, 3419, 3421, 3422, 3433, 3445, 3456, 3457, 3461, 3468, 3477, 3486, 3488, 3489, 3497, 3503, 3522, 3529, 3530, 3537, 3538, 3543, 3547, 3548, 3549, 3552, 3555, 3558, 3563, 3568, 3573, 3582, 3592, 3601, 3612, 3625, 3628, 3634, 3639, 3646, 3655, 3670, 3671, 3672, 3674, 3676, 3677, 3682, 3696, 3700, 3701, 3705, 3706, 3707, 3714, 3716, 3718, 3728, 3733, 3737, 3739, 3744, 3755, 3757, 3761, 3762, 3766, 3775, 3786, 3789, 3797, 3799, 3806, 3811, 3812, 3825, 3826, 3830, 3833, 3834, 3841, 3842, 3853, 3862, 3872, 3875, 3885, 3890, 3891, 3897, 3905, 3911, 3914, 3920, 3922, 3927, 3932, 3936, 3942, 3945, 3950, 3956, 3965, 3969, 3981, 3996, 4002, 4005, 4011, 4013, 4021, 4024, 4028, 4031, 4032, 4033, 4034, 4036, 4049, 4050, 4067, 4074, 4075, 4076, 4080, 4083, 4088, 4097, 4098, 4099, 4101, 4105, 4108, 4112, 4114, 4116, 4118, 4124, 4129, 4133, 4134, 4135, 4138, 4141, 4143, 4144, 4156, 4175, 4184, 4185, 4187, 4192, 4193, 4202, 4204, 4213, 4217, 4223, 4229, 4230, 4235, 4236, 4243, 4246, 4249, 4267, 4271, 4274, 4279, 4282, 4284, 4288, 4294, 4326, 4339, 4341, 4347, 4356, 4358, 4363, 4367, 4372, 4373, 4379, 4390, 4394, 4396, 4409, 4414, 4415, 4418, 4426, 4440, 4442, 4445, 4448, 4456, 4477, 4482, 4483, 4488, 4498, 4500, 4510, 4522, 4524, 4550, 4551, 4577, 4582, 4585, 4589, 4592, 4605, 4607, 4608, 4611, 4615, 4624, 4628, 4629, 4630, 4633, 4634, 4636, 4637, 4642, 4650, 4651, 4654, 4655, 4657, 4673, 4675, 4681, 4686, 4693, 4697, 4702, 4703, 4705, 4708, 4727, 4728, 4734, 4735, 4748, 4749, 4750, 4751, 4759, 4761, 4763, 4765, 4766, 4772, 4775, 4780, 4783, 4786, 4787, 4791, 4794, 4804, 4806, 4813, 4817, 4818, 4819, 4825, 4826, 4827, 4828, 4829, 4830, 4833, 4835, 4837, 4840, 4842, 4844, 4845, 4852, 4858, 4859, 4874, 4877, 4878, 4882, 4893, 4907, 4912, 4917, 4918, 4923, 4934, 4937, 4939, 4953, 4957, 4958, 4982, 4983, 4989, 4990, 4992, 4993, 5008, 5012, 5022, 5030, 5033, 5040, 5047, 5062, 5075, 5077, 5079, 5086, 5097, 5102, 5104, 5126, 5132, 5137, 5141, 5143, 5147, 5150, 5151, 5155, 5156, 5162, 5168, 5172, 5182, 5185, 5192, 5195, 5196]\n",
      "Predictions for Changed Indices: [0.01694514 0.02581174 0.03015035 0.03300071 0.03388599 0.04537833\n",
      " 0.04701019 0.04803725 0.04805419 0.05066967 0.05273295 0.05282561\n",
      " 0.05326954 0.06165349 0.06372709 0.06669655 0.06800205 0.06875116\n",
      " 0.07024726 0.07264149 0.07274039 0.07327169 0.07488477 0.07693076\n",
      " 0.07711196 0.07865817 0.08046716 0.08098479 0.08251212 0.08262155\n",
      " 0.08265212 0.08408795 0.0855613  0.0862864  0.08796231 0.08808159\n",
      " 0.08823882 0.08826631 0.08910097 0.09004895 0.09093753 0.09130768\n",
      " 0.09136763 0.0917757  0.09225572 0.09245602 0.09299512 0.09305998\n",
      " 0.09317248 0.09340738 0.09406317 0.09428128 0.094509   0.09458201\n",
      " 0.09469765 0.09607753 0.09632154 0.09727502 0.09747934 0.09779434\n",
      " 0.0983194  0.09907688 0.09978459 0.10012276 0.10016242 0.10022436\n",
      " 0.10041648 0.10050136 0.10064126 0.10070224 0.10170723 0.10184214\n",
      " 0.1023476  0.10263528 0.10334724 0.10338446 0.10363513 0.10408244\n",
      " 0.10427424 0.10440146 0.10448683 0.10489774 0.10503545 0.10509702\n",
      " 0.10522483 0.10618834 0.10626163 0.1077011  0.10812458 0.10812898\n",
      " 0.10868193 0.10911397 0.10922356 0.10930751 0.10969567 0.11059725\n",
      " 0.11103959 0.11107259 0.11121429 0.11156849 0.11173557 0.11176103\n",
      " 0.11212541 0.11264417 0.11265066 0.11273711 0.11295527 0.11301992\n",
      " 0.11314373 0.11335908 0.11352994 0.11461386 0.11563197 0.1163648\n",
      " 0.11646467 0.11665092 0.11670659 0.11688075 0.11729527 0.11789494\n",
      " 0.11794881 0.11807671 0.1182407  0.11837427 0.11839064 0.11840547\n",
      " 0.118543   0.11892862 0.11923861 0.11939144 0.11942102 0.1194596\n",
      " 0.11948208 0.11976007 0.11997168 0.12032025 0.12056229 0.12114426\n",
      " 0.12147068 0.12167819 0.12185467 0.12193356 0.12201471 0.12213869\n",
      " 0.12294275 0.12296835 0.12300605 0.12308559 0.12327173 0.12338836\n",
      " 0.12353009 0.12412424 0.12423677 0.12427217 0.12452724 0.12473174\n",
      " 0.12481193 0.12486929 0.12507    0.12511498 0.1252261  0.12531127\n",
      " 0.12555087 0.12565325 0.12574627 0.1262091  0.1264639  0.12685367\n",
      " 0.12748718 0.12783411 0.12813988 0.12826984 0.12832548 0.12847146\n",
      " 0.12849161 0.12863348 0.1286697  0.1287985  0.12895367 0.12902267\n",
      " 0.12902947 0.12905891 0.12924287 0.12933712 0.12935239 0.12952623\n",
      " 0.12959343 0.12969176 0.12973943 0.1299245  0.13004014 0.13032712\n",
      " 0.13037135 0.13088238 0.13098834 0.13110451 0.13123034 0.13154379\n",
      " 0.13160264 0.13161145 0.13181224 0.1318788  0.13204049 0.1321625\n",
      " 0.13267878 0.13273467 0.1329479  0.13330159 0.13356522 0.13374057\n",
      " 0.13377817 0.13377877 0.13424528 0.13428816 0.13451183 0.13463113\n",
      " 0.13477746 0.13493621 0.13504462 0.13506612 0.13508673 0.13508786\n",
      " 0.13517356 0.13563447 0.13615814 0.13630296 0.13649045 0.13653964\n",
      " 0.1365486  0.13656262 0.13664714 0.13699113 0.13727105 0.13733822\n",
      " 0.13758413 0.13802359 0.13812271 0.1381967  0.13819994 0.13827935\n",
      " 0.13831306 0.13842332 0.13851115 0.13853194 0.13859883 0.13864563\n",
      " 0.13866672 0.13872252 0.13879359 0.13915494 0.13928731 0.13929962\n",
      " 0.1393196  0.1395768  0.13975377 0.13976271 0.14004919 0.14030007\n",
      " 0.14079216 0.14096305 0.1409935  0.14118114 0.1412273  0.14124143\n",
      " 0.14127722 0.14155835 0.1416751  0.14173752 0.14182808 0.14193867\n",
      " 0.14194746 0.1421293  0.1421903  0.1422149  0.14222908 0.14228144\n",
      " 0.14244236 0.14244996 0.1425723  0.14266764 0.1427111  0.1428364\n",
      " 0.14286728 0.14304776 0.14317676 0.14319386 0.1432747  0.14327873\n",
      " 0.14335585 0.14336906 0.14339754 0.14354773 0.14357585 0.14369747\n",
      " 0.14371382 0.14371447 0.14379768 0.14384618 0.14388438 0.14394663\n",
      " 0.14405692 0.14406909 0.1441015  0.14413956 0.14437868 0.14442384\n",
      " 0.1445874  0.14458954 0.14461088 0.14466517 0.1446871  0.14469936\n",
      " 0.14485393 0.14499901 0.14510646 0.14526978 0.14531803 0.14541496\n",
      " 0.14544602 0.1454811  0.14557678 0.14581011 0.1458333  0.14584462\n",
      " 0.14594631 0.14603047 0.14605258 0.14607409 0.14607856 0.14611141\n",
      " 0.14656527 0.14673464 0.14685304 0.1468736  0.1469101  0.14694414\n",
      " 0.14701161 0.14702982 0.1470629  0.14711286 0.14775725 0.14789283\n",
      " 0.14801575 0.14807591 0.14816004 0.14831796 0.14845383 0.14845388\n",
      " 0.14858343 0.148646   0.14889623 0.1489018  0.14894508 0.14895852\n",
      " 0.14896135 0.1493544  0.14939474 0.14950237 0.14952753 0.14954638\n",
      " 0.1497097  0.14971392 0.14985335 0.15011512 0.15037908 0.15038711\n",
      " 0.15040125 0.15041393 0.15043437 0.15045482 0.15047526 0.15098286\n",
      " 0.150994   0.151036   0.15113693 0.151363   0.15149224 0.15157317\n",
      " 0.1516104  0.15161686 0.15177634 0.15194133 0.15194911 0.15196863\n",
      " 0.1519886  0.15201409 0.15212034 0.15223894 0.1522424  0.15232\n",
      " 0.15233186 0.15233795 0.15240122 0.15242518 0.15256247 0.15267627\n",
      " 0.15278408 0.15280631 0.15283868 0.15291427 0.15293576 0.15315358\n",
      " 0.15322648 0.15326683 0.15327087 0.15352418 0.15354444 0.15367058\n",
      " 0.15381712 0.15383206 0.15387103 0.15403408 0.15405585 0.15411767\n",
      " 0.15412013 0.15419105 0.15420952 0.1542301  0.15431611 0.15458046\n",
      " 0.1545976  0.15476044 0.1548154  0.15483934 0.15487826 0.15493372\n",
      " 0.15498322 0.15502866 0.1550369  0.15510726 0.15511476 0.15520558\n",
      " 0.15543005 0.15555803 0.15563034 0.15564941 0.1557874  0.15586194\n",
      " 0.15592188 0.15618178 0.15629128 0.15642639 0.15646932 0.15658805\n",
      " 0.15670642 0.15681294 0.15687954 0.15689568 0.15689644 0.15700722\n",
      " 0.1571263  0.15732425 0.15734811 0.15742384 0.15743576 0.15748966\n",
      " 0.15749092 0.15766095 0.15773213 0.15785672 0.15795828 0.15799439\n",
      " 0.15810141 0.15814383 0.15817147 0.15817831 0.15820953 0.15837769\n",
      " 0.15839732 0.15840217 0.15841223 0.15842949 0.15849587 0.15851527\n",
      " 0.1586804  0.15877397 0.15885034 0.15885228 0.15888505 0.1590118\n",
      " 0.15902069 0.15909708 0.1591297  0.15917023 0.1592555  0.15942983\n",
      " 0.15950395 0.15956436 0.15970385 0.1598541  0.15986319 0.15991545\n",
      " 0.15991648 0.15993276 0.16007349 0.16008131 0.16016527 0.16016635\n",
      " 0.1602761  0.16033658 0.16034332 0.16036    0.16045573 0.16050045\n",
      " 0.16056341 0.16061121 0.16062179 0.16064046 0.16083919 0.16085383\n",
      " 0.16087158 0.1609112  0.16100858 0.16100992 0.16103327 0.1610454\n",
      " 0.16112278 0.16112593 0.16119127 0.16123617 0.16128115 0.16129956\n",
      " 0.1613161  0.16132972 0.1613337  0.16139264 0.16158861 0.16160531\n",
      " 0.1617382  0.16176014 0.16177365 0.16177508 0.16180712 0.16181242\n",
      " 0.1619181  0.1619399  0.16197166 0.16198076 0.16198525 0.16200145\n",
      " 0.16200389 0.16211791 0.16230291 0.1623906  0.16241509 0.16248482\n",
      " 0.16251475 0.16256273 0.16263635 0.16268645 0.16270934 0.16287138\n",
      " 0.16302207 0.16304699 0.16309729 0.16315283 0.16328685 0.16335729\n",
      " 0.16340226 0.1634123  0.16347463 0.16351247 0.16357665 0.16363038\n",
      " 0.16371278 0.16377652 0.16381302 0.16384134 0.16391096 0.16391857\n",
      " 0.16406333 0.16409561 0.16413254 0.1641463  0.16414648 0.16417685\n",
      " 0.16419633 0.1642555  0.1642775  0.16437526 0.16445065 0.16457544\n",
      " 0.16461274 0.16463542 0.16464606 0.16468778 0.1647159  0.16471912\n",
      " 0.16474098 0.1647929  0.1647965  0.16488265 0.16495776 0.16507544\n",
      " 0.1650838  0.1651078  0.16535896 0.16541985 0.16545996 0.16549385\n",
      " 0.16550708 0.16565397 0.16593824 0.16604744 0.16613692 0.16622916\n",
      " 0.16636698 0.16640064 0.16649213 0.16661039 0.16662963 0.16664898\n",
      " 0.16675001 0.16678491 0.16679755 0.1668837  0.16704477 0.16722342\n",
      " 0.1672418  0.16726223 0.1672692  0.16730492 0.16735071 0.16736011\n",
      " 0.16737017 0.16740492 0.16756645 0.16761528 0.16766451 0.16766758\n",
      " 0.16772504 0.16773134 0.16774839 0.1677564  0.16785409 0.16787377\n",
      " 0.16798103 0.16807087 0.16810352 0.16825877 0.16841508 0.16843662\n",
      " 0.16852295 0.16858007 0.16861375 0.16871324 0.16879196 0.16880552\n",
      " 0.16882    0.16886137 0.1690257  0.16904017 0.16933782 0.16936094\n",
      " 0.16938438 0.16938645 0.16948321 0.16964468 0.16968536 0.16970256\n",
      " 0.1697585  0.1697637  0.16978875 0.16985641 0.16991666 0.17001441\n",
      " 0.17010155 0.17011869 0.17012094 0.1701417  0.1701801  0.17019872\n",
      " 0.1703897  0.1704107  0.17049892 0.17049918 0.170962   0.1709969\n",
      " 0.17106055 0.17112657 0.17116472 0.17126319 0.17128223 0.17131144\n",
      " 0.1713514  0.17137972 0.17138909 0.17144023 0.17144142 0.17145489\n",
      " 0.17151996 0.17155674 0.17175187 0.17178495 0.1718156  0.17187375\n",
      " 0.17196716 0.17202981 0.17210113 0.17212477 0.17219514 0.17228337\n",
      " 0.17228436 0.17232904 0.17233315 0.17236973 0.17241007 0.17247735\n",
      " 0.1725304  0.17256589 0.1725705  0.17258815 0.17259783 0.1726264\n",
      " 0.17264223 0.17264542 0.1726693  0.17273079 0.17274398 0.17274456\n",
      " 0.17279287 0.1728428  0.17286538 0.1730317  0.17305116 0.17305438\n",
      " 0.17305738 0.17309311 0.17315158 0.17321508 0.17322692 0.17323771\n",
      " 0.17325006 0.17330182 0.17337802 0.17339161 0.17341056 0.1734628\n",
      " 0.17356235 0.17359298 0.17362575 0.17366594 0.17367202 0.17369795\n",
      " 0.17378797 0.17379986 0.17390141 0.1739308  0.17395528 0.17397478\n",
      " 0.17398229 0.17407872 0.17409575 0.17410767 0.17415395 0.17415963\n",
      " 0.17422697 0.17428519 0.1743288  0.17433257 0.1743481  0.17438307\n",
      " 0.17447013 0.17453223 0.17458053 0.17460173 0.17460693 0.17474623\n",
      " 0.17480363 0.17480755 0.1748382  0.17490304 0.17493792 0.17495862\n",
      " 0.17499237 0.17499496 0.17501256 0.17501663 0.1750482  0.17508782\n",
      " 0.17511657 0.17515111 0.17521404 0.17523022 0.17523147 0.17527495\n",
      " 0.17528977 0.17537029 0.17542338 0.17546493 0.17549865 0.17554362\n",
      " 0.1756727  0.17571847 0.1757607  0.17578281 0.17580345 0.17581365\n",
      " 0.17591126 0.17593665 0.17598388 0.17608227 0.17614084 0.1761531\n",
      " 0.17615333 0.17625533 0.17627251 0.17627972 0.17628674 0.17634761\n",
      " 0.17637022 0.17640291 0.17648149 0.17649175 0.1765097  0.1765831\n",
      " 0.17661597 0.1766234  0.1766694  0.17668849 0.17673293 0.17677878\n",
      " 0.17678005 0.17679147 0.17681871 0.17684886 0.17688857 0.17690794\n",
      " 0.17691332 0.17696452 0.1769808  0.17699058]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [370, 986, 564, 918, 578, 837, 882, 822, 953, 666, 378, 249, 1014, 1151, 1187, 15, 905, 201, 803, 533, 1291, 913, 1299, 1265, 351, 865, 326, 973, 684, 91, 1029, 945, 926, 672, 1244, 534, 388, 120, 266, 455, 1217, 1086, 1109, 839, 1177, 357, 73, 947, 465, 786, 329, 1007, 1193, 428, 689, 1157, 829, 830, 336, 1089, 306, 368, 1148, 1019, 698, 674, 158, 710, 1245]\n",
      "Total Number of Eagles the model believes are not eagles 69\n",
      "Total Number of images the model believes are not eagles 832\n",
      "Change ratio: 0.1600\n",
      "Iteration 2...\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step\n",
      "Adjusted 665 indices, avg confidence: 0.0600\n",
      "Changed Indices: [95, 106, 160, 169, 193, 204, 298, 675, 734, 777, 810, 1050, 1201, 1223, 1268, 1301, 1303, 1305, 1307, 1314, 1315, 1317, 1335, 1339, 1364, 1369, 1373, 1374, 1380, 1404, 1407, 1415, 1421, 1423, 1431, 1435, 1436, 1438, 1441, 1442, 1450, 1451, 1461, 1462, 1476, 1483, 1490, 1500, 1501, 1512, 1513, 1515, 1516, 1532, 1533, 1536, 1537, 1544, 1547, 1548, 1558, 1567, 1570, 1572, 1574, 1580, 1585, 1587, 1590, 1596, 1606, 1609, 1611, 1613, 1622, 1628, 1637, 1644, 1657, 1667, 1669, 1673, 1674, 1676, 1683, 1695, 1696, 1698, 1702, 1703, 1705, 1709, 1718, 1727, 1736, 1745, 1755, 1768, 1772, 1777, 1780, 1786, 1788, 1809, 1814, 1819, 1826, 1835, 1836, 1838, 1841, 1843, 1851, 1861, 1863, 1868, 1875, 1876, 1887, 1890, 1891, 1893, 1900, 1902, 1903, 1910, 1915, 1943, 1949, 1950, 1954, 1955, 1956, 1961, 1970, 1973, 1975, 1978, 1979, 1981, 1983, 1997, 2014, 2025, 2027, 2044, 2048, 2050, 2053, 2071, 2074, 2086, 2102, 2105, 2112, 2127, 2129, 2132, 2133, 2142, 2147, 2165, 2166, 2176, 2178, 2183, 2191, 2194, 2199, 2208, 2213, 2215, 2222, 2229, 2230, 2261, 2264, 2272, 2279, 2285, 2295, 2296, 2302, 2309, 2311, 2314, 2316, 2328, 2335, 2347, 2348, 2353, 2354, 2355, 2364, 2370, 2384, 2387, 2391, 2393, 2400, 2401, 2408, 2409, 2411, 2422, 2426, 2427, 2429, 2431, 2441, 2442, 2474, 2482, 2483, 2486, 2487, 2496, 2500, 2504, 2508, 2512, 2518, 2520, 2525, 2538, 2542, 2545, 2551, 2554, 2563, 2575, 2577, 2589, 2613, 2614, 2620, 2628, 2634, 2641, 2662, 2666, 2669, 2673, 2675, 2682, 2686, 2697, 2699, 2711, 2717, 2724, 2727, 2753, 2754, 2755, 2756, 2757, 2759, 2763, 2769, 2787, 2788, 2791, 2803, 2813, 2830, 2831, 2835, 2838, 2840, 2848, 2849, 2860, 2861, 2867, 2868, 2879, 2882, 2885, 2893, 2894, 2896, 2898, 2908, 2911, 2914, 2925, 2933, 2939, 2940, 2959, 2976, 2988, 3002, 3011, 3018, 3023, 3025, 3029, 3039, 3042, 3049, 3064, 3068, 3072, 3076, 3077, 3090, 3103, 3115, 3120, 3133, 3137, 3152, 3157, 3160, 3166, 3184, 3185, 3189, 3193, 3197, 3200, 3201, 3207, 3213, 3215, 3219, 3221, 3223, 3228, 3233, 3257, 3262, 3263, 3274, 3277, 3278, 3281, 3282, 3283, 3284, 3302, 3303, 3305, 3306, 3328, 3329, 3333, 3334, 3336, 3337, 3347, 3357, 3360, 3368, 3376, 3390, 3394, 3406, 3408, 3409, 3410, 3413, 3416, 3420, 3426, 3434, 3438, 3446, 3447, 3451, 3459, 3475, 3478, 3479, 3490, 3492, 3493, 3500, 3502, 3508, 3509, 3513, 3519, 3523, 3534, 3535, 3556, 3559, 3565, 3567, 3591, 3608, 3618, 3621, 3622, 3623, 3632, 3633, 3635, 3638, 3643, 3649, 3652, 3653, 3669, 3675, 3678, 3684, 3685, 3686, 3699, 3709, 3711, 3715, 3731, 3742, 3756, 3764, 3776, 3783, 3788, 3793, 3796, 3801, 3803, 3813, 3815, 3818, 3835, 3836, 3837, 3840, 3844, 3845, 3851, 3856, 3867, 3870, 3879, 3887, 3888, 3917, 3918, 3928, 3930, 3937, 3958, 3968, 3977, 3984, 3988, 3991, 4009, 4018, 4025, 4027, 4035, 4037, 4040, 4048, 4051, 4053, 4062, 4063, 4065, 4084, 4085, 4090, 4091, 4093, 4094, 4095, 4107, 4111, 4128, 4139, 4145, 4154, 4155, 4157, 4162, 4164, 4177, 4188, 4200, 4208, 4219, 4226, 4234, 4239, 4240, 4254, 4259, 4261, 4265, 4266, 4278, 4283, 4289, 4298, 4300, 4301, 4316, 4318, 4330, 4331, 4335, 4336, 4342, 4346, 4352, 4355, 4365, 4376, 4382, 4386, 4387, 4402, 4404, 4405, 4406, 4407, 4413, 4417, 4419, 4422, 4424, 4430, 4433, 4444, 4450, 4453, 4454, 4455, 4458, 4462, 4470, 4473, 4475, 4487, 4491, 4492, 4501, 4502, 4507, 4512, 4514, 4519, 4525, 4531, 4535, 4536, 4547, 4549, 4566, 4568, 4569, 4575, 4576, 4578, 4580, 4583, 4586, 4591, 4597, 4612, 4616, 4625, 4638, 4641, 4643, 4653, 4661, 4662, 4668, 4677, 4687, 4689, 4692, 4701, 4704, 4709, 4720, 4732, 4768, 4774, 4781, 4782, 4789, 4812, 4832, 4838, 4850, 4851, 4854, 4856, 4857, 4867, 4872, 4879, 4883, 4889, 4897, 4899, 4903, 4910, 4911, 4915, 4916, 4926, 4928, 4932, 4940, 4943, 4945, 4947, 4949, 4951, 4959, 4960, 4967, 4969, 4975, 4978, 4979, 4996, 4997, 5018, 5019, 5020, 5021, 5026, 5043, 5049, 5053, 5057, 5064, 5068, 5069, 5072, 5074, 5078, 5103, 5105, 5106, 5107, 5116, 5117, 5118, 5128, 5131, 5134, 5135, 5138, 5139, 5157, 5167, 5169, 5170, 5180, 5181, 5189]\n",
      "Predictions for Changed Indices: [0.03281756 0.03477775 0.03798406 0.03863531 0.04047747 0.04256749\n",
      " 0.0432607  0.04420732 0.04451834 0.04575427 0.0458946  0.04646275\n",
      " 0.04651359 0.04709512 0.04765128 0.04774873 0.0478419  0.04852087\n",
      " 0.04880232 0.04898259 0.04916974 0.04944508 0.04946696 0.04966502\n",
      " 0.04968438 0.04980546 0.04984805 0.05012333 0.05020554 0.05025266\n",
      " 0.05025779 0.05027657 0.05033034 0.05036289 0.05036763 0.05038131\n",
      " 0.05038768 0.05039472 0.05043476 0.0505029  0.05057727 0.05064286\n",
      " 0.0507415  0.0507664  0.0508116  0.05084551 0.05088851 0.05103362\n",
      " 0.05106883 0.05114057 0.05136649 0.05150035 0.05152388 0.05152625\n",
      " 0.05153067 0.05158782 0.0518476  0.05202644 0.05215301 0.05228826\n",
      " 0.05232592 0.05250075 0.05251115 0.05254579 0.05258051 0.0526775\n",
      " 0.05268914 0.0527847  0.05278622 0.05281841 0.05293072 0.05305805\n",
      " 0.05306306 0.05308815 0.05313702 0.05316879 0.05317484 0.0531764\n",
      " 0.05324036 0.05330455 0.05345477 0.0535538  0.05359535 0.05368119\n",
      " 0.05375284 0.05377679 0.05384032 0.05392434 0.05394037 0.05397897\n",
      " 0.05399118 0.05408907 0.05419176 0.05420131 0.05425362 0.05427886\n",
      " 0.05430993 0.05434268 0.05438506 0.05440202 0.05440925 0.05443021\n",
      " 0.05446015 0.05446332 0.05447244 0.0546033  0.05461913 0.05465548\n",
      " 0.05465992 0.05472016 0.05478691 0.05488264 0.05493661 0.05495303\n",
      " 0.054959   0.05496854 0.05497184 0.05498952 0.05499282 0.05508277\n",
      " 0.05512544 0.05516411 0.05519337 0.05525146 0.05525534 0.05525679\n",
      " 0.05526375 0.05527088 0.05538512 0.05544188 0.05553323 0.05560188\n",
      " 0.05569167 0.0557032  0.055709   0.05575291 0.05577453 0.05586437\n",
      " 0.05589953 0.05591244 0.05597384 0.05599196 0.05606617 0.05608318\n",
      " 0.05612104 0.05612963 0.05613977 0.05615498 0.05615658 0.0561648\n",
      " 0.05616967 0.05627888 0.0563332  0.05637879 0.05638055 0.05639302\n",
      " 0.05639565 0.05640969 0.05642638 0.05643006 0.05649102 0.05650297\n",
      " 0.05653354 0.05655193 0.05663185 0.05663861 0.0566847  0.05669094\n",
      " 0.05669655 0.05670281 0.05677067 0.05679639 0.05680422 0.05683107\n",
      " 0.056947   0.05695637 0.05698859 0.05703836 0.05705206 0.05705655\n",
      " 0.05708316 0.05712946 0.05721296 0.05732061 0.05733494 0.05740774\n",
      " 0.05742854 0.05743367 0.05745612 0.05746959 0.05748969 0.05755736\n",
      " 0.05757333 0.05759755 0.05759845 0.05760406 0.05763636 0.05768129\n",
      " 0.05769154 0.05770228 0.05771023 0.05771895 0.05779729 0.05781226\n",
      " 0.05783103 0.05783568 0.05787021 0.05789153 0.05791317 0.05795653\n",
      " 0.05800197 0.05801744 0.05803581 0.05804101 0.05808193 0.05810077\n",
      " 0.058139   0.05819955 0.05823763 0.05824229 0.05828064 0.05828678\n",
      " 0.05830479 0.05833807 0.05835951 0.05840416 0.05840972 0.05842464\n",
      " 0.05842926 0.05843025 0.05844183 0.05845627 0.05845765 0.05851549\n",
      " 0.05858674 0.05861161 0.05863662 0.05865809 0.05869849 0.05871784\n",
      " 0.05876455 0.05878127 0.05889523 0.05890447 0.05893072 0.05899041\n",
      " 0.05904816 0.05905165 0.05906747 0.05907052 0.0591073  0.05912874\n",
      " 0.05913735 0.05915212 0.05923915 0.05926501 0.05934289 0.05934333\n",
      " 0.05937583 0.05937628 0.05937941 0.05939739 0.05941416 0.05943372\n",
      " 0.05943601 0.05944368 0.05947026 0.05948368 0.05948592 0.05951772\n",
      " 0.05953678 0.05955074 0.05958337 0.0596244  0.05967145 0.05971915\n",
      " 0.05975468 0.05977986 0.05981479 0.05983735 0.05983797 0.05984501\n",
      " 0.05985149 0.05985863 0.05987596 0.05989111 0.05989454 0.05991074\n",
      " 0.05992597 0.05993245 0.05993712 0.06001452 0.06003141 0.06003152\n",
      " 0.06005336 0.06005558 0.06012047 0.06015325 0.06018612 0.06021228\n",
      " 0.06021444 0.06023866 0.06030491 0.06031831 0.06034439 0.06035384\n",
      " 0.0603646  0.06043729 0.06044896 0.06044974 0.06046066 0.06046396\n",
      " 0.06046689 0.06049231 0.06056976 0.06057738 0.0606033  0.06065152\n",
      " 0.06066839 0.06069365 0.06073611 0.06073928 0.06074456 0.06075154\n",
      " 0.06075349 0.0607863  0.06078701 0.06079378 0.06079398 0.06080402\n",
      " 0.06080904 0.06083537 0.06090592 0.06096124 0.06097752 0.060978\n",
      " 0.06099581 0.06100968 0.0610318  0.0610381  0.06104264 0.0610752\n",
      " 0.06108389 0.0611087  0.06110919 0.06112092 0.06112278 0.06112387\n",
      " 0.06115012 0.06116214 0.06119077 0.06121389 0.06123515 0.06127129\n",
      " 0.06131312 0.06132076 0.06132118 0.06133978 0.06135527 0.06135983\n",
      " 0.06142088 0.06143336 0.06145283 0.0614575  0.06146578 0.06151835\n",
      " 0.0615219  0.06153208 0.06157374 0.06157937 0.06158447 0.06161316\n",
      " 0.06162149 0.06170471 0.06174344 0.06175075 0.06180022 0.06181704\n",
      " 0.06182418 0.06182439 0.06183185 0.06187848 0.06188947 0.06189081\n",
      " 0.06189575 0.06193855 0.06196357 0.0620064  0.06201047 0.06212523\n",
      " 0.06214632 0.06220464 0.06221602 0.06224463 0.06225455 0.0622556\n",
      " 0.06226625 0.06228379 0.0622932  0.06229563 0.06230283 0.0623046\n",
      " 0.0623505  0.06236169 0.06244218 0.0624456  0.06245357 0.06246994\n",
      " 0.06249888 0.06250321 0.06250885 0.06252147 0.06252442 0.0625258\n",
      " 0.06252937 0.06255613 0.06256074 0.06256187 0.06260743 0.06261294\n",
      " 0.0626709  0.06267915 0.06271271 0.06271414 0.06273994 0.06274245\n",
      " 0.06274924 0.06275863 0.06278268 0.06278888 0.06279165 0.06284889\n",
      " 0.06287077 0.06287532 0.06287962 0.06289213 0.06291022 0.06297716\n",
      " 0.06298421 0.06298476 0.06300044 0.06305175 0.06305584 0.06306084\n",
      " 0.06307657 0.0630811  0.06308135 0.06311154 0.06311502 0.06312593\n",
      " 0.06316037 0.0632221  0.06323982 0.06325127 0.06330374 0.06332158\n",
      " 0.06334331 0.06334388 0.06334852 0.06335211 0.06338064 0.06338155\n",
      " 0.0634545  0.06345809 0.06348416 0.06353273 0.06355686 0.06355866\n",
      " 0.06356209 0.06357115 0.06359513 0.06363743 0.0637081  0.06371724\n",
      " 0.0637829  0.06378461 0.06381662 0.06383017 0.06384005 0.0638774\n",
      " 0.06389685 0.06390031 0.06393071 0.06394043 0.06398136 0.06398979\n",
      " 0.06400128 0.06401294 0.06401639 0.06402231 0.06404422 0.06405602\n",
      " 0.06407762 0.06409852 0.06412233 0.06413723 0.06415796 0.06418639\n",
      " 0.0641956  0.06423725 0.06425928 0.06430057 0.06432648 0.06436627\n",
      " 0.06437012 0.06437795 0.06439379 0.06440394 0.0644291  0.06443557\n",
      " 0.06446352 0.06446845 0.06452428 0.06452788 0.06453202 0.06453223\n",
      " 0.06455529 0.06458646 0.0646197  0.06467301 0.06470264 0.06470361\n",
      " 0.06472822 0.06473681 0.06475957 0.06480176 0.06480382 0.06482146\n",
      " 0.06483085 0.06487874 0.0649246  0.06496932 0.06498178 0.06503803\n",
      " 0.06504639 0.06507932 0.06508694 0.065097   0.06510182 0.06510987\n",
      " 0.06515474 0.06518122 0.06519017 0.06521908 0.06522176 0.06530187\n",
      " 0.06531615 0.06533884 0.06534275 0.06535146 0.06535628 0.06536069\n",
      " 0.06538595 0.06539515 0.06539854 0.06540002 0.06541223 0.06541906\n",
      " 0.06542628 0.06546916 0.06556831 0.06557588 0.06559554 0.06561498\n",
      " 0.06561549 0.06562271 0.06563957 0.0656619  0.06572895 0.06575238\n",
      " 0.06577637 0.06582841 0.06583464 0.06584278 0.0658927  0.06590544\n",
      " 0.0659231  0.06598438 0.06601582 0.0660349  0.06604698 0.06605484\n",
      " 0.06606172 0.06606863 0.0660694  0.06608079 0.06610766 0.06611012\n",
      " 0.06612045 0.06612625 0.06614014 0.06615222 0.06618999 0.0661902\n",
      " 0.06619462 0.06620461 0.06621687 0.06625091 0.06626214 0.06627002\n",
      " 0.06630293 0.06632478 0.06640156 0.06647376 0.0664779  0.06654593\n",
      " 0.06655343 0.06655426 0.06657238 0.06666602 0.06667728 0.06668691\n",
      " 0.06672877 0.06674163 0.06678507 0.0668296  0.06684694 0.06685422\n",
      " 0.06691243 0.06692068 0.06697322 0.06698744 0.06699709 0.06701213\n",
      " 0.06701422 0.06705014 0.06705107 0.06708135 0.06709266 0.06709379\n",
      " 0.0671066  0.06715748 0.06720023 0.06723543 0.06724679 0.06731577\n",
      " 0.06732583 0.06733762 0.06734485 0.0673538  0.06738289 0.06740421\n",
      " 0.06742223 0.06745476 0.06745972 0.06747379 0.06751462 0.06753879\n",
      " 0.0675706  0.06760522 0.06761905 0.06762801 0.06763246 0.06763728\n",
      " 0.06764066 0.0676605  0.06769967 0.06772449 0.06772719 0.06773368\n",
      " 0.06775638 0.06779329 0.06782081 0.06782595 0.06783927]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [1050, 675, 1223, 160, 106, 204, 777, 95, 298, 1268, 810, 1201, 734, 169, 193]\n",
      "Total Number of Eagles the model believes are not eagles 84\n",
      "Total Number of images the model believes are not eagles 1497\n",
      "Change ratio: 0.1279\n",
      "Iteration 3...\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step\n",
      "Adjusted 532 indices, avg confidence: 0.0323\n",
      "Changed Indices: [10, 18, 123, 665, 731, 738, 811, 850, 859, 877, 1103, 1308, 1309, 1312, 1320, 1324, 1328, 1348, 1357, 1359, 1363, 1367, 1370, 1377, 1383, 1396, 1420, 1429, 1440, 1443, 1448, 1470, 1489, 1496, 1499, 1502, 1518, 1525, 1526, 1551, 1554, 1565, 1566, 1579, 1593, 1599, 1600, 1615, 1618, 1626, 1630, 1631, 1643, 1648, 1659, 1661, 1670, 1678, 1699, 1724, 1733, 1744, 1750, 1759, 1769, 1770, 1789, 1795, 1796, 1811, 1813, 1825, 1852, 1856, 1864, 1869, 1871, 1877, 1881, 1895, 1897, 1899, 1906, 1907, 1914, 1919, 1920, 1921, 1927, 1929, 1930, 1963, 1964, 1965, 1974, 1980, 1990, 1993, 2000, 2001, 2018, 2020, 2024, 2031, 2032, 2035, 2041, 2046, 2061, 2064, 2077, 2079, 2087, 2094, 2099, 2100, 2103, 2106, 2109, 2110, 2115, 2121, 2131, 2135, 2139, 2144, 2160, 2168, 2173, 2175, 2181, 2184, 2190, 2202, 2203, 2205, 2210, 2219, 2225, 2231, 2239, 2247, 2249, 2254, 2257, 2277, 2278, 2280, 2283, 2297, 2299, 2310, 2317, 2320, 2331, 2332, 2337, 2373, 2389, 2390, 2394, 2396, 2397, 2413, 2430, 2438, 2448, 2449, 2464, 2480, 2493, 2497, 2502, 2503, 2527, 2532, 2547, 2548, 2559, 2562, 2564, 2568, 2569, 2584, 2592, 2600, 2603, 2612, 2621, 2633, 2647, 2656, 2658, 2667, 2680, 2688, 2689, 2691, 2694, 2695, 2702, 2703, 2708, 2748, 2762, 2771, 2774, 2786, 2793, 2797, 2802, 2809, 2812, 2818, 2820, 2824, 2842, 2845, 2856, 2859, 2870, 2871, 2875, 2904, 2910, 2912, 2934, 2941, 2952, 2957, 2962, 2965, 2972, 2982, 2991, 2992, 3000, 3001, 3009, 3013, 3017, 3020, 3028, 3062, 3063, 3070, 3073, 3074, 3079, 3080, 3093, 3099, 3101, 3105, 3108, 3117, 3118, 3122, 3124, 3129, 3138, 3159, 3165, 3176, 3180, 3199, 3214, 3217, 3248, 3249, 3254, 3267, 3290, 3307, 3310, 3340, 3342, 3348, 3353, 3354, 3364, 3374, 3385, 3386, 3402, 3405, 3437, 3441, 3442, 3443, 3452, 3454, 3455, 3460, 3470, 3472, 3476, 3481, 3495, 3506, 3510, 3521, 3527, 3533, 3539, 3540, 3544, 3550, 3566, 3577, 3578, 3579, 3602, 3605, 3606, 3617, 3626, 3630, 3641, 3656, 3668, 3680, 3681, 3683, 3690, 3691, 3693, 3704, 3730, 3732, 3745, 3763, 3765, 3770, 3779, 3781, 3795, 3800, 3802, 3805, 3829, 3860, 3864, 3871, 3876, 3880, 3881, 3899, 3921, 3923, 3926, 3947, 3957, 3972, 3979, 3986, 3987, 4008, 4023, 4026, 4038, 4041, 4043, 4045, 4057, 4058, 4064, 4070, 4071, 4079, 4081, 4082, 4086, 4087, 4096, 4117, 4119, 4122, 4137, 4151, 4158, 4160, 4183, 4186, 4196, 4199, 4205, 4207, 4212, 4222, 4241, 4242, 4252, 4253, 4255, 4263, 4268, 4272, 4309, 4311, 4317, 4320, 4323, 4337, 4338, 4345, 4354, 4359, 4388, 4389, 4399, 4403, 4410, 4416, 4420, 4431, 4437, 4449, 4459, 4465, 4467, 4495, 4496, 4503, 4505, 4506, 4509, 4516, 4520, 4530, 4533, 4534, 4537, 4539, 4542, 4544, 4553, 4558, 4571, 4574, 4593, 4596, 4603, 4614, 4621, 4626, 4632, 4640, 4656, 4658, 4659, 4660, 4663, 4672, 4680, 4690, 4696, 4715, 4722, 4725, 4736, 4738, 4743, 4745, 4755, 4757, 4758, 4776, 4777, 4778, 4785, 4793, 4797, 4799, 4811, 4853, 4860, 4863, 4870, 4880, 4894, 4898, 4920, 4925, 4931, 4935, 4952, 4964, 4974, 4976, 4995, 4999, 5000, 5001, 5004, 5006, 5010, 5014, 5027, 5031, 5035, 5039, 5044, 5052, 5058, 5070, 5071, 5073, 5082, 5083, 5085, 5087, 5090, 5091, 5093, 5096, 5109, 5111, 5114, 5115, 5123, 5127, 5142, 5144, 5158, 5165, 5178, 5184, 5187, 5188, 5190, 5197]\n",
      "Predictions for Changed Indices: [0.02656695 0.02713931 0.02755603 0.02796793 0.02805499 0.02811271\n",
      " 0.02825825 0.02826587 0.02828159 0.02836966 0.02846802 0.02857652\n",
      " 0.02869342 0.02870949 0.02880513 0.02886307 0.02887192 0.02892771\n",
      " 0.02895657 0.02910369 0.02911424 0.02911569 0.02920198 0.02931523\n",
      " 0.02950643 0.0295204  0.02954742 0.02955173 0.0295675  0.02958881\n",
      " 0.02971439 0.02973196 0.02975246 0.02977254 0.02979705 0.02979723\n",
      " 0.02981973 0.0298305  0.02993349 0.0300057  0.03000909 0.03006792\n",
      " 0.03009172 0.03014732 0.03016192 0.03018674 0.03024155 0.03028646\n",
      " 0.03031308 0.03033371 0.03038607 0.03041517 0.03042156 0.03042196\n",
      " 0.03042453 0.03044731 0.03045039 0.03045184 0.03046222 0.03046681\n",
      " 0.03050453 0.03051351 0.03053032 0.03053353 0.0305441  0.03055987\n",
      " 0.03057037 0.03057526 0.0305986  0.03060256 0.03060642 0.03063906\n",
      " 0.03065883 0.03066114 0.03066552 0.03069849 0.03070002 0.03070363\n",
      " 0.03071135 0.03071911 0.03072251 0.03072386 0.03073907 0.03076795\n",
      " 0.0307688  0.03077489 0.03077601 0.03078255 0.03078368 0.03080224\n",
      " 0.03082412 0.03083062 0.03083246 0.03083478 0.03085363 0.03086844\n",
      " 0.03087414 0.03088024 0.03089096 0.03090962 0.0309308  0.03093094\n",
      " 0.03093358 0.03093544 0.03095734 0.03096648 0.03097023 0.03098311\n",
      " 0.03098786 0.03100032 0.03100562 0.03102642 0.03103853 0.03104346\n",
      " 0.03105512 0.03106929 0.03109095 0.03111473 0.03115518 0.03118083\n",
      " 0.03120078 0.03120839 0.03121452 0.03122493 0.03123183 0.031234\n",
      " 0.03123762 0.03124384 0.03125302 0.0312634  0.03127122 0.03127849\n",
      " 0.03129154 0.03130696 0.03131055 0.0313166  0.0313172  0.031324\n",
      " 0.03132532 0.03133181 0.03135484 0.03137979 0.03139313 0.03139907\n",
      " 0.03140327 0.0314157  0.03143837 0.03145663 0.03147592 0.03147659\n",
      " 0.03147834 0.03147882 0.03148434 0.0314844  0.03149057 0.03149654\n",
      " 0.03151146 0.03152228 0.03152363 0.03152791 0.03153539 0.03154133\n",
      " 0.03154615 0.03158515 0.03159519 0.03160419 0.03161633 0.03161717\n",
      " 0.03162691 0.03163713 0.03166303 0.03167394 0.03168524 0.03168584\n",
      " 0.03168741 0.0316875  0.03169667 0.03172284 0.0317242  0.03174664\n",
      " 0.03174909 0.03175595 0.03176567 0.03178052 0.03180402 0.03180518\n",
      " 0.03181521 0.03182045 0.03182624 0.03183572 0.0318533  0.03186084\n",
      " 0.03186566 0.03187533 0.03188365 0.03188818 0.03189595 0.0318997\n",
      " 0.0319009  0.03190439 0.03190789 0.03192093 0.03193438 0.03195417\n",
      " 0.03196476 0.03197841 0.03197989 0.03200305 0.03201475 0.03201707\n",
      " 0.03202676 0.0320307  0.03204388 0.03204977 0.03205346 0.03205498\n",
      " 0.03205739 0.03205784 0.0320616  0.032076   0.03209661 0.03211027\n",
      " 0.03211381 0.03211613 0.03212506 0.03212645 0.03213462 0.03213585\n",
      " 0.03216009 0.03216065 0.03217003 0.03217392 0.032184   0.03218862\n",
      " 0.03219775 0.032205   0.03222637 0.03222764 0.03224371 0.03224738\n",
      " 0.03225302 0.03226277 0.03226326 0.03226697 0.0322723  0.03227556\n",
      " 0.03227564 0.03228581 0.0323068  0.03231768 0.03232233 0.03234848\n",
      " 0.03235335 0.0323662  0.03237747 0.03240633 0.03240951 0.03241182\n",
      " 0.03241245 0.03242199 0.03243303 0.0324503  0.032461   0.03247159\n",
      " 0.03247393 0.03247918 0.03248086 0.03248659 0.03249548 0.03250907\n",
      " 0.03254159 0.03254894 0.03255961 0.03257321 0.03258171 0.03258695\n",
      " 0.03259113 0.03259521 0.03260071 0.03262144 0.03262212 0.03264652\n",
      " 0.03267561 0.03268354 0.03268809 0.03269355 0.0326953  0.0327123\n",
      " 0.03271484 0.03272496 0.03273034 0.03273052 0.0327342  0.03273669\n",
      " 0.03273878 0.03274099 0.03274755 0.03275252 0.03275713 0.03276038\n",
      " 0.03276518 0.03278027 0.03278957 0.03279449 0.0327985  0.03283627\n",
      " 0.03284519 0.0328485  0.03284968 0.03285051 0.03285285 0.03286935\n",
      " 0.03287496 0.03288531 0.03289119 0.03289214 0.03292103 0.03292241\n",
      " 0.03292597 0.03293796 0.03294568 0.03295532 0.03295816 0.03296236\n",
      " 0.03297622 0.03299224 0.03299648 0.03299758 0.0329994  0.03300228\n",
      " 0.03301024 0.03301643 0.03302807 0.03303084 0.03305918 0.03307088\n",
      " 0.03308012 0.03308079 0.03309149 0.03309273 0.03309647 0.03310173\n",
      " 0.03310221 0.03311562 0.03311751 0.03312055 0.03313658 0.03314462\n",
      " 0.03314641 0.03314939 0.0331577  0.03315884 0.03316202 0.03316794\n",
      " 0.03318392 0.03319893 0.03320588 0.03320758 0.03321399 0.03322221\n",
      " 0.03323179 0.03323507 0.03324238 0.03324269 0.0332448  0.03325095\n",
      " 0.03326056 0.03326487 0.03328485 0.03328632 0.03330056 0.03330595\n",
      " 0.03331202 0.03331777 0.03331792 0.03333907 0.03333975 0.03334722\n",
      " 0.03336881 0.03337268 0.03338411 0.03340735 0.03341371 0.03342199\n",
      " 0.0334274  0.03343935 0.0334413  0.03344693 0.033447   0.03344909\n",
      " 0.033454   0.03345688 0.03346997 0.03347794 0.03348615 0.03349502\n",
      " 0.03350529 0.03351455 0.03352537 0.03353777 0.03353906 0.03354658\n",
      " 0.03355394 0.03356346 0.03356546 0.03358037 0.03358545 0.03358882\n",
      " 0.03360814 0.03360875 0.03360964 0.0336105  0.03361872 0.0336216\n",
      " 0.03362162 0.0336297  0.03364185 0.03364306 0.03365721 0.03366635\n",
      " 0.03367672 0.03368049 0.03369443 0.03370328 0.03371945 0.03372018\n",
      " 0.03373043 0.03374399 0.03374418 0.03374896 0.03376824 0.03376858\n",
      " 0.03378061 0.03378601 0.03379632 0.03379664 0.03379799 0.03380294\n",
      " 0.03380302 0.03381244 0.03381617 0.03381952 0.0338283  0.03383657\n",
      " 0.0338391  0.03384436 0.03384452 0.03386173 0.03387374 0.03387618\n",
      " 0.03387955 0.03392136 0.03393739 0.0339639  0.03396692 0.03397145\n",
      " 0.03398368 0.03398771 0.03398781 0.0339924  0.03401593 0.03404751\n",
      " 0.0340528  0.03405499 0.03407818 0.03408053 0.0340925  0.03410511\n",
      " 0.03410628 0.03411064 0.03411508 0.03411728 0.03413697 0.03413744\n",
      " 0.03413754 0.03414845 0.03415085 0.03415663 0.03415824 0.03415948\n",
      " 0.03418548 0.03419133 0.03419527 0.03419799 0.03420075 0.03420955\n",
      " 0.03421055 0.0342118  0.034212   0.03421277 0.03421864 0.03422618\n",
      " 0.03422906 0.03424929 0.03427188 0.03427286 0.03427831 0.03428181\n",
      " 0.03429135 0.03429806 0.03430196 0.03431336 0.03432022 0.0343333\n",
      " 0.03434021 0.03434525 0.03436381 0.03436745 0.03437226 0.03438288\n",
      " 0.03439944 0.03440267 0.03440368 0.03440793 0.03441038 0.03442279\n",
      " 0.03442497 0.0344344  0.03443703 0.03444412 0.03446303 0.0344644\n",
      " 0.03448021 0.03450415 0.03453319 0.03454756 0.0345603  0.03456486\n",
      " 0.03457854 0.03459103 0.03460618 0.03461555]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [731, 10, 850, 877, 859, 123, 1103, 738, 665, 18, 811]\n",
      "Total Number of Eagles the model believes are not eagles 95\n",
      "Total Number of images the model believes are not eagles 2029\n",
      "Change ratio: 0.1023\n",
      "Iteration 4...\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step\n",
      "Adjusted 426 indices, avg confidence: 0.0233\n",
      "Changed Indices: [138, 149, 228, 341, 377, 818, 923, 1083, 1200, 1316, 1329, 1331, 1336, 1338, 1353, 1360, 1375, 1379, 1386, 1392, 1393, 1400, 1424, 1426, 1459, 1466, 1471, 1491, 1494, 1535, 1552, 1556, 1573, 1575, 1577, 1581, 1594, 1598, 1607, 1608, 1620, 1645, 1649, 1660, 1679, 1688, 1714, 1720, 1721, 1734, 1737, 1748, 1751, 1753, 1757, 1762, 1779, 1781, 1782, 1783, 1793, 1797, 1799, 1805, 1817, 1823, 1832, 1833, 1839, 1854, 1855, 1879, 1905, 1913, 1916, 1917, 1925, 1942, 1946, 1952, 1958, 1962, 1966, 1985, 1994, 1996, 2015, 2016, 2030, 2045, 2057, 2065, 2068, 2069, 2070, 2076, 2081, 2084, 2140, 2169, 2177, 2185, 2186, 2188, 2192, 2227, 2232, 2243, 2251, 2255, 2265, 2268, 2270, 2273, 2291, 2308, 2318, 2319, 2342, 2377, 2380, 2385, 2388, 2402, 2405, 2445, 2462, 2465, 2467, 2477, 2481, 2489, 2491, 2513, 2523, 2536, 2539, 2541, 2544, 2546, 2549, 2555, 2579, 2580, 2587, 2596, 2615, 2623, 2627, 2629, 2631, 2635, 2648, 2653, 2655, 2665, 2670, 2674, 2706, 2709, 2721, 2745, 2767, 2773, 2779, 2792, 2800, 2816, 2836, 2837, 2847, 2851, 2869, 2876, 2900, 2901, 2919, 2926, 2927, 2931, 2936, 2954, 2973, 2978, 3008, 3036, 3041, 3053, 3054, 3058, 3066, 3089, 3096, 3109, 3119, 3126, 3149, 3164, 3167, 3172, 3182, 3186, 3209, 3211, 3212, 3224, 3225, 3237, 3246, 3251, 3269, 3270, 3272, 3273, 3279, 3292, 3294, 3311, 3314, 3335, 3341, 3365, 3369, 3371, 3378, 3404, 3412, 3427, 3429, 3435, 3448, 3464, 3474, 3483, 3487, 3494, 3496, 3499, 3504, 3518, 3553, 3562, 3574, 3581, 3586, 3587, 3593, 3627, 3644, 3702, 3703, 3713, 3717, 3727, 3734, 3735, 3753, 3758, 3769, 3777, 3808, 3810, 3821, 3827, 3839, 3843, 3854, 3859, 3866, 3869, 3874, 3892, 3895, 3900, 3902, 3908, 3912, 3924, 3931, 3934, 3943, 3948, 3954, 3959, 3963, 3964, 3966, 3978, 3992, 3998, 4016, 4019, 4022, 4029, 4030, 4055, 4068, 4078, 4089, 4106, 4126, 4146, 4147, 4150, 4152, 4159, 4161, 4191, 4203, 4209, 4232, 4245, 4251, 4262, 4264, 4270, 4275, 4276, 4285, 4325, 4327, 4328, 4329, 4343, 4344, 4357, 4360, 4366, 4370, 4374, 4377, 4381, 4385, 4401, 4421, 4423, 4427, 4438, 4441, 4443, 4451, 4452, 4457, 4460, 4476, 4479, 4508, 4517, 4528, 4555, 4556, 4557, 4564, 4567, 4587, 4594, 4595, 4601, 4604, 4613, 4623, 4639, 4649, 4667, 4669, 4674, 4679, 4682, 4685, 4691, 4694, 4699, 4710, 4714, 4721, 4723, 4724, 4726, 4730, 4771, 4779, 4790, 4798, 4801, 4803, 4816, 4831, 4836, 4848, 4849, 4861, 4869, 4871, 4875, 4876, 4887, 4888, 4891, 4900, 4922, 4933, 4944, 4950, 4966, 4968, 4980, 4986, 4987, 4994, 5002, 5017, 5025, 5038, 5051, 5059, 5067, 5099, 5119, 5124, 5133, 5145, 5161, 5175, 5177, 5179, 5191]\n",
      "Predictions for Changed Indices: [0.01964899 0.02027071 0.02059083 0.02085315 0.02094316 0.02105904\n",
      " 0.02106923 0.02109822 0.02112881 0.02133847 0.0214081  0.02149824\n",
      " 0.02163182 0.02163611 0.02167537 0.02168033 0.02168238 0.02170252\n",
      " 0.02171174 0.02172576 0.02175619 0.02181452 0.02183826 0.02184361\n",
      " 0.02186885 0.02192538 0.02192805 0.02193644 0.02198331 0.02199288\n",
      " 0.02202259 0.02204191 0.02208249 0.02211396 0.02217951 0.02217968\n",
      " 0.02218433 0.0221892  0.02219052 0.02219418 0.02219807 0.02221897\n",
      " 0.0222255  0.02224445 0.02226188 0.02226195 0.02228368 0.02230768\n",
      " 0.02232871 0.02235391 0.02236015 0.0223695  0.02237324 0.02238308\n",
      " 0.02238719 0.02239325 0.02240599 0.02242314 0.02243022 0.02245648\n",
      " 0.02245965 0.02246506 0.02247628 0.02248999 0.02249311 0.02250515\n",
      " 0.02250775 0.02250807 0.02251412 0.0225172  0.02252981 0.02253501\n",
      " 0.02253986 0.02255518 0.02256424 0.02256938 0.02257346 0.02258128\n",
      " 0.02259281 0.02260403 0.02260949 0.02262262 0.0226243  0.02262886\n",
      " 0.02263397 0.02263823 0.02264526 0.02266846 0.02267381 0.02268204\n",
      " 0.0226835  0.02269004 0.02269584 0.02271817 0.02274186 0.02274248\n",
      " 0.02274293 0.02277787 0.02279232 0.02279277 0.0227936  0.02279426\n",
      " 0.02279478 0.02279789 0.02280014 0.02280583 0.02280747 0.02280953\n",
      " 0.02281933 0.02281997 0.02282192 0.02282362 0.02282782 0.02283701\n",
      " 0.02284634 0.02284656 0.02284967 0.02287097 0.02288039 0.02289712\n",
      " 0.02290132 0.02290511 0.0229138  0.02291506 0.02292676 0.02293106\n",
      " 0.02293965 0.02294667 0.02294755 0.02295226 0.02295364 0.02295539\n",
      " 0.02295556 0.02296188 0.02296456 0.02296703 0.02297596 0.02298474\n",
      " 0.02299193 0.0229985  0.02301062 0.02301892 0.0230379  0.02303807\n",
      " 0.02304293 0.02306256 0.02306461 0.02307777 0.02307926 0.023088\n",
      " 0.02308977 0.02309083 0.0230919  0.02310313 0.02310335 0.0231035\n",
      " 0.02310426 0.02312401 0.02312881 0.02314103 0.02314194 0.02314444\n",
      " 0.02314845 0.02315279 0.02316951 0.02316975 0.02317594 0.02318888\n",
      " 0.02319064 0.02320014 0.02320619 0.0232102  0.02321167 0.02321189\n",
      " 0.02321644 0.02321648 0.0232224  0.02322359 0.0232266  0.02322885\n",
      " 0.02323019 0.02323395 0.02323535 0.02323535 0.02323947 0.02324443\n",
      " 0.02326815 0.02327151 0.0232843  0.02328539 0.0232863  0.02328709\n",
      " 0.02329841 0.02330232 0.02330645 0.02331017 0.02331532 0.02333162\n",
      " 0.02333314 0.02333394 0.0233463  0.02334648 0.0233482  0.02335719\n",
      " 0.0233693  0.02337618 0.02338077 0.02338385 0.023386   0.02340205\n",
      " 0.02340508 0.0234063  0.02341064 0.02341442 0.02341564 0.02341839\n",
      " 0.02343577 0.0234523  0.02345249 0.02346437 0.02349953 0.02351031\n",
      " 0.02351468 0.0235179  0.02352195 0.02352491 0.02352542 0.02352592\n",
      " 0.0235417  0.02354868 0.02356037 0.02356176 0.02356882 0.02358155\n",
      " 0.02358408 0.0235884  0.02359486 0.02359566 0.02359618 0.02359761\n",
      " 0.02359886 0.02360756 0.02360909 0.02361028 0.02361342 0.02361521\n",
      " 0.02361733 0.02361977 0.02362058 0.02362185 0.02362604 0.02362763\n",
      " 0.02363342 0.02363892 0.02365236 0.02366178 0.02367021 0.02367105\n",
      " 0.02367361 0.02367895 0.02368174 0.02369945 0.02370006 0.02370287\n",
      " 0.02371011 0.02371537 0.02371969 0.0237243  0.02373196 0.02373713\n",
      " 0.02373722 0.02373815 0.02374477 0.02375085 0.02375217 0.02375395\n",
      " 0.02375614 0.02375752 0.02376942 0.02377181 0.02377541 0.02378444\n",
      " 0.02378635 0.02378842 0.02378852 0.02378859 0.02379009 0.02379408\n",
      " 0.02380197 0.02380644 0.02380724 0.02380962 0.02382952 0.02383502\n",
      " 0.02383898 0.02383922 0.0238502  0.02385859 0.02386165 0.02387704\n",
      " 0.02387786 0.02388117 0.02388385 0.02388843 0.02389438 0.02389862\n",
      " 0.02389918 0.02390135 0.02390278 0.02390294 0.02390772 0.02390973\n",
      " 0.02391422 0.02391458 0.02393113 0.02394558 0.02394907 0.02395451\n",
      " 0.0239568  0.02395941 0.02395986 0.02396143 0.02396173 0.02399746\n",
      " 0.02400201 0.02402451 0.02402455 0.02403297 0.02404751 0.02404983\n",
      " 0.02405306 0.02405626 0.0240604  0.0240643  0.02406837 0.02406925\n",
      " 0.02407441 0.02409323 0.02409757 0.02410142 0.02410292 0.02410607\n",
      " 0.02410798 0.02410991 0.02411639 0.02412511 0.02413005 0.02413475\n",
      " 0.02413543 0.0241355  0.02413747 0.02414122 0.02416719 0.02417113\n",
      " 0.02417206 0.02418186 0.02418516 0.02419905 0.0242077  0.02421481\n",
      " 0.0242166  0.02422658 0.02423637 0.02424202 0.02424853 0.02425459\n",
      " 0.0242634  0.02426359 0.02426616 0.02426658 0.02426966 0.02427342\n",
      " 0.02427666 0.02428472 0.02429971 0.02431031 0.02431821 0.02431872\n",
      " 0.02431926 0.02431961 0.0243272  0.0243331  0.02433377 0.02433459\n",
      " 0.02434326 0.024348   0.02436533 0.02437503 0.02438229 0.02438445\n",
      " 0.02438817 0.02438857 0.02439114 0.02439448 0.02440813 0.02441309\n",
      " 0.02442074 0.02443058 0.02443621 0.02443829 0.0244391  0.02444014\n",
      " 0.02444162 0.02445484 0.02445687 0.02446186 0.0244619  0.02446262\n",
      " 0.02446988 0.02448384 0.02448522 0.02449414 0.02449804 0.02449903\n",
      " 0.0245044  0.02450452 0.02451351 0.02452198 0.0245326  0.02453805\n",
      " 0.02454353 0.02454391 0.02454888 0.02455131 0.02457913 0.02458081]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [149, 377, 923, 138, 341, 818, 1200, 1083, 228]\n",
      "Total Number of Eagles the model believes are not eagles 104\n",
      "Total Number of images the model believes are not eagles 2455\n",
      "Change ratio: 0.0819\n",
      "Iteration 5...\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step\n",
      "Adjusted 341 indices, avg confidence: 0.0149\n",
      "Changed Indices: [124, 224, 255, 311, 387, 400, 460, 720, 794, 879, 909, 1078, 1087, 1179, 1235, 1306, 1337, 1347, 1366, 1390, 1409, 1413, 1437, 1445, 1452, 1467, 1474, 1475, 1479, 1480, 1497, 1503, 1508, 1530, 1531, 1539, 1542, 1583, 1586, 1647, 1651, 1663, 1677, 1701, 1711, 1713, 1732, 1740, 1765, 1771, 1787, 1794, 1798, 1804, 1828, 1872, 1888, 1931, 1976, 1988, 2002, 2004, 2009, 2011, 2038, 2062, 2073, 2082, 2083, 2091, 2125, 2155, 2159, 2162, 2164, 2212, 2226, 2233, 2234, 2241, 2259, 2260, 2274, 2290, 2323, 2327, 2336, 2357, 2361, 2368, 2369, 2375, 2425, 2434, 2440, 2461, 2494, 2495, 2529, 2537, 2572, 2583, 2595, 2605, 2610, 2622, 2630, 2636, 2638, 2639, 2640, 2645, 2664, 2701, 2734, 2735, 2747, 2749, 2758, 2772, 2776, 2789, 2826, 2828, 2841, 2846, 2863, 2888, 2899, 2917, 2918, 2929, 2935, 2946, 2950, 2951, 2961, 2966, 2968, 2986, 2998, 3003, 3006, 3007, 3014, 3021, 3035, 3037, 3052, 3084, 3087, 3100, 3116, 3123, 3131, 3134, 3140, 3142, 3148, 3151, 3168, 3169, 3192, 3194, 3196, 3203, 3205, 3208, 3226, 3234, 3238, 3253, 3255, 3258, 3259, 3264, 3285, 3287, 3288, 3293, 3297, 3350, 3359, 3380, 3383, 3392, 3417, 3425, 3428, 3436, 3440, 3458, 3462, 3465, 3480, 3485, 3491, 3520, 3525, 3564, 3575, 3585, 3588, 3595, 3600, 3603, 3607, 3610, 3613, 3640, 3664, 3679, 3687, 3719, 3721, 3746, 3748, 3754, 3759, 3774, 3791, 3792, 3804, 3809, 3817, 3820, 3822, 3831, 3850, 3852, 3858, 3863, 3868, 3882, 3883, 3903, 3909, 3913, 3916, 3925, 3949, 3955, 3960, 3967, 3971, 3974, 3975, 3976, 3982, 3985, 4003, 4004, 4012, 4017, 4020, 4042, 4052, 4109, 4115, 4131, 4136, 4140, 4163, 4166, 4172, 4176, 4197, 4233, 4238, 4244, 4247, 4250, 4257, 4287, 4291, 4297, 4312, 4313, 4315, 4319, 4322, 4351, 4362, 4383, 4393, 4436, 4463, 4464, 4471, 4472, 4485, 4486, 4513, 4515, 4538, 4548, 4559, 4561, 4565, 4618, 4622, 4683, 4695, 4707, 4711, 4746, 4795, 4800, 4802, 4810, 4814, 4822, 4824, 4873, 4886, 4896, 4913, 4919, 4936, 4973, 4981, 4984, 5007, 5009, 5023, 5029, 5034, 5048, 5061, 5063, 5076, 5089, 5101, 5113, 5146, 5154, 5159, 5160, 5164, 5186, 5194]\n",
      "Predictions for Changed Indices: [0.0110193  0.01152887 0.01160854 0.01183889 0.01223115 0.01275585\n",
      " 0.0129347  0.01312748 0.01318003 0.01335191 0.01347043 0.01362168\n",
      " 0.01362759 0.01369558 0.0137053  0.01379453 0.0138037  0.01380972\n",
      " 0.01381181 0.01386452 0.01391499 0.01394343 0.01395488 0.01400407\n",
      " 0.01402385 0.01403994 0.01406108 0.01409543 0.01410831 0.01412676\n",
      " 0.01413081 0.01416142 0.01416698 0.01417265 0.01419026 0.01419546\n",
      " 0.01419937 0.01421734 0.01429543 0.01429692 0.01430817 0.01431388\n",
      " 0.01433044 0.01434446 0.01434516 0.01435323 0.01436661 0.01436691\n",
      " 0.01437123 0.01437426 0.01437945 0.01438203 0.01439939 0.01440468\n",
      " 0.01440587 0.01442838 0.01443397 0.0144561  0.01446906 0.01448418\n",
      " 0.01448607 0.01450052 0.01451459 0.01452181 0.0145304  0.01454344\n",
      " 0.01458871 0.01459176 0.01460845 0.01461479 0.01461933 0.01462206\n",
      " 0.01462767 0.01463783 0.01466039 0.01467001 0.01467497 0.0146814\n",
      " 0.01469096 0.01470996 0.01471105 0.01471186 0.01472276 0.01472972\n",
      " 0.01474269 0.01475436 0.01475707 0.01475819 0.01476319 0.01476807\n",
      " 0.01478995 0.01479104 0.01480167 0.01480868 0.01481069 0.01482643\n",
      " 0.01483005 0.01483014 0.01483018 0.01483132 0.01483595 0.01483745\n",
      " 0.01485291 0.01485584 0.01485703 0.01485899 0.01486201 0.01486432\n",
      " 0.0148701  0.0148767  0.01488397 0.01488553 0.01489087 0.01489291\n",
      " 0.01489651 0.01490006 0.01490229 0.01490273 0.01490619 0.01491536\n",
      " 0.0149243  0.01492998 0.01493321 0.01493363 0.01493581 0.01493663\n",
      " 0.01494122 0.01494758 0.01494884 0.01495035 0.01495052 0.01495131\n",
      " 0.01495416 0.0149624  0.01496307 0.01496403 0.01497648 0.0149784\n",
      " 0.01498383 0.01499011 0.01499154 0.01499671 0.01500116 0.0150047\n",
      " 0.0150061  0.01500629 0.01500661 0.01501005 0.01501115 0.01501806\n",
      " 0.01502155 0.01502369 0.01502546 0.01502873 0.0150298  0.01503352\n",
      " 0.01504666 0.01504938 0.01504964 0.01505473 0.01506402 0.01506959\n",
      " 0.01507135 0.01507178 0.01507222 0.01507765 0.01507902 0.01508039\n",
      " 0.01508055 0.01508849 0.01508997 0.01509496 0.0150967  0.01509769\n",
      " 0.01509812 0.01509991 0.01510029 0.01510032 0.01510056 0.01511062\n",
      " 0.01511519 0.01511624 0.01511681 0.01512273 0.01512492 0.01512635\n",
      " 0.01512978 0.0151315  0.01514197 0.01514407 0.01514456 0.01514653\n",
      " 0.01515205 0.01515496 0.01515744 0.01516217 0.01516319 0.01516336\n",
      " 0.0151648  0.01516548 0.01516622 0.01517902 0.01518338 0.01519535\n",
      " 0.01520051 0.01520139 0.01520706 0.01520779 0.01520888 0.01520974\n",
      " 0.01521221 0.01521458 0.01521867 0.01522166 0.01522222 0.01522321\n",
      " 0.01522516 0.01522527 0.01522693 0.01522881 0.01523186 0.01523357\n",
      " 0.01523358 0.01523747 0.01524084 0.01525219 0.0152577  0.01526119\n",
      " 0.01526139 0.01526186 0.01526417 0.01526424 0.01526508 0.01527238\n",
      " 0.01528927 0.01529253 0.01529478 0.01529489 0.01530105 0.01530111\n",
      " 0.01530321 0.01530397 0.01530594 0.01530747 0.01530921 0.01531238\n",
      " 0.01531316 0.01531653 0.01532009 0.01532156 0.0153252  0.01532689\n",
      " 0.01532886 0.01533151 0.01533192 0.0153322  0.0153365  0.01533883\n",
      " 0.01534204 0.01534413 0.01534478 0.015346   0.0153508  0.01535233\n",
      " 0.01535822 0.0153601  0.01536573 0.01536992 0.01537006 0.01537064\n",
      " 0.01537536 0.01537801 0.01537901 0.01537903 0.01538241 0.01538985\n",
      " 0.01539018 0.01539187 0.01539261 0.01539265 0.01539388 0.01539481\n",
      " 0.0153955  0.01539587 0.01539596 0.01539749 0.01540786 0.01540801\n",
      " 0.0154083  0.01540918 0.01541085 0.01541172 0.0154122  0.01541989\n",
      " 0.01542487 0.01542748 0.01543056 0.01543074 0.01543537 0.01543842\n",
      " 0.01543978 0.01544445 0.01544742 0.01544836 0.01545082 0.01545355\n",
      " 0.01545389 0.01545636 0.01546035 0.01546307 0.01546424 0.01546514\n",
      " 0.01546753 0.01546845 0.01547036 0.01547065 0.01547389 0.01547475\n",
      " 0.01547851 0.01548575 0.01548681 0.01549052 0.01549177 0.01549228\n",
      " 0.0154987  0.01550154 0.01550529 0.01551913 0.01552042 0.01552058\n",
      " 0.01552256 0.015523   0.0155238  0.01552456 0.01553491 0.01553533\n",
      " 0.01553863 0.01554045 0.01554509 0.01554604 0.01554605]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [720, 311, 460, 794, 1087, 1078, 255, 879, 909, 1179, 387, 400, 1235, 224, 124]\n",
      "Total Number of Eagles the model believes are not eagles 119\n",
      "Total Number of images the model believes are not eagles 2796\n",
      "Change ratio: 0.0656\n",
      "Iteration 6...\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step\n",
      "Adjusted 272 indices, avg confidence: 0.0100\n",
      "Changed Indices: [33, 71, 313, 417, 436, 477, 499, 553, 598, 623, 656, 677, 825, 901, 976, 1068, 1082, 1211, 1304, 1311, 1333, 1350, 1358, 1365, 1408, 1473, 1505, 1521, 1522, 1527, 1529, 1564, 1578, 1592, 1601, 1617, 1625, 1629, 1632, 1641, 1653, 1655, 1675, 1687, 1697, 1706, 1723, 1731, 1766, 1773, 1808, 1827, 1834, 1844, 1850, 1894, 1924, 1947, 1957, 1998, 2010, 2029, 2039, 2056, 2059, 2080, 2098, 2104, 2117, 2123, 2124, 2161, 2172, 2189, 2207, 2216, 2221, 2223, 2275, 2276, 2306, 2312, 2322, 2349, 2392, 2398, 2432, 2452, 2453, 2455, 2457, 2460, 2472, 2492, 2507, 2510, 2533, 2552, 2560, 2576, 2591, 2597, 2601, 2606, 2608, 2626, 2632, 2642, 2644, 2649, 2677, 2714, 2732, 2737, 2738, 2746, 2751, 2827, 2854, 2864, 2865, 2878, 2887, 2892, 2922, 2928, 2944, 2967, 2977, 2979, 2997, 3030, 3031, 3040, 3044, 3086, 3102, 3112, 3121, 3136, 3143, 3144, 3147, 3150, 3177, 3181, 3227, 3236, 3245, 3266, 3275, 3370, 3393, 3399, 3430, 3431, 3439, 3444, 3453, 3466, 3471, 3511, 3515, 3517, 3536, 3551, 3560, 3571, 3584, 3598, 3609, 3614, 3615, 3637, 3650, 3654, 3662, 3663, 3666, 3692, 3712, 3725, 3771, 3773, 3785, 3787, 3790, 3847, 3849, 3877, 3878, 3884, 3894, 3906, 3944, 3946, 3989, 3994, 3999, 4010, 4044, 4047, 4054, 4066, 4104, 4130, 4149, 4171, 4181, 4190, 4201, 4227, 4269, 4277, 4306, 4307, 4324, 4334, 4364, 4380, 4429, 4434, 4446, 4461, 4469, 4480, 4504, 4523, 4540, 4546, 4554, 4560, 4570, 4572, 4598, 4602, 4631, 4646, 4665, 4713, 4731, 4741, 4753, 4805, 4807, 4808, 4839, 4865, 4868, 4892, 4905, 4914, 4921, 4927, 4948, 4954, 4955, 4988, 5011, 5013, 5024, 5032, 5036, 5037, 5042, 5046, 5050, 5060, 5066, 5122, 5152, 5166]\n",
      "Predictions for Changed Indices: [0.00653727 0.00664633 0.0068696  0.00778083 0.00787013 0.00814379\n",
      " 0.00814397 0.008183   0.00846235 0.0084736  0.00852912 0.00870153\n",
      " 0.0087186  0.00872971 0.00887962 0.00888663 0.00889543 0.00897132\n",
      " 0.00897946 0.00898314 0.00904394 0.00910005 0.00914792 0.00917235\n",
      " 0.0091751  0.00920747 0.00920986 0.00921195 0.00924108 0.00927798\n",
      " 0.00928139 0.00953446 0.00954784 0.00957039 0.00957501 0.00960813\n",
      " 0.00962628 0.00962664 0.00962895 0.00964554 0.00965013 0.00966664\n",
      " 0.00967923 0.00968206 0.00969808 0.00971943 0.00972477 0.00972542\n",
      " 0.00975257 0.00975817 0.0097599  0.00977357 0.00978803 0.00979338\n",
      " 0.009821   0.0098261  0.00982683 0.00982978 0.00983832 0.00983966\n",
      " 0.00984708 0.0098601  0.00987205 0.00988088 0.00988185 0.00988327\n",
      " 0.00989008 0.00990329 0.00990331 0.00991761 0.00991929 0.00992251\n",
      " 0.00992363 0.00992518 0.00992659 0.00992691 0.00994229 0.00994472\n",
      " 0.00994984 0.00995019 0.009952   0.00995248 0.00995493 0.00995617\n",
      " 0.00996053 0.00996415 0.00996694 0.00996898 0.00996992 0.00997612\n",
      " 0.00998326 0.00998495 0.00998664 0.00998856 0.00999076 0.01000344\n",
      " 0.01000484 0.01000598 0.0100123  0.01001421 0.01001438 0.01001678\n",
      " 0.01001753 0.01001888 0.01002055 0.01002659 0.0100281  0.01003569\n",
      " 0.01003704 0.0100377  0.01004063 0.01004368 0.0100482  0.0100506\n",
      " 0.01005125 0.01005302 0.01005795 0.01006798 0.01006808 0.01007169\n",
      " 0.01007725 0.01009022 0.01010538 0.01010857 0.01010981 0.01011108\n",
      " 0.01011305 0.01011312 0.01011789 0.01012437 0.01012597 0.01012691\n",
      " 0.01012728 0.01013363 0.01014206 0.01014423 0.01014438 0.01014487\n",
      " 0.01014708 0.01014881 0.01015298 0.01015532 0.01015537 0.01015664\n",
      " 0.01015951 0.01016139 0.01017086 0.01017416 0.01017503 0.01017565\n",
      " 0.01018639 0.01018705 0.01018914 0.01019296 0.01019403 0.01019569\n",
      " 0.01020121 0.01020267 0.01020342 0.0102035  0.01020386 0.01020501\n",
      " 0.01020749 0.01020961 0.01021352 0.01021486 0.01021867 0.01022686\n",
      " 0.01022913 0.01023232 0.01023489 0.01023589 0.01023719 0.01023759\n",
      " 0.01023993 0.01024025 0.01024039 0.01024084 0.01024323 0.01024384\n",
      " 0.0102446  0.01024557 0.01025471 0.01025557 0.01026465 0.01026562\n",
      " 0.01026771 0.01026871 0.01026943 0.01027006 0.01027042 0.01027185\n",
      " 0.0102739  0.0102773  0.01028345 0.01028402 0.0102844  0.01028534\n",
      " 0.0102855  0.01028621 0.01029017 0.01029178 0.01029263 0.01029404\n",
      " 0.0102999  0.01030053 0.01030128 0.01030318 0.01030487 0.01030527\n",
      " 0.01030598 0.01030747 0.01031202 0.01031369 0.01031538 0.01031827\n",
      " 0.01031908 0.01031917 0.01031943 0.01031974 0.01032153 0.01032165\n",
      " 0.01032238 0.01032653 0.01032673 0.01032857 0.01032944 0.01032956\n",
      " 0.0103306  0.01033134 0.01033325 0.01033595 0.01033639 0.0103392\n",
      " 0.01034091 0.01034249 0.0103448  0.01034499 0.01034904 0.01034917\n",
      " 0.01035168 0.01035269 0.01035364 0.01035509 0.01035574 0.01035758\n",
      " 0.01035811 0.01036077 0.0103613  0.0103615  0.01036223 0.01036356\n",
      " 0.01036639 0.01036981 0.01037043 0.01037271 0.01037427 0.01037439\n",
      " 0.01037817 0.01037997 0.01038035 0.01038237 0.01038738 0.01038768\n",
      " 0.01038773 0.01039117 0.01039317 0.01039516 0.0103973  0.01040123\n",
      " 0.01040148 0.01040158]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [499, 71, 313, 1211, 1082, 553, 598, 677, 901, 417, 477, 976, 623, 656, 1068, 436, 33, 825]\n",
      "Total Number of Eagles the model believes are not eagles 137\n",
      "Total Number of images the model believes are not eagles 3068\n",
      "Change ratio: 0.0523\n",
      "Iteration 7...\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step\n",
      "Adjusted 218 indices, avg confidence: 0.0080\n",
      "Changed Indices: [7, 25, 269, 386, 423, 574, 724, 782, 849, 861, 866, 1001, 1039, 1055, 1213, 1310, 1313, 1326, 1345, 1354, 1384, 1394, 1399, 1428, 1433, 1434, 1478, 1482, 1492, 1507, 1514, 1538, 1555, 1560, 1563, 1571, 1582, 1656, 1692, 1693, 1725, 1735, 1749, 1754, 1756, 1760, 1761, 1791, 1807, 1816, 1818, 1848, 1853, 1860, 1901, 1912, 1926, 1940, 1992, 2003, 2007, 2022, 2033, 2042, 2049, 2058, 2072, 2088, 2089, 2090, 2137, 2148, 2149, 2158, 2187, 2204, 2242, 2244, 2256, 2288, 2305, 2376, 2381, 2382, 2412, 2419, 2451, 2454, 2470, 2473, 2521, 2530, 2590, 2599, 2652, 2685, 2692, 2764, 2765, 2794, 2814, 2815, 2832, 2852, 2872, 2880, 2897, 2906, 2907, 2921, 2930, 2932, 2958, 2984, 2994, 3015, 3022, 3055, 3057, 3065, 3067, 3085, 3092, 3179, 3188, 3210, 3243, 3256, 3276, 3280, 3286, 3295, 3301, 3332, 3349, 3367, 3379, 3396, 3450, 3482, 3484, 3498, 3531, 3580, 3589, 3629, 3636, 3660, 3688, 3697, 3698, 3738, 3750, 3798, 3814, 3816, 3819, 3823, 3848, 3896, 3901, 3904, 3935, 3938, 3953, 4000, 4014, 4056, 4102, 4121, 4148, 4153, 4168, 4170, 4180, 4182, 4210, 4215, 4216, 4221, 4231, 4237, 4258, 4281, 4308, 4348, 4392, 4435, 4526, 4529, 4541, 4610, 4617, 4619, 4635, 4648, 4684, 4698, 4712, 4729, 4739, 4756, 4764, 4770, 4773, 4821, 4846, 4864, 4866, 4956, 4991, 4998, 5028, 5065, 5108, 5148, 5173, 5193]\n",
      "Predictions for Changed Indices: [0.00724324 0.00734467 0.00741468 0.00744989 0.00749985 0.00753702\n",
      " 0.00761822 0.00769203 0.00771867 0.00771877 0.00772404 0.00772953\n",
      " 0.00773656 0.00775306 0.00778549 0.00779599 0.00780775 0.00780817\n",
      " 0.00781732 0.00782068 0.00783017 0.00783067 0.00784025 0.00784682\n",
      " 0.00785102 0.00785361 0.00785415 0.00786271 0.00786752 0.00787244\n",
      " 0.0078761  0.00788254 0.00788407 0.00788973 0.00789006 0.00789527\n",
      " 0.0078965  0.00789976 0.00791354 0.00791364 0.00791523 0.0079218\n",
      " 0.00792671 0.0079305  0.00793131 0.00793248 0.00793945 0.0079414\n",
      " 0.00794451 0.00794452 0.00794494 0.00794619 0.00794624 0.00794624\n",
      " 0.00794961 0.00794975 0.00795076 0.00795314 0.00795566 0.0079587\n",
      " 0.00795887 0.00796175 0.0079619  0.00796201 0.00796218 0.00796705\n",
      " 0.00796861 0.00797572 0.00797578 0.00797825 0.00797895 0.00797986\n",
      " 0.00798274 0.00798308 0.00798321 0.00798403 0.00798432 0.00798446\n",
      " 0.00798472 0.00798618 0.00798879 0.00798948 0.00799172 0.00799237\n",
      " 0.00799301 0.00800414 0.00800486 0.00800633 0.00801005 0.00801274\n",
      " 0.00801401 0.0080153  0.00801542 0.00801708 0.00801714 0.0080176\n",
      " 0.00801764 0.00801925 0.00801984 0.00801993 0.00802314 0.00802365\n",
      " 0.00802441 0.0080249  0.00802588 0.00802706 0.00802869 0.00803006\n",
      " 0.00803255 0.0080359  0.00803781 0.00803788 0.00804205 0.00804208\n",
      " 0.00804808 0.0080485  0.00805231 0.00805383 0.00805514 0.00805607\n",
      " 0.00805729 0.008058   0.00805845 0.00805893 0.00806347 0.00806587\n",
      " 0.00806961 0.00807116 0.00807244 0.00807486 0.00807647 0.0080786\n",
      " 0.00808051 0.00808095 0.00808113 0.00808128 0.00808143 0.00808176\n",
      " 0.00808249 0.00808324 0.00808455 0.00808456 0.00808565 0.00808599\n",
      " 0.00808652 0.00808705 0.00808747 0.00808835 0.00808903 0.00809204\n",
      " 0.00809205 0.00809275 0.00809423 0.00809472 0.0080967  0.00809703\n",
      " 0.00810203 0.00810485 0.00810701 0.00811173 0.0081133  0.00811452\n",
      " 0.00811558 0.00811568 0.00811612 0.00811742 0.00811792 0.00811902\n",
      " 0.00811919 0.00811977 0.00812126 0.00812216 0.00812255 0.00812288\n",
      " 0.00812343 0.00812429 0.0081249  0.00812688 0.00812961 0.00813065\n",
      " 0.00813147 0.00813346 0.0081341  0.00813495 0.00813655 0.0081371\n",
      " 0.00813844 0.00813903 0.00814085 0.00814152 0.0081422  0.00814496\n",
      " 0.0081451  0.00814583 0.00814782 0.00814856 0.00814871 0.00814976\n",
      " 0.00814982 0.00815001 0.00815113 0.00815133 0.00815274 0.00815355\n",
      " 0.00815361 0.008154   0.00815412 0.00815452 0.00815505 0.0081557\n",
      " 0.00815605 0.00815959 0.00815994 0.00816097 0.00816168 0.0081624\n",
      " 0.00816345 0.00816396]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [782, 861, 25, 849, 574, 7, 269, 724, 1213, 386, 1039, 423, 1001, 1055, 866]\n",
      "Total Number of Eagles the model believes are not eagles 152\n",
      "Total Number of images the model believes are not eagles 3286\n",
      "Change ratio: 0.0419\n",
      "Iteration 8...\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step\n",
      "Adjusted 174 indices, avg confidence: 0.0084\n",
      "Changed Indices: [0, 59, 134, 290, 293, 304, 480, 743, 969, 1230, 1344, 1410, 1465, 1469, 1517, 1520, 1541, 1543, 1568, 1589, 1610, 1614, 1619, 1635, 1746, 1774, 1775, 1829, 1846, 1858, 1865, 1866, 1878, 1889, 1948, 1984, 1989, 2005, 2012, 2028, 2043, 2063, 2078, 2085, 2092, 2107, 2113, 2152, 2167, 2197, 2200, 2235, 2238, 2281, 2289, 2313, 2360, 2363, 2415, 2418, 2436, 2456, 2476, 2490, 2515, 2534, 2540, 2573, 2574, 2578, 2604, 2663, 2671, 2672, 2693, 2710, 2726, 2781, 2783, 2790, 2806, 2821, 2843, 2886, 2902, 2938, 2947, 2980, 2985, 3012, 3038, 3091, 3107, 3128, 3171, 3174, 3175, 3187, 3216, 3252, 3265, 3291, 3319, 3382, 3397, 3467, 3514, 3528, 3541, 3542, 3546, 3569, 3570, 3596, 3597, 3619, 3631, 3661, 3695, 3723, 3724, 3736, 3768, 3772, 3855, 3873, 3893, 3907, 3939, 3970, 3980, 3997, 4046, 4100, 4125, 4194, 4214, 4225, 4280, 4305, 4310, 4333, 4340, 4391, 4411, 4432, 4518, 4543, 4545, 4590, 4606, 4627, 4645, 4716, 4744, 4784, 4796, 4881, 4895, 4902, 4946, 4971, 5005, 5041, 5045, 5054, 5056, 5080, 5081, 5098, 5112, 5130, 5163, 5183]\n",
      "Predictions for Changed Indices: [0.00800087 0.00801238 0.00804512 0.00807672 0.00808785 0.00809223\n",
      " 0.00809354 0.00810289 0.00812824 0.00816818 0.00816824 0.00817334\n",
      " 0.00818478 0.00818828 0.00819744 0.00820123 0.00820894 0.00821098\n",
      " 0.00822609 0.0082261  0.00823016 0.00823616 0.00824146 0.00824327\n",
      " 0.00824787 0.00825972 0.00826669 0.00826787 0.00827369 0.00827547\n",
      " 0.00827762 0.00827777 0.00828382 0.00828602 0.00829093 0.00829377\n",
      " 0.00829599 0.00829852 0.00830274 0.00831127 0.00831313 0.00831402\n",
      " 0.00831606 0.00831975 0.00832446 0.0083257  0.00832806 0.0083283\n",
      " 0.00832879 0.00832907 0.00833016 0.00833293 0.00833406 0.00833657\n",
      " 0.00833885 0.00833952 0.00834097 0.00834304 0.00834331 0.00834944\n",
      " 0.00835032 0.00835053 0.00835391 0.00835759 0.00835991 0.00836248\n",
      " 0.00836379 0.00836754 0.00836793 0.0083693  0.00836994 0.00837161\n",
      " 0.00837237 0.00837338 0.00837339 0.0083745  0.00837464 0.00837649\n",
      " 0.0083771  0.00837806 0.00838202 0.00838331 0.00838358 0.00838385\n",
      " 0.00838472 0.00838572 0.00838704 0.00838719 0.00838965 0.00838995\n",
      " 0.00839896 0.00839929 0.00840031 0.00840112 0.00840625 0.00840888\n",
      " 0.00840911 0.00840931 0.00840953 0.00841018 0.00841135 0.00841153\n",
      " 0.00841614 0.00841881 0.00841987 0.00841998 0.00842126 0.00842262\n",
      " 0.00842274 0.0084233  0.00842343 0.00842398 0.0084265  0.0084268\n",
      " 0.00842889 0.00842992 0.00843247 0.00843458 0.00843795 0.0084385\n",
      " 0.00843876 0.008441   0.00844188 0.00844475 0.00844633 0.00844653\n",
      " 0.00844694 0.00844833 0.00844913 0.00845241 0.00845601 0.00845611\n",
      " 0.00845734 0.00845966 0.00846124 0.00846271 0.00846552 0.00846966\n",
      " 0.0084697  0.00847146 0.00847577 0.00847583 0.00847986 0.00848113\n",
      " 0.00848485 0.00848788 0.008488   0.00848969 0.00848994 0.0084901\n",
      " 0.00849041 0.00849062 0.00849192 0.00849203 0.00849631 0.00849753\n",
      " 0.00849788 0.00849875 0.00849883 0.00849948 0.00850319 0.00850882\n",
      " 0.00850913 0.00850973 0.00851153 0.00851213 0.00851463 0.00851758\n",
      " 0.00852002 0.00852104 0.00852169 0.00852394 0.00852531 0.00852585]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [969, 743, 134, 59, 290, 0, 304, 480, 293, 1230]\n",
      "Total Number of Eagles the model believes are not eagles 162\n",
      "Total Number of images the model believes are not eagles 3460\n",
      "Change ratio: 0.0335\n",
      "Iteration 9...\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step\n",
      "Adjusted 140 indices, avg confidence: 0.0070\n",
      "Changed Indices: [47, 93, 283, 482, 537, 802, 884, 1178, 1322, 1351, 1352, 1361, 1385, 1405, 1439, 1447, 1495, 1504, 1506, 1523, 1557, 1588, 1591, 1634, 1652, 1662, 1686, 1690, 1700, 1741, 1752, 1803, 1883, 1908, 1923, 1944, 1987, 1995, 2013, 2019, 2054, 2060, 2120, 2134, 2217, 2240, 2263, 2293, 2298, 2321, 2365, 2414, 2420, 2514, 2567, 2581, 2654, 2700, 2722, 2795, 2853, 2883, 2963, 2996, 3071, 3111, 3222, 3239, 3247, 3260, 3300, 3304, 3320, 3327, 3346, 3363, 3366, 3414, 3424, 3524, 3554, 3576, 3616, 3665, 3689, 3722, 3729, 3782, 3794, 3824, 3889, 3941, 3951, 4015, 4039, 4120, 4123, 4127, 4218, 4220, 4286, 4292, 4295, 4302, 4303, 4304, 4350, 4371, 4395, 4400, 4468, 4481, 4490, 4497, 4499, 4563, 4573, 4717, 4719, 4740, 4747, 4760, 4788, 4820, 4823, 4834, 4901, 4906, 4972, 4977, 4985, 5003, 5015, 5016, 5084, 5088, 5092, 5120, 5176, 5199]\n",
      "Predictions for Changed Indices: [0.00624902 0.00649415 0.00661523 0.00662869 0.00669051 0.006752\n",
      " 0.00676071 0.00680065 0.00680275 0.00680324 0.00680363 0.00681313\n",
      " 0.00681319 0.00682495 0.00683763 0.00684375 0.00685189 0.00685368\n",
      " 0.00687696 0.00687949 0.0068834  0.0068859  0.00688726 0.00689229\n",
      " 0.00689276 0.00690428 0.00690435 0.00691597 0.00692638 0.0069264\n",
      " 0.00692908 0.00692945 0.00694079 0.00694526 0.00695017 0.00695244\n",
      " 0.0069655  0.00697017 0.0069731  0.00697483 0.00697761 0.00697781\n",
      " 0.00698074 0.006983   0.00698925 0.00699458 0.0069951  0.00699971\n",
      " 0.00700355 0.00700431 0.00700829 0.0070093  0.00701022 0.00701237\n",
      " 0.00701318 0.00701478 0.00701746 0.00702065 0.0070237  0.00702665\n",
      " 0.00702831 0.00703279 0.00703306 0.00703723 0.00703735 0.00703929\n",
      " 0.00704324 0.00704399 0.00704438 0.00704664 0.00704914 0.00704975\n",
      " 0.00704989 0.00704997 0.00705323 0.00705672 0.00705743 0.00705975\n",
      " 0.00705995 0.00705999 0.0070611  0.00706159 0.00706391 0.00706556\n",
      " 0.00706599 0.00706872 0.00707198 0.00707457 0.0070749  0.0070753\n",
      " 0.00707559 0.00707814 0.00708114 0.0070818  0.00708335 0.00708591\n",
      " 0.00709115 0.00709184 0.00709217 0.00709346 0.00709718 0.00709755\n",
      " 0.0070983  0.0070984  0.00710093 0.00710393 0.00710447 0.00710625\n",
      " 0.00710797 0.00710865 0.00711189 0.00711315 0.0071145  0.00711457\n",
      " 0.00711576 0.00712468 0.00712709 0.00712943 0.00712944 0.00713085\n",
      " 0.00713287 0.00713325 0.00713422 0.0071358  0.00713892 0.00713957\n",
      " 0.00714226 0.00714391 0.00714446 0.00714468 0.00714518 0.00714521\n",
      " 0.00714796 0.00714988 0.00715575 0.00715754 0.00716083 0.00716103\n",
      " 0.00716139 0.00716155]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [1178, 283, 537, 47, 884, 93, 482, 802]\n",
      "Total Number of Eagles the model believes are not eagles 170\n",
      "Total Number of images the model believes are not eagles 3600\n",
      "Change ratio: 0.0269\n",
      "Iteration 10...\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step\n",
      "Adjusted 112 indices, avg confidence: 0.0084\n",
      "Changed Indices: [55, 239, 530, 566, 781, 1032, 1232, 1263, 1356, 1395, 1458, 1486, 1493, 1511, 1553, 1561, 1603, 1604, 1650, 1680, 1742, 1801, 1824, 1885, 1959, 1968, 2023, 2026, 2052, 2096, 2141, 2143, 2150, 2282, 2307, 2428, 2501, 2517, 2543, 2571, 2582, 2594, 2643, 2646, 2683, 2775, 2808, 2862, 2873, 2916, 2981, 3004, 3019, 3114, 3139, 3206, 3218, 3250, 3289, 3298, 3372, 3449, 3473, 3512, 3557, 3624, 3647, 3657, 3694, 3708, 3710, 3740, 3749, 3752, 3807, 3838, 4007, 4061, 4072, 4073, 4077, 4142, 4169, 4179, 4189, 4198, 4224, 4296, 4299, 4321, 4332, 4349, 4588, 4609, 4644, 4700, 4706, 4752, 4754, 4762, 4792, 4855, 4929, 4930, 4938, 5055, 5094, 5100, 5110, 5121, 5129, 5198]\n",
      "Predictions for Changed Indices: [0.0081478  0.00823641 0.00824653 0.00824754 0.00825156 0.00826423\n",
      " 0.00826906 0.00828747 0.00828825 0.00829122 0.00830042 0.00830859\n",
      " 0.00831532 0.00831656 0.00831769 0.00832062 0.00832258 0.00833473\n",
      " 0.00833596 0.00833704 0.00834223 0.00834279 0.00834431 0.00834618\n",
      " 0.00834878 0.00835625 0.00835664 0.00835974 0.00836064 0.00836076\n",
      " 0.00836077 0.00836368 0.00836616 0.00836664 0.00836734 0.00837157\n",
      " 0.00837164 0.0083731  0.00837427 0.00837624 0.00837784 0.00837903\n",
      " 0.00838267 0.00838281 0.00838504 0.00838613 0.00838627 0.00838759\n",
      " 0.00839127 0.00839277 0.00839281 0.00839291 0.00839428 0.00839441\n",
      " 0.00839513 0.00839543 0.00839583 0.00839597 0.00840098 0.00840683\n",
      " 0.0084122  0.00841675 0.00841898 0.00842236 0.00842263 0.00842445\n",
      " 0.00842517 0.0084326  0.00843377 0.00843436 0.00843531 0.00843788\n",
      " 0.008438   0.00844203 0.00844246 0.00844536 0.00844737 0.00844761\n",
      " 0.00844796 0.00845008 0.0084538  0.00845521 0.00845664 0.00845696\n",
      " 0.00845933 0.00845938 0.00846255 0.00846265 0.00846387 0.00846624\n",
      " 0.00846748 0.00846998 0.00847007 0.00847094 0.00847141 0.00847142\n",
      " 0.00847382 0.00847764 0.00847998 0.00848113 0.00848135 0.00848212\n",
      " 0.00848222 0.00848745 0.00848969 0.00849053 0.0084915  0.00849423\n",
      " 0.00849719 0.00849781 0.0084983  0.00849944]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [1032, 781, 55, 1263, 1232, 530, 239, 566]\n",
      "Total Number of Eagles the model believes are not eagles 178\n",
      "Total Number of images the model believes are not eagles 3712\n",
      "Change ratio: 0.0215\n",
      "Iteration 11...\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step\n",
      "Adjusted 89 indices, avg confidence: 0.0070\n",
      "Changed Indices: [74, 82, 105, 130, 260, 322, 353, 547, 572, 790, 833, 836, 1085, 1100, 1323, 1387, 1398, 1444, 1481, 1485, 1576, 1764, 1820, 1859, 1886, 1918, 1933, 1991, 2101, 2118, 2211, 2250, 2252, 2330, 2334, 2345, 2466, 2468, 2698, 2723, 2955, 3005, 3024, 3033, 3048, 3060, 3158, 3161, 3173, 3229, 3321, 3345, 3352, 3401, 3407, 3463, 3526, 3545, 3583, 3642, 3667, 3747, 3778, 3832, 3865, 3898, 3915, 4228, 4248, 4256, 4314, 4369, 4384, 4397, 4408, 4412, 4494, 4599, 4676, 4737, 4742, 4769, 4809, 4815, 4841, 4847, 4924, 4970, 5140]\n",
      "Predictions for Changed Indices: [0.00687233 0.0068806  0.00689464 0.00692525 0.00695289 0.00695291\n",
      " 0.00695326 0.00695431 0.00695821 0.00696236 0.00696245 0.00698031\n",
      " 0.00698214 0.00698471 0.00698522 0.00698524 0.00698729 0.0069884\n",
      " 0.00698971 0.00699162 0.00699902 0.00699933 0.00700615 0.00700875\n",
      " 0.00700942 0.00701089 0.00701134 0.00701446 0.00701814 0.0070188\n",
      " 0.00702673 0.00702888 0.00703086 0.00703254 0.00703307 0.00703725\n",
      " 0.00703807 0.00703856 0.00704227 0.00704369 0.00704437 0.00704523\n",
      " 0.00704552 0.00704716 0.00704845 0.00705144 0.00705199 0.00705378\n",
      " 0.00705766 0.00705814 0.00705965 0.00706291 0.00706343 0.00706365\n",
      " 0.00706389 0.0070644  0.00706499 0.00706527 0.00706779 0.00706946\n",
      " 0.00707443 0.00707623 0.00707634 0.00708206 0.00708311 0.00708325\n",
      " 0.00708431 0.00708555 0.00708614 0.00708727 0.00708771 0.00709053\n",
      " 0.00709392 0.00709627 0.00709633 0.00709723 0.00709774 0.00709817\n",
      " 0.00709871 0.00710064 0.00710617 0.00710675 0.00710724 0.00710837\n",
      " 0.00711042 0.00711053 0.00711187 0.00711217 0.00711275]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [260, 130, 547, 790, 105, 1085, 1100, 322, 74, 572, 82, 833, 836, 353]\n",
      "Total Number of Eagles the model believes are not eagles 192\n",
      "Total Number of images the model believes are not eagles 3801\n",
      "Change ratio: 0.0171\n",
      "Iteration 12...\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step\n",
      "Adjusted 71 indices, avg confidence: 0.0054\n",
      "Changed Indices: [43, 72, 107, 146, 207, 392, 470, 546, 687, 996, 1052, 1140, 1240, 1249, 1319, 1416, 1464, 1510, 1546, 1694, 1726, 1729, 2021, 2051, 2193, 2343, 2356, 2379, 2728, 2736, 2784, 2874, 2889, 2924, 2948, 2989, 3088, 3095, 3355, 3358, 3384, 3388, 3501, 3620, 3658, 3659, 3673, 3780, 3784, 3828, 3933, 3940, 3990, 4059, 4103, 4165, 4167, 4195, 4368, 4375, 4378, 4527, 4670, 4678, 4718, 4733, 4843, 4884, 4941, 4965, 5095]\n",
      "Predictions for Changed Indices: [0.00363812 0.00503369 0.00504738 0.00512911 0.00513559 0.0051752\n",
      " 0.00519001 0.00522663 0.00523677 0.00527872 0.00529305 0.00532169\n",
      " 0.0053239  0.00535504 0.00535834 0.0053626  0.00536335 0.00536457\n",
      " 0.00538429 0.00539063 0.00539476 0.00539899 0.00539944 0.00540366\n",
      " 0.00543054 0.0054306  0.0054584  0.00545843 0.00545947 0.00546109\n",
      " 0.00546405 0.00546795 0.00547252 0.00547665 0.00548172 0.00548472\n",
      " 0.00548629 0.00549617 0.00549656 0.00549908 0.00550747 0.00552193\n",
      " 0.00552701 0.00553128 0.00553413 0.00553462 0.00553532 0.00553618\n",
      " 0.00554067 0.00554216 0.00554305 0.00555061 0.00555231 0.00555414\n",
      " 0.00555868 0.00555955 0.00556298 0.00556458 0.00556993 0.00557026\n",
      " 0.00557404 0.0055742  0.00557488 0.0055751  0.00557715 0.00557927\n",
      " 0.00557991 0.00558227 0.00558314 0.00558462 0.00558639]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [1052, 72, 546, 1240, 470, 1140, 1249, 107, 207, 392, 43, 687, 996, 146]\n",
      "Total Number of Eagles the model believes are not eagles 206\n",
      "Total Number of images the model believes are not eagles 3872\n",
      "Change ratio: 0.0137\n",
      "Iteration 13...\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step\n",
      "Adjusted 57 indices, avg confidence: 0.0058\n",
      "Changed Indices: [36, 245, 352, 362, 364, 438, 601, 634, 1041, 1417, 1425, 1519, 1550, 1584, 1758, 1904, 1972, 2180, 2237, 2304, 2366, 2399, 2423, 2437, 2511, 2531, 2566, 2585, 2598, 2733, 3010, 3078, 3097, 3098, 3156, 3162, 3232, 3240, 3244, 3271, 3375, 3423, 3516, 3590, 3611, 3910, 3962, 3973, 4060, 4290, 4425, 4466, 4474, 4493, 4532, 4562, 4885]\n",
      "Predictions for Changed Indices: [0.00553802 0.00556204 0.00559226 0.00562668 0.00562758 0.00565179\n",
      " 0.00565575 0.00567154 0.00570028 0.00570788 0.00571085 0.00571371\n",
      " 0.0057246  0.00574976 0.00575862 0.005759   0.00576127 0.00576193\n",
      " 0.00576733 0.00576902 0.00577003 0.00578297 0.00578299 0.00578363\n",
      " 0.00578499 0.00578893 0.00579067 0.00579188 0.00579284 0.00579623\n",
      " 0.00579819 0.00579902 0.00580177 0.00581516 0.00581528 0.0058226\n",
      " 0.00582268 0.00582949 0.0058321  0.00583236 0.0058338  0.00583388\n",
      " 0.00583437 0.00584042 0.00585017 0.00585039 0.0058513  0.00585483\n",
      " 0.0058559  0.00585907 0.00586012 0.00586054 0.00586068 0.00586554\n",
      " 0.0058705  0.00587382 0.00587443]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [634, 601, 362, 438, 1041, 245, 352, 364, 36]\n",
      "Total Number of Eagles the model believes are not eagles 215\n",
      "Total Number of images the model believes are not eagles 3929\n",
      "Change ratio: 0.0110\n",
      "Iteration 14...\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step\n",
      "Adjusted 46 indices, avg confidence: 0.0073\n",
      "Changed Indices: [46, 688, 723, 1362, 1368, 1602, 1738, 1849, 1941, 1945, 2067, 2196, 2220, 2444, 2522, 2668, 2684, 2780, 2858, 2937, 3391, 3507, 3561, 3645, 3767, 3846, 3886, 3919, 4206, 4211, 4260, 4353, 4398, 4428, 4489, 4511, 4521, 4552, 4647, 4666, 4688, 4862, 4942, 4961, 5149, 5171]\n",
      "Predictions for Changed Indices: [0.00704948 0.0070645  0.00708078 0.00708729 0.00709631 0.00709699\n",
      " 0.0071529  0.00716333 0.00717262 0.00720992 0.00725507 0.00727901\n",
      " 0.00729859 0.00729965 0.00730304 0.00731736 0.00732228 0.0073366\n",
      " 0.0073388  0.0073515  0.00735205 0.00736943 0.00737494 0.00737655\n",
      " 0.0073861  0.00738839 0.00740185 0.00740193 0.00740771 0.00742043\n",
      " 0.00742636 0.00744309 0.00744311 0.00744639 0.00745023 0.00745705\n",
      " 0.0074945  0.00749922 0.00750034 0.00751355 0.00751599 0.00751991\n",
      " 0.00752635 0.00753129 0.00754148 0.00754361]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [688, 723, 46]\n",
      "Total Number of Eagles the model believes are not eagles 218\n",
      "Total Number of images the model believes are not eagles 3975\n",
      "Change ratio: 0.0088\n",
      "Iteration 15...\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step\n",
      "Adjusted 37 indices, avg confidence: 0.0067\n",
      "Changed Indices: [150, 315, 403, 414, 486, 573, 841, 874, 900, 1419, 1427, 1633, 1896, 1967, 2111, 2341, 2371, 2374, 2469, 2484, 2690, 2712, 2729, 2817, 3027, 3415, 3572, 3726, 4173, 4178, 4273, 4439, 4478, 4652, 4908, 4909, 4963]\n",
      "Predictions for Changed Indices: [0.00659859 0.00660872 0.00661447 0.00664    0.00664523 0.00665014\n",
      " 0.00667    0.00667199 0.0066772  0.0066788  0.00667909 0.00668523\n",
      " 0.00668872 0.00669315 0.00669826 0.00669869 0.00670175 0.00670633\n",
      " 0.00670841 0.00670997 0.00671205 0.0067141  0.00671618 0.00671911\n",
      " 0.00672465 0.00672721 0.00672879 0.00673044 0.00673053 0.00673476\n",
      " 0.00674355 0.0067453  0.00674632 0.00674795 0.00674869 0.00674962\n",
      " 0.00675025]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [874, 573, 900, 414, 841, 486, 315, 403, 150]\n",
      "Total Number of Eagles the model believes are not eagles 227\n",
      "Total Number of images the model believes are not eagles 4012\n",
      "Change ratio: 0.0071\n"
     ]
    }
   ],
   "source": [
    "n_percentage = 20  # % of least confident eagle predictions to adjust\n",
    "max_iterations = 15 \n",
    "convergence_tolerance = 0  # stop if change ratio is this\n",
    "high_conf_threshold = 0.8  # threshold for confident eagle flips\n",
    "confidence_init = confidence_init = np.array([.35 if x<.5 else 1 for x in label_train]).reshape(5200,1)\n",
    "\n",
    "num_incorrectly_modified = 0\n",
    "num_modified = 0\n",
    "for iteration in range(max_iterations):\n",
    "    print(f\"Iteration {iteration + 1}...\")\n",
    "\n",
    "    # step 1: train model\n",
    "    history = model.fit(\n",
    "        [images_train, confidence_init],\n",
    "        label_train,\n",
    "        batch_size=100, # ok this might seem crazy but im wondering if w batch=32 it wasn't encountering enough wrong labels \n",
    "        epochs=1,\n",
    "        # validation_data=([images_val, np.ones((len(images_val), 1))], label_val), # this was a line from chat, replaced w ours instead below\n",
    "        validation_data=([images_val, label_val.reshape(-1,1)], label_val),\n",
    "        callbacks=callbacks,\n",
    "        shuffle = True,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # step 2: predict probabilities\n",
    "    preds = model.predict([images_train, confidence_init]).flatten()\n",
    "\n",
    "    # step 3: identify least confident eagle predictions\n",
    "    low_confidence_indices = np.where((label_train == 0) & (preds < 0.5) & (preds != 0))[0] # grabbing indices where label_train is 0 (noneagle), focusing in on the misclassified\n",
    "    filtered_indices = low_confidence_indices[confidence_init.flatten()[low_confidence_indices]!= 0]\n",
    "    sorted_indices = filtered_indices[np.argsort(preds[filtered_indices])] # sorts the preds low to high\n",
    "    to_adjust = sorted_indices[:int(len(sorted_indices) * (n_percentage / 100))] # only grabbing 5% rn of the bottom\n",
    "    \n",
    "\n",
    "    # step 4: update confidence for least confident predictions\n",
    "    if len(to_adjust) > 0:\n",
    "        confidence_init[to_adjust] = 0  # reduce confidence to 0 for the indices we picked by %\n",
    "        avg_confidence = np.mean(preds[to_adjust])\n",
    "        print(f\"Adjusted {len(to_adjust)} indices, avg confidence: {avg_confidence:.4f}\")\n",
    "    else:\n",
    "        print(\"No indices to adjust in this iteration.\")\n",
    "        \n",
    "    print(f\"Changed Indices: {sorted(to_adjust)}\")\n",
    "    print(f\"Predictions for Changed Indices: {preds[to_adjust]}\")\n",
    "    wrongly_switched = [x for x in changed_indices if x in to_adjust]\n",
    "    print(\"FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES:\", wrongly_switched)\n",
    "    num_incorrectly_modified += len(wrongly_switched)\n",
    "    num_modified += len(to_adjust)\n",
    "    print(\"Total Number of Eagles the model believes are not eagles\", num_incorrectly_modified)\n",
    "    print(\"Total Number of images the model believes are not eagles\", num_modified)\n",
    "\n",
    "\n",
    "    # step 5: check for convergence\n",
    "    if len(to_adjust) > 0:\n",
    "            change_ratio = len(to_adjust) / len(label_train)  \n",
    "    else:\n",
    "            change_ratio=0\n",
    "    print(f\"Change ratio: {change_ratio:.4f}\")\n",
    "\n",
    "    if change_ratio < convergence_tolerance:\n",
    "        print(\"Convergence reached.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d5f8a43-623a-4e17-9786-85b18742f7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many actual eagles in top 5% is: 46\n",
      "How many total are in the top 5% is: 208\n"
     ]
    }
   ],
   "source": [
    "def top_n_percent_indices(predictions, n_percent):\n",
    "\n",
    "    target_indices = np.where(label_train == 0)[0]\n",
    "    filtered_preds = predictions[target_indices]\n",
    "    \n",
    "    # calculate the number of top elements to select\n",
    "    num_top_elements = int(np.ceil(len(filtered_preds) * n_percent / 100))\n",
    "    \n",
    "    # get the indices of the sorted values (descending order)\n",
    "    sorted_indices = np.argsort(filtered_preds)[::-1]\n",
    "    \n",
    "    # select the top n_percent indices\n",
    "    top_indices = sorted_indices[:num_top_elements]\n",
    "\n",
    "    top_original_indices = target_indices[top_indices]\n",
    "    \n",
    "    return top_original_indices\n",
    "n_percent = 5\n",
    "high_confidence_indices = top_n_percent_indices(preds,n_percent)\n",
    "high_confidence_indices\n",
    "actually_eagles = [x for x in high_confidence_indices if x in changed_indices]\n",
    "# print(\"The least confident not-eagles that are actually eagles:\", actually_eagles)\n",
    "print(f\"How many actual eagles in top {n_percent}% is: {len(actually_eagles)}\")\n",
    "print(f\"How many total are in the top {n_percent}% is: {len(high_confidence_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b988f3c-a1b6-4798-8f4a-bbc6878d2172",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### just to test distributions / debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38406e69-705d-44df-837e-d79871e2f2fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f0cea15-01b9-416e-9a34-e2a4e47b57cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly labeled eagles: 1040/1040\n",
      "Correctly labeled noneagles: 4160/4160\n"
     ]
    }
   ],
   "source": [
    "# Compare relabeled classes to original ground truth\n",
    "original_labels = new_labels  \n",
    "correct_eagles = np.sum((label_train == 1) & (original_labels == 1))  # True eagles as eagles\n",
    "correct_noneagles = np.sum((label_train == 0) & (original_labels == 0))  # True noneagles as noneagles\n",
    "\n",
    "print(f\"Correctly labeled eagles: {correct_eagles}/{np.sum(original_labels == 1)}\")\n",
    "print(f\"Correctly labeled noneagles: {correct_noneagles}/{np.sum(original_labels == 0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8b2ea307-66f6-4820-af0f-e7387923936e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean confidence for eagles: 0.0297\n",
      "Mean confidence for noneagles: 0.0079\n"
     ]
    }
   ],
   "source": [
    "eagle_confidence = preds[label_train == 1]\n",
    "noneagle_confidence = preds[label_train == 0]\n",
    "\n",
    "print(f\"Mean confidence for eagles: {eagle_confidence.mean():.4f}\")\n",
    "print(f\"Mean confidence for noneagles: {noneagle_confidence.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "091e90f8-5dd7-45d0-904b-b4f1e03d1997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-statistic: 285.7584, P-value: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "t_stat, p_val = ttest_ind(eagle_confidence, noneagle_confidence, equal_var=False)\n",
    "print(f\"T-statistic: {t_stat:.4f}, P-value: {p_val:.4e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "92a903ff-2813-4184-bbc5-479a20d2dbe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(eagle_confidence, bins=20, alpha=0.7, label='Eagles (1)')\n",
    "plt.hist(noneagle_confidence, bins=20, alpha=0.7, label='Noneagles (0)')\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Confidence Distribution')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8fe5eac0-43f7-49ec-a853-c79292cd062d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Label=0, Confidence=0.4747\n",
      "Sample 1: Label=1, Confidence=0.5125\n",
      "Sample 2: Label=1, Confidence=0.5125\n",
      "Sample 3: Label=1, Confidence=0.5125\n",
      "Sample 4: Label=1, Confidence=0.5125\n",
      "Sample 5: Label=1, Confidence=0.5125\n",
      "Sample 6: Label=1, Confidence=0.5125\n",
      "Sample 7: Label=0, Confidence=0.4747\n",
      "Sample 8: Label=1, Confidence=0.5125\n",
      "Sample 9: Label=1, Confidence=0.5125\n"
     ]
    }
   ],
   "source": [
    "# Get indices of ambiguous samples\n",
    "ambiguous_indices = np.where((preds > 0.4) & (preds < 0.6))[0]\n",
    "\n",
    "# Inspect some ambiguous samples (requires image visualization logic)\n",
    "for idx in ambiguous_indices[:10]:  # First 10 ambiguous cases\n",
    "    print(f\"Sample {idx}: Label={label_train[idx]}, Confidence={preds[idx]:.4f}\")\n",
    "    # Add your image visualization code here if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372577b8-e20f-4931-9411-7cdd8356c626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d91cb0-eff3-4deb-8493-b6a45b535c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
