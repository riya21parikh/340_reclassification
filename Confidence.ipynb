{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aeda16a-b466-44e5-8ba3-f73264f23edf",
   "metadata": {},
   "source": [
    "# ParikhSamuolisReclassificationNN Final Project File 2\n",
    "## date last modified: Dec 4, 2024\n",
    "### how to link to github --> https://saturncloud.io/blog/how-to-add-jupyter-notebook-to-github/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb8fed7-0ecb-40d3-9f29-f6f781b9984f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Loading images and necessary functions - run each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b497b9ed-b6ac-44a6-9aff-456e09a9c144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e95280c4-13b0-45cc-a013-6ca2f30b0ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directories for train and validation sets\n",
    "root_dir = '/projectnb/ds340/projects/Samuolis_Parikh_Image_Data/'\n",
    "\n",
    "train_dir = root_dir +\"resized_images/train\"\n",
    "validation_dir = root_dir + \"resized_images/validation\"\n",
    "\n",
    "train_target = train_dir +\"/baldeagle\"\n",
    "train_nontarget = train_dir +\"/nonbaldeagle\"\n",
    "\n",
    "val_target = validation_dir +\"/baldeagle\"\n",
    "val_nontarget = validation_dir +\"/nonbaldeagle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a74d33b9-6ff2-4aac-a4b7-9576a8d52d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folders(folder1, folder2, img_size = (224,224)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # load images from the first folder\n",
    "    for filename in os.listdir(folder1):\n",
    "        img_path = os.path.join(folder1, filename)\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                img = img.resize(img_size)\n",
    "                images.append(np.array(img))  # convert image to array\n",
    "                labels.append(1)  # class label for folder1\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load image {filename} from {folder1}: {e}\")\n",
    "\n",
    "    # load images from the second folder\n",
    "    for filename in os.listdir(folder2):\n",
    "        img_path = os.path.join(folder2, filename)\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                img = img.resize(img_size)\n",
    "                images.append(np.array(img))\n",
    "                labels.append(0)  # class label for folder2\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load image {filename} from {folder2}: {e}\")\n",
    "\n",
    "    # convert lists to NumPy arrays\n",
    "    \n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "images_train, label_train = load_images_from_folders(train_target, train_nontarget)\n",
    "images_val, label_val = load_images_from_folders(val_target, val_nontarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2fc677bc-cf4f-4345-9025-45a27bf0a1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5200, 224, 224, 3) (5200,) <class 'numpy.ndarray'>\n",
      "0 255\n",
      "Initial eagle count: 1300\n",
      "Initial noneagle count: 3900\n"
     ]
    }
   ],
   "source": [
    "## for debugging:\n",
    "print(images_train.shape, label_train.shape, type(images_train))\n",
    "print(images_train.min(), images_train.max())  # expected: 0 255, later will normalize\n",
    "print(f\"Initial eagle count: {np.sum(label_train == 1)}\")\n",
    "print(f\"Initial noneagle count: {np.sum(label_train == 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "79236d4d-b08c-4630-a005-0da7c7d0c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_labels(labels, percentage):\n",
    "    random.seed(340)\n",
    "    label_one_indices = np.where(labels == 1)[0]\n",
    "    \n",
    "    n = int(len(label_one_indices) * (percentage / 100))\n",
    "    \n",
    "    indices_to_change = np.random.choice(label_one_indices, size=n, replace=False)\n",
    "    \n",
    "    labels[indices_to_change] = 0\n",
    "    \n",
    "    return labels, indices_to_change\n",
    "\n",
    "# for example, change 20% of label 1s to label 0\n",
    "percentage = 0  \n",
    "# changed_indices\n",
    "# label_train, changed_indices = change_labels(label_train, percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f8fe98c-89ce-4d1f-a0f3-840d49eebdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "tf.keras.utils.set_random_seed(340)\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "66d6aebd-c991-4a74-9b33-f1e5920218c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "]\n",
    "epochs = 15\n",
    "# restore best weights make the model be the one that was the best instead of last one\n",
    "# patience changed from 4-->3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e3319a3-82bf-42a0-afb7-d005da2bbfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload images\n",
    "images_train, label_train = load_images_from_folders(train_target, train_nontarget)\n",
    "percentage = 20  \n",
    "# changed_indices\n",
    "label_train, changed_indices = change_labels(label_train, percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e64eef54-df72-4075-a370-f590a2f831db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New eagle count: 1040\n",
      "New noneagle count: 4160\n"
     ]
    }
   ],
   "source": [
    "print(f\"New eagle count: {np.sum(label_train == 1)}\")\n",
    "print(f\"New noneagle count: {np.sum(label_train == 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44e5183d-4eb9-40e1-8066-a17ae72e4d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New eagle count: 1040\n",
      "New noneagle count: 4160\n",
      "Confidence values: [0.35 1.   1.   1.   1.   1.   1.   0.35 1.   1.  ]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dropout, Concatenate\n",
    "confidence_init = confidence_init = np.array([.35 if x<.5 else 1 for x in label_train]).reshape(5200,1)\n",
    "# start with all 1s for confidence\n",
    "# this doesn't work --- we don't know before hand which indices we aren't confident about, we especially don't know \n",
    "# to specifically be less confident for the labels that we changed\n",
    "# confidence_init[label_train == 0] = 0  # Set confidence to 0 for original 0 labels\n",
    "# confidence_init[changed_indices] = 0.35  # Set confidence to 0.35 for flipped labels\n",
    "# confidence_init = confidence_init.reshape(-1, 1)  # Reshape to (N, 1)\n",
    "\n",
    "# print data statistics\n",
    "print(f\"New eagle count: {np.sum(label_train == 1)}\")\n",
    "print(f\"New noneagle count: {np.sum(label_train == 0)}\")\n",
    "print(f\"Confidence values: {confidence_init[:10].flatten()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc5ea703-0063-4f68-84fa-da03d64bef30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remake models\n",
    "# mutliple inputs taken from chat and https://pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/\n",
    "# we have full confidence if it is a 1, the lower the number the more confident you are in the 0 class -- .999999 vs .00004\n",
    "\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "\n",
    "# add new fully connected layers for binary classification\n",
    "image_input = base_model.input\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "\n",
    "additional_input = Input(shape=(1,), name=\"additional_input\") # shape is just 1 feature for the confidence \n",
    "y = Dense(64, activation='relu')(additional_input) \n",
    "y = Dropout(0.1)(y) # when .5, the additional input was too powerful, the prediction vals were always either to close to 1 or 0, we try to make the additional input less important than the images\n",
    "\n",
    "combined = Concatenate()([x, y]) # 2 channels\n",
    "combined = Dense(256, activation='relu')(combined)\n",
    "combined = Dense(1, activation='sigmoid')(combined)  # sigmoid for binary \n",
    "\n",
    "model = Model(inputs=[image_input, additional_input], outputs=combined)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'], jit_compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba674e1-757a-4db3-b351-f36bc3c645a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Playing around with different parameters to get the best combination\n",
    "### n_percentage = [5, 10, 15, 20]\n",
    "### max_iterations = [5, 10, 15]\n",
    "### n_percent = [5, 10, 15, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e705eac-f141-4c4b-861e-13af0781541e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Indices: 5\n"
     ]
    }
   ],
   "source": [
    "# has no relevance to the code - just leaving here for our future reference to how we indexed into our lists\n",
    "data_array = np.array([0, 1, 0, 3, 0, 5])  # The array containing 0s and other values\n",
    "index_array = np.array([0, 1, 2, 3, 4, 5])  # The array of indices\n",
    "\n",
    "# filter the index_array where the corresponding value in data_array is not 0\n",
    "filtered_indices = index_array[data_array[index_array] != 0]\n",
    "\n",
    "print(\"Filtered Indices:\", filtered_indices[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74453bdf-a5c0-4764-b238-901b1f3a8810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1...\n",
      "\u001b[1m  5/163\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 42ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 21:43:02.746316: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step\n",
      "Adjusted 208 indices, avg confidence: 0.0000\n",
      "Changed Indices: [np.int64(530), np.int64(578), np.int64(1302), np.int64(1314), np.int64(1329), np.int64(1332), np.int64(1334), np.int64(1371), np.int64(1372), np.int64(1429), np.int64(1440), np.int64(1441), np.int64(1442), np.int64(1446), np.int64(1454), np.int64(1476), np.int64(1484), np.int64(1488), np.int64(1528), np.int64(1534), np.int64(1565), np.int64(1587), np.int64(1594), np.int64(1596), np.int64(1599), np.int64(1622), np.int64(1640), np.int64(1645), np.int64(1660), np.int64(1668), np.int64(1676), np.int64(1682), np.int64(1695), np.int64(1723), np.int64(1785), np.int64(1797), np.int64(1800), np.int64(1821), np.int64(1825), np.int64(1862), np.int64(1873), np.int64(1895), np.int64(1922), np.int64(1927), np.int64(1953), np.int64(1974), np.int64(1977), np.int64(2014), np.int64(2108), np.int64(2119), np.int64(2122), np.int64(2128), np.int64(2144), np.int64(2151), np.int64(2168), np.int64(2173), np.int64(2185), np.int64(2198), np.int64(2210), np.int64(2241), np.int64(2246), np.int64(2272), np.int64(2292), np.int64(2318), np.int64(2326), np.int64(2340), np.int64(2352), np.int64(2369), np.int64(2375), np.int64(2383), np.int64(2447), np.int64(2453), np.int64(2459), np.int64(2464), np.int64(2467), np.int64(2468), np.int64(2478), np.int64(2483), np.int64(2493), np.int64(2498), np.int64(2503), np.int64(2505), np.int64(2553), np.int64(2602), np.int64(2611), np.int64(2643), np.int64(2651), np.int64(2696), np.int64(2713), np.int64(2743), np.int64(2744), np.int64(2755), np.int64(2760), np.int64(2762), np.int64(2807), np.int64(2825), np.int64(2829), np.int64(2854), np.int64(2867), np.int64(2883), np.int64(2915), np.int64(2964), np.int64(2999), np.int64(3016), np.int64(3045), np.int64(3066), np.int64(3068), np.int64(3075), np.int64(3121), np.int64(3133), np.int64(3143), np.int64(3215), np.int64(3264), np.int64(3330), np.int64(3341), np.int64(3385), np.int64(3387), np.int64(3403), np.int64(3407), np.int64(3422), np.int64(3433), np.int64(3434), np.int64(3445), np.int64(3455), np.int64(3495), np.int64(3510), np.int64(3514), np.int64(3522), np.int64(3538), np.int64(3543), np.int64(3552), np.int64(3565), np.int64(3579), np.int64(3587), np.int64(3638), np.int64(3642), np.int64(3656), np.int64(3676), np.int64(3704), np.int64(3723), np.int64(3724), np.int64(3765), np.int64(3789), np.int64(3806), np.int64(3812), np.int64(3887), np.int64(3891), np.int64(3933), np.int64(3947), np.int64(3950), np.int64(3978), np.int64(3981), np.int64(4036), np.int64(4048), np.int64(4098), np.int64(4105), np.int64(4118), np.int64(4135), np.int64(4149), np.int64(4172), np.int64(4179), np.int64(4186), np.int64(4227), np.int64(4230), np.int64(4247), np.int64(4265), np.int64(4285), np.int64(4290), np.int64(4310), np.int64(4320), np.int64(4359), np.int64(4379), np.int64(4396), np.int64(4426), np.int64(4446), np.int64(4466), np.int64(4505), np.int64(4514), np.int64(4526), np.int64(4551), np.int64(4601), np.int64(4656), np.int64(4681), np.int64(4708), np.int64(4720), np.int64(4748), np.int64(4772), np.int64(4774), np.int64(4780), np.int64(4785), np.int64(4804), np.int64(4806), np.int64(4812), np.int64(4813), np.int64(4827), np.int64(4912), np.int64(4917), np.int64(4949), np.int64(5008), np.int64(5012), np.int64(5021), np.int64(5047), np.int64(5058), np.int64(5070), np.int64(5109), np.int64(5138), np.int64(5156), np.int64(5180)]\n",
      "Predictions for Changed Indices: [8.7454914e-09 4.9178166e-08 6.5326432e-08 4.0678364e-07 4.8147109e-07\n",
      " 5.0621708e-07 6.6759429e-07 6.7791018e-07 7.8842908e-07 8.6817801e-07\n",
      " 9.4510557e-07 9.5379073e-07 1.0041176e-06 1.0419724e-06 1.0681204e-06\n",
      " 1.0805153e-06 1.1233801e-06 1.1406205e-06 1.1416716e-06 1.1538555e-06\n",
      " 1.2312950e-06 1.2460990e-06 1.2701067e-06 1.3260403e-06 1.3746859e-06\n",
      " 1.3809694e-06 1.3838802e-06 1.4656190e-06 1.4663628e-06 1.4697257e-06\n",
      " 1.4907180e-06 1.5161340e-06 1.5738259e-06 1.6306590e-06 1.6381906e-06\n",
      " 1.6707337e-06 1.6794688e-06 1.6833428e-06 1.6872096e-06 1.7464076e-06\n",
      " 1.7681029e-06 1.7860057e-06 1.7939212e-06 1.8002951e-06 1.8063902e-06\n",
      " 1.8387078e-06 1.8493561e-06 1.8686075e-06 1.8980552e-06 1.9301381e-06\n",
      " 1.9321549e-06 1.9902473e-06 1.9982961e-06 2.0077612e-06 2.0269306e-06\n",
      " 2.0293212e-06 2.0305022e-06 2.0828886e-06 2.1163069e-06 2.1365695e-06\n",
      " 2.1375988e-06 2.2414129e-06 2.2559268e-06 2.2764884e-06 2.3240752e-06\n",
      " 2.3822017e-06 2.4302801e-06 2.4582143e-06 2.4702374e-06 2.4925432e-06\n",
      " 2.4974497e-06 2.5024515e-06 2.5492150e-06 2.5683903e-06 2.5775628e-06\n",
      " 2.6030905e-06 2.6075804e-06 2.6103794e-06 2.6481055e-06 2.6506677e-06\n",
      " 2.6727209e-06 2.7427541e-06 2.7777592e-06 2.7892531e-06 2.8015857e-06\n",
      " 2.8117113e-06 2.8142358e-06 2.8840520e-06 2.9459002e-06 2.9513192e-06\n",
      " 2.9773953e-06 2.9787075e-06 3.0011997e-06 3.0243546e-06 3.0324788e-06\n",
      " 3.0498775e-06 3.0526974e-06 3.1195007e-06 3.1389282e-06 3.1441739e-06\n",
      " 3.1780889e-06 3.1792556e-06 3.2051535e-06 3.2157043e-06 3.2164680e-06\n",
      " 3.2191042e-06 3.2265175e-06 3.2400044e-06 3.2418218e-06 3.2515409e-06\n",
      " 3.2621790e-06 3.2639996e-06 3.2708704e-06 3.2716564e-06 3.2858052e-06\n",
      " 3.3104518e-06 3.4066788e-06 3.4315838e-06 3.4430045e-06 3.4556458e-06\n",
      " 3.4699585e-06 3.4725147e-06 3.5499904e-06 3.5548521e-06 3.5599921e-06\n",
      " 3.5630180e-06 3.6100064e-06 3.6245124e-06 3.6308813e-06 3.6354618e-06\n",
      " 3.6720662e-06 3.6999597e-06 3.7195912e-06 3.7213265e-06 3.7408288e-06\n",
      " 3.7522805e-06 3.7957363e-06 3.8002743e-06 3.8273633e-06 3.8289227e-06\n",
      " 3.8325247e-06 3.8495914e-06 3.8662356e-06 3.8768876e-06 3.8914750e-06\n",
      " 3.9067727e-06 3.9086844e-06 3.9196625e-06 3.9693737e-06 3.9911965e-06\n",
      " 4.0075661e-06 4.0115892e-06 4.0640157e-06 4.0646632e-06 4.0651516e-06\n",
      " 4.0680684e-06 4.0713130e-06 4.0754694e-06 4.0885884e-06 4.0919590e-06\n",
      " 4.1065523e-06 4.1078642e-06 4.1216535e-06 4.1365424e-06 4.1575076e-06\n",
      " 4.1596609e-06 4.1828962e-06 4.1857697e-06 4.2393126e-06 4.2634115e-06\n",
      " 4.2691199e-06 4.3011755e-06 4.3636578e-06 4.4020821e-06 4.4031449e-06\n",
      " 4.4152930e-06 4.4251192e-06 4.4325739e-06 4.4337366e-06 4.4343537e-06\n",
      " 4.4429466e-06 4.5272591e-06 4.5337870e-06 4.5610695e-06 4.5703964e-06\n",
      " 4.5931788e-06 4.5979777e-06 4.5996753e-06 4.5999868e-06 4.6229475e-06\n",
      " 4.6462756e-06 4.6583041e-06 4.6724927e-06 4.6834266e-06 4.6866035e-06\n",
      " 4.7071367e-06 4.7077783e-06 4.7251451e-06 4.7267854e-06 4.7300955e-06\n",
      " 4.7703411e-06 4.8065813e-06 4.8128104e-06 4.8203988e-06 4.8288648e-06\n",
      " 4.8360357e-06 4.8641073e-06 4.8952720e-06]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [np.int64(578), np.int64(530)]\n",
      "Total Number of Eagles the model believes are not eagles 2\n",
      "Total Number of images the model believes are not eagles 208\n",
      "Change ratio: 0.0400\n",
      "Iteration 2...\n",
      "\u001b[1m  5/163\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 42ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 21:43:35.428100: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step\n",
      "Adjusted 197 indices, avg confidence: 0.0000\n",
      "Changed Indices: [np.int64(731), np.int64(1307), np.int64(1339), np.int64(1400), np.int64(1463), np.int64(1468), np.int64(1487), np.int64(1502), np.int64(1513), np.int64(1521), np.int64(1545), np.int64(1570), np.int64(1611), np.int64(1623), np.int64(1665), np.int64(1666), np.int64(1681), np.int64(1688), np.int64(1696), np.int64(1711), np.int64(1715), np.int64(1719), np.int64(1768), np.int64(1773), np.int64(1778), np.int64(1780), np.int64(1795), np.int64(1859), np.int64(1897), np.int64(1899), np.int64(1965), np.int64(1971), np.int64(1976), np.int64(1984), np.int64(2061), np.int64(2073), np.int64(2079), np.int64(2083), np.int64(2100), np.int64(2103), np.int64(2114), np.int64(2148), np.int64(2160), np.int64(2229), np.int64(2290), np.int64(2296), np.int64(2299), np.int64(2307), np.int64(2313), np.int64(2327), np.int64(2348), np.int64(2357), np.int64(2389), np.int64(2412), np.int64(2423), np.int64(2446), np.int64(2449), np.int64(2499), np.int64(2508), np.int64(2514), np.int64(2519), np.int64(2540), np.int64(2556), np.int64(2587), np.int64(2608), np.int64(2630), np.int64(2632), np.int64(2637), np.int64(2670), np.int64(2742), np.int64(2754), np.int64(2774), np.int64(2805), np.int64(2808), np.int64(2836), np.int64(2922), np.int64(2923), np.int64(2939), np.int64(2965), np.int64(2972), np.int64(2991), np.int64(3000), np.int64(3020), np.int64(3069), np.int64(3095), np.int64(3119), np.int64(3131), np.int64(3132), np.int64(3149), np.int64(3159), np.int64(3198), np.int64(3200), np.int64(3220), np.int64(3235), np.int64(3246), np.int64(3251), np.int64(3262), np.int64(3268), np.int64(3302), np.int64(3307), np.int64(3373), np.int64(3376), np.int64(3420), np.int64(3438), np.int64(3464), np.int64(3544), np.int64(3547), np.int64(3566), np.int64(3585), np.int64(3617), np.int64(3618), np.int64(3623), np.int64(3627), np.int64(3634), np.int64(3696), np.int64(3761), np.int64(3796), np.int64(3804), np.int64(3805), np.int64(3813), np.int64(3833), np.int64(3837), np.int64(3851), np.int64(3860), np.int64(3876), np.int64(3914), np.int64(3923), np.int64(3958), np.int64(3977), np.int64(4011), np.int64(4025), np.int64(4051), np.int64(4079), np.int64(4089), np.int64(4112), np.int64(4156), np.int64(4175), np.int64(4192), np.int64(4207), np.int64(4219), np.int64(4232), np.int64(4243), np.int64(4262), np.int64(4272), np.int64(4282), np.int64(4288), np.int64(4297), np.int64(4342), np.int64(4363), np.int64(4367), np.int64(4382), np.int64(4404), np.int64(4433), np.int64(4443), np.int64(4479), np.int64(4485), np.int64(4503), np.int64(4530), np.int64(4586), np.int64(4650), np.int64(4651), np.int64(4660), np.int64(4666), np.int64(4683), np.int64(4697), np.int64(4757), np.int64(4761), np.int64(4782), np.int64(4786), np.int64(4788), np.int64(4791), np.int64(4809), np.int64(4818), np.int64(4837), np.int64(4840), np.int64(4844), np.int64(4848), np.int64(4852), np.int64(4853), np.int64(4856), np.int64(4857), np.int64(4863), np.int64(4875), np.int64(4889), np.int64(4971), np.int64(4980), np.int64(4993), np.int64(5002), np.int64(5010), np.int64(5026), np.int64(5043), np.int64(5073), np.int64(5084), np.int64(5113), np.int64(5123), np.int64(5128), np.int64(5162)]\n",
      "Predictions for Changed Indices: [2.1148260e-06 2.1254084e-06 2.1603637e-06 2.2529341e-06 2.2619113e-06\n",
      " 2.2627721e-06 2.2876336e-06 2.3098364e-06 2.3269870e-06 2.3394880e-06\n",
      " 2.3542752e-06 2.3573780e-06 2.3644186e-06 2.3733987e-06 2.3794498e-06\n",
      " 2.4110341e-06 2.4144347e-06 2.4266947e-06 2.4271669e-06 2.4654007e-06\n",
      " 2.4685653e-06 2.4831927e-06 2.5051065e-06 2.5120994e-06 2.5151055e-06\n",
      " 2.5273248e-06 2.5374707e-06 2.5395307e-06 2.5397171e-06 2.5503114e-06\n",
      " 2.5679251e-06 2.6290670e-06 2.6293028e-06 2.6614227e-06 2.6697737e-06\n",
      " 2.6702446e-06 2.6730397e-06 2.6737764e-06 2.6745797e-06 2.6794689e-06\n",
      " 2.6834225e-06 2.6868076e-06 2.7095234e-06 2.7102910e-06 2.7123156e-06\n",
      " 2.7230481e-06 2.7342144e-06 2.7447952e-06 2.7534757e-06 2.7566839e-06\n",
      " 2.7743467e-06 2.7810247e-06 2.7968447e-06 2.8050210e-06 2.8105774e-06\n",
      " 2.8228585e-06 2.8277191e-06 2.8410889e-06 2.8447762e-06 2.8744087e-06\n",
      " 2.8808277e-06 2.8839831e-06 2.8881889e-06 2.8923591e-06 2.9011967e-06\n",
      " 2.9107273e-06 2.9209523e-06 2.9327873e-06 2.9359858e-06 2.9440298e-06\n",
      " 2.9542082e-06 2.9816804e-06 2.9893163e-06 2.9901205e-06 3.0140091e-06\n",
      " 3.0165800e-06 3.0170088e-06 3.0200226e-06 3.0264002e-06 3.0486126e-06\n",
      " 3.0514514e-06 3.0790350e-06 3.0812819e-06 3.0907381e-06 3.0936044e-06\n",
      " 3.0948820e-06 3.0974422e-06 3.1075879e-06 3.1179213e-06 3.1182603e-06\n",
      " 3.1190752e-06 3.1199679e-06 3.1237785e-06 3.1314860e-06 3.1318950e-06\n",
      " 3.1426812e-06 3.1507354e-06 3.1603774e-06 3.1711618e-06 3.1741724e-06\n",
      " 3.1812697e-06 3.1918178e-06 3.1944915e-06 3.2009839e-06 3.2045059e-06\n",
      " 3.2132887e-06 3.2141038e-06 3.2245148e-06 3.2507442e-06 3.2549663e-06\n",
      " 3.2650673e-06 3.2656840e-06 3.2750779e-06 3.2851126e-06 3.3393162e-06\n",
      " 3.3473032e-06 3.3478555e-06 3.3544354e-06 3.3666940e-06 3.3722149e-06\n",
      " 3.3784597e-06 3.4282798e-06 3.4284469e-06 3.4378404e-06 3.4386073e-06\n",
      " 3.4497884e-06 3.4597449e-06 3.4626300e-06 3.4814377e-06 3.4883931e-06\n",
      " 3.4888890e-06 3.5036533e-06 3.5050637e-06 3.5103758e-06 3.5151797e-06\n",
      " 3.5226567e-06 3.5381372e-06 3.5385253e-06 3.5506000e-06 3.5700862e-06\n",
      " 3.5713731e-06 3.5951025e-06 3.5958979e-06 3.6147637e-06 3.6162637e-06\n",
      " 3.6189617e-06 3.6191270e-06 3.6223526e-06 3.6232507e-06 3.6232577e-06\n",
      " 3.6234305e-06 3.6407528e-06 3.6425208e-06 3.6458014e-06 3.6480274e-06\n",
      " 3.6504493e-06 3.6785157e-06 3.6842803e-06 3.6888543e-06 3.7240350e-06\n",
      " 3.7316822e-06 3.7526061e-06 3.7542061e-06 3.7813884e-06 3.8078240e-06\n",
      " 3.8131950e-06 3.8269109e-06 3.8349231e-06 3.8476605e-06 3.8594535e-06\n",
      " 3.8606759e-06 3.8859416e-06 3.8983762e-06 3.9032348e-06 3.9077490e-06\n",
      " 3.9310653e-06 3.9395659e-06 3.9400957e-06 3.9448560e-06 3.9598872e-06\n",
      " 3.9601555e-06 3.9817601e-06 3.9852171e-06 3.9859051e-06 3.9917672e-06\n",
      " 3.9917900e-06 3.9951074e-06 3.9976035e-06 3.9992433e-06 4.0013642e-06\n",
      " 4.0079867e-06 4.0148616e-06 4.0187651e-06 4.0352634e-06 4.0459872e-06\n",
      " 4.0741793e-06 4.0756172e-06]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [np.int64(731)]\n",
      "Total Number of Eagles the model believes are not eagles 3\n",
      "Total Number of images the model believes are not eagles 405\n",
      "Change ratio: 0.0379\n",
      "Iteration 3...\n",
      "\u001b[1m  5/163\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 42ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 21:44:07.944383: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step\n",
      "Adjusted 187 indices, avg confidence: 0.0000\n",
      "Changed Indices: [np.int64(1308), np.int64(1344), np.int64(1376), np.int64(1383), np.int64(1388), np.int64(1418), np.int64(1470), np.int64(1493), np.int64(1552), np.int64(1578), np.int64(1580), np.int64(1609), np.int64(1786), np.int64(1812), np.int64(1817), np.int64(1828), np.int64(1833), np.int64(1835), np.int64(1842), np.int64(1889), np.int64(1905), np.int64(1913), np.int64(1942), np.int64(1954), np.int64(1962), np.int64(1981), np.int64(1982), np.int64(1987), np.int64(1998), np.int64(2002), np.int64(2016), np.int64(2035), np.int64(2038), np.int64(2046), np.int64(2075), np.int64(2094), np.int64(2112), np.int64(2131), np.int64(2133), np.int64(2156), np.int64(2157), np.int64(2161), np.int64(2183), np.int64(2190), np.int64(2359), np.int64(2360), np.int64(2372), np.int64(2380), np.int64(2395), np.int64(2397), np.int64(2422), np.int64(2495), np.int64(2555), np.int64(2557), np.int64(2564), np.int64(2569), np.int64(2601), np.int64(2615), np.int64(2623), np.int64(2638), np.int64(2655), np.int64(2660), np.int64(2665), np.int64(2686), np.int64(2690), np.int64(2703), np.int64(2729), np.int64(2756), np.int64(2761), np.int64(2813), np.int64(2844), np.int64(2859), np.int64(2871), np.int64(2885), np.int64(2898), np.int64(2901), np.int64(2921), np.int64(2940), np.int64(2960), np.int64(2983), np.int64(2987), np.int64(2995), np.int64(3009), np.int64(3025), np.int64(3039), np.int64(3044), np.int64(3059), np.int64(3093), np.int64(3104), np.int64(3155), np.int64(3167), np.int64(3189), np.int64(3233), np.int64(3316), np.int64(3324), np.int64(3333), np.int64(3344), np.int64(3360), np.int64(3427), np.int64(3439), np.int64(3451), np.int64(3460), np.int64(3489), np.int64(3497), np.int64(3499), np.int64(3527), np.int64(3573), np.int64(3584), np.int64(3593), np.int64(3596), np.int64(3600), np.int64(3613), np.int64(3649), np.int64(3670), np.int64(3678), np.int64(3684), np.int64(3706), np.int64(3725), np.int64(3754), np.int64(3819), np.int64(3870), np.int64(3902), np.int64(3908), np.int64(3957), np.int64(3975), np.int64(3987), np.int64(4003), np.int64(4017), np.int64(4029), np.int64(4032), np.int64(4034), np.int64(4055), np.int64(4071), np.int64(4073), np.int64(4083), np.int64(4093), np.int64(4106), np.int64(4128), np.int64(4129), np.int64(4240), np.int64(4260), np.int64(4318), np.int64(4328), np.int64(4335), np.int64(4355), np.int64(4361), np.int64(4394), np.int64(4399), np.int64(4414), np.int64(4424), np.int64(4442), np.int64(4448), np.int64(4450), np.int64(4475), np.int64(4486), np.int64(4499), np.int64(4509), np.int64(4550), np.int64(4567), np.int64(4588), np.int64(4629), np.int64(4634), np.int64(4659), np.int64(4687), np.int64(4694), np.int64(4705), np.int64(4743), np.int64(4744), np.int64(4745), np.int64(4778), np.int64(4829), np.int64(4842), np.int64(4877), np.int64(4899), np.int64(4923), np.int64(4939), np.int64(4952), np.int64(4958), np.int64(4988), np.int64(4994), np.int64(4996), np.int64(5097), np.int64(5100), np.int64(5135), np.int64(5164), np.int64(5175), np.int64(5186)]\n",
      "Predictions for Changed Indices: [4.5125598e-06 4.8507331e-06 5.8275286e-06 5.8598239e-06 5.8853111e-06\n",
      " 5.8853111e-06 5.9954218e-06 6.0514517e-06 6.0841226e-06 6.1762098e-06\n",
      " 6.2049194e-06 6.2569193e-06 6.2630870e-06 6.2641616e-06 6.3346902e-06\n",
      " 6.3410366e-06 6.3420703e-06 6.3699717e-06 6.3769926e-06 6.3818284e-06\n",
      " 6.3902630e-06 6.3944635e-06 6.4547153e-06 6.4585702e-06 6.4764940e-06\n",
      " 6.4891929e-06 6.5184254e-06 6.5505365e-06 6.5757736e-06 6.5881895e-06\n",
      " 6.5974191e-06 6.6158427e-06 6.6171929e-06 6.6382272e-06 6.6760786e-06\n",
      " 6.6982461e-06 6.6994471e-06 6.7103688e-06 6.7456649e-06 6.7624442e-06\n",
      " 6.7945853e-06 6.8225122e-06 6.8535919e-06 6.8571157e-06 6.8992276e-06\n",
      " 6.9084585e-06 6.9155903e-06 6.9189018e-06 6.9354965e-06 6.9430143e-06\n",
      " 6.9560047e-06 6.9871130e-06 6.9924522e-06 6.9931193e-06 7.0116639e-06\n",
      " 7.0154160e-06 7.0214533e-06 7.0261685e-06 7.0313567e-06 7.0379988e-06\n",
      " 7.0415035e-06 7.0666297e-06 7.0933947e-06 7.0944361e-06 7.1152576e-06\n",
      " 7.1645281e-06 7.1842396e-06 7.1928780e-06 7.2081016e-06 7.2162038e-06\n",
      " 7.2527846e-06 7.2536486e-06 7.2619896e-06 7.2762837e-06 7.2790872e-06\n",
      " 7.3125957e-06 7.3305973e-06 7.3323586e-06 7.3362971e-06 7.3661831e-06\n",
      " 7.4089476e-06 7.4175232e-06 7.4396266e-06 7.4591562e-06 7.4700615e-06\n",
      " 7.4740947e-06 7.4892218e-06 7.4973400e-06 7.5088747e-06 7.5226867e-06\n",
      " 7.5422895e-06 7.5610437e-06 7.5631274e-06 7.5871831e-06 7.5882044e-06\n",
      " 7.5913167e-06 7.5953571e-06 7.6046194e-06 7.6069118e-06 7.6305041e-06\n",
      " 7.6494844e-06 7.6566512e-06 7.6593169e-06 7.6634442e-06 7.6672459e-06\n",
      " 7.6709248e-06 7.6805218e-06 7.7241530e-06 7.7291497e-06 7.7312507e-06\n",
      " 7.7752302e-06 7.7911736e-06 7.7979603e-06 7.8109106e-06 7.8332750e-06\n",
      " 7.8609191e-06 7.8836820e-06 7.8883068e-06 7.9183874e-06 7.9323472e-06\n",
      " 7.9450592e-06 7.9664087e-06 7.9862548e-06 7.9929523e-06 8.0007239e-06\n",
      " 8.0177879e-06 8.0315858e-06 8.0319687e-06 8.0341142e-06 8.0356076e-06\n",
      " 8.0519394e-06 8.0762957e-06 8.0817508e-06 8.1285843e-06 8.1380467e-06\n",
      " 8.1402977e-06 8.1472408e-06 8.1514381e-06 8.1644539e-06 8.1876824e-06\n",
      " 8.1877597e-06 8.1901180e-06 8.2095512e-06 8.2221104e-06 8.2373681e-06\n",
      " 8.2423894e-06 8.2497972e-06 8.2672032e-06 8.2706174e-06 8.3021014e-06\n",
      " 8.3026234e-06 8.3031136e-06 8.3313516e-06 8.3451714e-06 8.3549585e-06\n",
      " 8.3751329e-06 8.3769382e-06 8.3778496e-06 8.4824997e-06 8.5142128e-06\n",
      " 8.5168922e-06 8.5202073e-06 8.5322490e-06 8.5588572e-06 8.5636093e-06\n",
      " 8.6022274e-06 8.6025475e-06 8.6308728e-06 8.6412174e-06 8.6758073e-06\n",
      " 8.6764276e-06 8.6779419e-06 8.6783812e-06 8.6886494e-06 8.6942191e-06\n",
      " 8.7328590e-06 8.7407170e-06 8.7471799e-06 8.7477965e-06 8.7526205e-06\n",
      " 8.7613553e-06 8.7829558e-06 8.8273091e-06 8.8322777e-06 8.8450051e-06\n",
      " 8.8922416e-06 8.8976458e-06]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: []\n",
      "Total Number of Eagles the model believes are not eagles 3\n",
      "Total Number of images the model believes are not eagles 592\n",
      "Change ratio: 0.0360\n",
      "Iteration 4...\n",
      "\u001b[1m  5/163\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 42ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 21:44:40.673121: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step\n",
      "Adjusted 178 indices, avg confidence: 0.0000\n",
      "Changed Indices: [np.int64(1331), np.int64(1349), np.int64(1387), np.int64(1430), np.int64(1436), np.int64(1518), np.int64(1526), np.int64(1553), np.int64(1607), np.int64(1619), np.int64(1620), np.int64(1629), np.int64(1689), np.int64(1701), np.int64(1716), np.int64(1718), np.int64(1724), np.int64(1731), np.int64(1744), np.int64(1762), np.int64(1770), np.int64(1826), np.int64(1840), np.int64(1858), np.int64(1928), np.int64(1943), np.int64(1979), np.int64(1994), np.int64(1999), np.int64(2008), np.int64(2015), np.int64(2045), np.int64(2066), np.int64(2121), np.int64(2180), np.int64(2203), np.int64(2217), np.int64(2225), np.int64(2239), np.int64(2297), np.int64(2315), np.int64(2364), np.int64(2426), np.int64(2433), np.int64(2435), np.int64(2482), np.int64(2512), np.int64(2528), np.int64(2549), np.int64(2551), np.int64(2559), np.int64(2563), np.int64(2565), np.int64(2570), np.int64(2575), np.int64(2579), np.int64(2584), np.int64(2598), np.int64(2605), np.int64(2657), np.int64(2797), np.int64(2798), np.int64(2828), np.int64(2830), np.int64(2839), np.int64(2842), np.int64(2845), np.int64(2886), np.int64(2959), np.int64(2982), np.int64(2988), np.int64(3013), np.int64(3017), np.int64(3023), np.int64(3031), np.int64(3055), np.int64(3060), np.int64(3087), np.int64(3101), np.int64(3130), np.int64(3137), np.int64(3141), np.int64(3157), np.int64(3181), np.int64(3196), np.int64(3222), np.int64(3223), np.int64(3228), np.int64(3248), np.int64(3271), np.int64(3273), np.int64(3276), np.int64(3284), np.int64(3305), np.int64(3308), np.int64(3310), np.int64(3323), np.int64(3338), np.int64(3348), np.int64(3354), np.int64(3359), np.int64(3364), np.int64(3386), np.int64(3413), np.int64(3442), np.int64(3483), np.int64(3511), np.int64(3516), np.int64(3534), np.int64(3548), np.int64(3582), np.int64(3679), np.int64(3700), np.int64(3710), np.int64(3736), np.int64(3758), np.int64(3776), np.int64(3783), np.int64(3854), np.int64(3856), np.int64(3866), np.int64(3879), np.int64(3881), np.int64(3916), np.int64(3925), np.int64(3942), np.int64(3964), np.int64(3986), np.int64(4002), np.int64(4010), np.int64(4061), np.int64(4065), np.int64(4070), np.int64(4088), np.int64(4122), np.int64(4124), np.int64(4144), np.int64(4157), np.int64(4160), np.int64(4178), np.int64(4212), np.int64(4222), np.int64(4251), np.int64(4266), np.int64(4300), np.int64(4323), np.int64(4338), np.int64(4413), np.int64(4465), np.int64(4491), np.int64(4492), np.int64(4512), np.int64(4524), np.int64(4572), np.int64(4577), np.int64(4589), np.int64(4603), np.int64(4607), np.int64(4608), np.int64(4614), np.int64(4633), np.int64(4676), np.int64(4734), np.int64(4766), np.int64(4819), np.int64(4887), np.int64(4905), np.int64(4926), np.int64(4950), np.int64(4981), np.int64(5004), np.int64(5006), np.int64(5112), np.int64(5132), np.int64(5133), np.int64(5144), np.int64(5155), np.int64(5167)]\n",
      "Predictions for Changed Indices: [6.7231158e-06 6.9327989e-06 6.9700222e-06 7.0125534e-06 7.1109434e-06\n",
      " 7.1142808e-06 7.1228219e-06 7.2067405e-06 7.2206994e-06 7.2306775e-06\n",
      " 7.2588182e-06 7.2705816e-06 7.2918292e-06 7.2946118e-06 7.3113270e-06\n",
      " 7.3868796e-06 7.3924325e-06 7.3946749e-06 7.4143763e-06 7.4249833e-06\n",
      " 7.4310683e-06 7.4407694e-06 7.4439058e-06 7.4621803e-06 7.4643654e-06\n",
      " 7.4664154e-06 7.4796289e-06 7.4912714e-06 7.5173866e-06 7.5202829e-06\n",
      " 7.5305456e-06 7.5439593e-06 7.5505882e-06 7.5651396e-06 7.5961830e-06\n",
      " 7.5981393e-06 7.6170900e-06 7.6317483e-06 7.6325196e-06 7.6344268e-06\n",
      " 7.6454726e-06 7.6601282e-06 7.6653960e-06 7.6680508e-06 7.6725710e-06\n",
      " 7.6755941e-06 7.6782298e-06 7.6784563e-06 7.6875422e-06 7.7182704e-06\n",
      " 7.7227032e-06 7.7350041e-06 7.7353507e-06 7.7373797e-06 7.7477471e-06\n",
      " 7.7575878e-06 7.7645673e-06 7.7783970e-06 7.7784116e-06 7.7799177e-06\n",
      " 7.7822106e-06 7.7871546e-06 7.8009061e-06 7.8095854e-06 7.8151133e-06\n",
      " 7.8193925e-06 7.8344628e-06 7.8366975e-06 7.8439498e-06 7.8567145e-06\n",
      " 7.8714966e-06 7.8910762e-06 7.9024703e-06 7.9280744e-06 7.9332776e-06\n",
      " 7.9502279e-06 7.9523279e-06 7.9814590e-06 7.9943557e-06 8.0000909e-06\n",
      " 8.0057689e-06 8.0126738e-06 8.0236314e-06 8.0259506e-06 8.0375476e-06\n",
      " 8.0633281e-06 8.0692434e-06 8.0740538e-06 8.0913360e-06 8.0939599e-06\n",
      " 8.1013659e-06 8.1249646e-06 8.1272665e-06 8.1332055e-06 8.1424796e-06\n",
      " 8.1429844e-06 8.1577691e-06 8.1636517e-06 8.2146735e-06 8.2351062e-06\n",
      " 8.2590077e-06 8.2650668e-06 8.2679990e-06 8.2699707e-06 8.2771512e-06\n",
      " 8.2905808e-06 8.2945980e-06 8.3020368e-06 8.3035738e-06 8.3063142e-06\n",
      " 8.3067098e-06 8.3126533e-06 8.3174264e-06 8.3398963e-06 8.3558907e-06\n",
      " 8.3617579e-06 8.3644454e-06 8.3787445e-06 8.3802624e-06 8.3927553e-06\n",
      " 8.3996101e-06 8.4001540e-06 8.4023659e-06 8.4195481e-06 8.4217718e-06\n",
      " 8.4531939e-06 8.4548055e-06 8.4670955e-06 8.4704789e-06 8.4784961e-06\n",
      " 8.4815374e-06 8.4882613e-06 8.4886178e-06 8.4893618e-06 8.4985395e-06\n",
      " 8.5147485e-06 8.5155280e-06 8.5201982e-06 8.5218890e-06 8.5273605e-06\n",
      " 8.5334941e-06 8.5742822e-06 8.5756637e-06 8.5917491e-06 8.5923393e-06\n",
      " 8.5923639e-06 8.5968641e-06 8.6121099e-06 8.6145255e-06 8.6173359e-06\n",
      " 8.6219798e-06 8.6394875e-06 8.6442924e-06 8.6535629e-06 8.6672562e-06\n",
      " 8.6678929e-06 8.7008302e-06 8.7026474e-06 8.7035360e-06 8.7211329e-06\n",
      " 8.7223971e-06 8.7336175e-06 8.7603948e-06 8.7832823e-06 8.7973658e-06\n",
      " 8.8098586e-06 8.8146153e-06 8.8192319e-06 8.8736224e-06 8.8795387e-06\n",
      " 8.8827910e-06 8.8851466e-06 8.8932511e-06 8.9223049e-06 8.9468431e-06\n",
      " 8.9527666e-06 8.9593095e-06 8.9625819e-06]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: []\n",
      "Total Number of Eagles the model believes are not eagles 3\n",
      "Total Number of images the model believes are not eagles 770\n",
      "Change ratio: 0.0342\n",
      "Iteration 5...\n",
      "\u001b[1m  5/163\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 43ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 21:45:13.482331: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step\n",
      "Adjusted 169 indices, avg confidence: 0.0000\n",
      "Changed Indices: [np.int64(1301), np.int64(1311), np.int64(1322), np.int64(1361), np.int64(1369), np.int64(1370), np.int64(1424), np.int64(1432), np.int64(1478), np.int64(1480), np.int64(1482), np.int64(1507), np.int64(1548), np.int64(1549), np.int64(1568), np.int64(1575), np.int64(1635), np.int64(1643), np.int64(1739), np.int64(1754), np.int64(1771), np.int64(1776), np.int64(1849), np.int64(1854), np.int64(1881), np.int64(1894), np.int64(1907), np.int64(1912), np.int64(1921), np.int64(1931), np.int64(2004), np.int64(2025), np.int64(2036), np.int64(2154), np.int64(2226), np.int64(2230), np.int64(2236), np.int64(2249), np.int64(2301), np.int64(2306), np.int64(2343), np.int64(2347), np.int64(2393), np.int64(2466), np.int64(2506), np.int64(2521), np.int64(2525), np.int64(2538), np.int64(2554), np.int64(2617), np.int64(2652), np.int64(2679), np.int64(2683), np.int64(2685), np.int64(2746), np.int64(2796), np.int64(2809), np.int64(2876), np.int64(2893), np.int64(2903), np.int64(2908), np.int64(2912), np.int64(2926), np.int64(2971), np.int64(2993), np.int64(3004), np.int64(3008), np.int64(3026), np.int64(3027), np.int64(3028), np.int64(3038), np.int64(3042), np.int64(3049), np.int64(3063), np.int64(3076), np.int64(3086), np.int64(3089), np.int64(3108), np.int64(3117), np.int64(3129), np.int64(3162), np.int64(3174), np.int64(3205), np.int64(3213), np.int64(3224), np.int64(3231), np.int64(3303), np.int64(3365), np.int64(3487), np.int64(3502), np.int64(3508), np.int64(3528), np.int64(3537), np.int64(3549), np.int64(3572), np.int64(3601), np.int64(3608), np.int64(3622), np.int64(3681), np.int64(3733), np.int64(3756), np.int64(3767), np.int64(3769), np.int64(3770), np.int64(3807), np.int64(3846), np.int64(3869), np.int64(3872), np.int64(3899), np.int64(3937), np.int64(4050), np.int64(4068), np.int64(4099), np.int64(4138), np.int64(4150), np.int64(4162), np.int64(4190), np.int64(4193), np.int64(4199), np.int64(4201), np.int64(4213), np.int64(4239), np.int64(4246), np.int64(4279), np.int64(4283), np.int64(4287), np.int64(4289), np.int64(4304), np.int64(4337), np.int64(4346), np.int64(4372), np.int64(4417), np.int64(4419), np.int64(4420), np.int64(4430), np.int64(4451), np.int64(4495), np.int64(4496), np.int64(4541), np.int64(4544), np.int64(4564), np.int64(4613), np.int64(4636), np.int64(4646), np.int64(4685), np.int64(4724), np.int64(4727), np.int64(4735), np.int64(4739), np.int64(4741), np.int64(4758), np.int64(4793), np.int64(4794), np.int64(4797), np.int64(4799), np.int64(4807), np.int64(4808), np.int64(4810), np.int64(4824), np.int64(4943), np.int64(4946), np.int64(4955), np.int64(4959), np.int64(4962), np.int64(5005), np.int64(5019), np.int64(5102), np.int64(5116), np.int64(5146)]\n",
      "Predictions for Changed Indices: [8.59178999e-06 8.89691546e-06 8.96661732e-06 9.19071863e-06\n",
      " 9.23872631e-06 9.32460352e-06 9.48862089e-06 9.50555841e-06\n",
      " 9.50773392e-06 9.50858521e-06 9.54053212e-06 9.56180065e-06\n",
      " 9.57011162e-06 9.58398505e-06 9.61059050e-06 9.63038110e-06\n",
      " 9.63196089e-06 9.63368802e-06 9.66912376e-06 9.68476616e-06\n",
      " 9.69310986e-06 9.69799203e-06 9.69865778e-06 9.70819929e-06\n",
      " 9.72618636e-06 9.72917314e-06 9.76617139e-06 9.78153275e-06\n",
      " 9.79282595e-06 9.79532888e-06 9.80465666e-06 9.81718495e-06\n",
      " 9.81753146e-06 9.81863559e-06 9.82691745e-06 9.83117206e-06\n",
      " 9.83469818e-06 9.85905808e-06 9.89877753e-06 9.90049648e-06\n",
      " 9.90963963e-06 9.93545382e-06 9.95328264e-06 9.99102849e-06\n",
      " 1.00023062e-05 1.00032694e-05 1.00140460e-05 1.00416546e-05\n",
      " 1.00462630e-05 1.00528368e-05 1.00530870e-05 1.00695515e-05\n",
      " 1.01029100e-05 1.01034111e-05 1.01055593e-05 1.01116520e-05\n",
      " 1.01194564e-05 1.01371133e-05 1.01416481e-05 1.01429059e-05\n",
      " 1.01509859e-05 1.01582873e-05 1.01773612e-05 1.01813321e-05\n",
      " 1.01902488e-05 1.01916285e-05 1.02011190e-05 1.02038639e-05\n",
      " 1.02120594e-05 1.02181584e-05 1.02268250e-05 1.02303848e-05\n",
      " 1.02430577e-05 1.02439562e-05 1.02569475e-05 1.02679387e-05\n",
      " 1.02712484e-05 1.02779895e-05 1.02781269e-05 1.02844024e-05\n",
      " 1.02893264e-05 1.02903678e-05 1.02994773e-05 1.03088332e-05\n",
      " 1.03167895e-05 1.03271350e-05 1.03494367e-05 1.03573557e-05\n",
      " 1.03698476e-05 1.04059573e-05 1.04286983e-05 1.04410574e-05\n",
      " 1.04489663e-05 1.04778846e-05 1.04969076e-05 1.05027348e-05\n",
      " 1.05089675e-05 1.05359413e-05 1.05381923e-05 1.05494946e-05\n",
      " 1.05778017e-05 1.05778427e-05 1.06016059e-05 1.06045272e-05\n",
      " 1.06135021e-05 1.06232528e-05 1.06257967e-05 1.06437274e-05\n",
      " 1.06458092e-05 1.06688085e-05 1.06817688e-05 1.06862117e-05\n",
      " 1.06934494e-05 1.07070919e-05 1.07340729e-05 1.07378009e-05\n",
      " 1.07462629e-05 1.07996038e-05 1.08290787e-05 1.08379218e-05\n",
      " 1.08630156e-05 1.08758486e-05 1.08791573e-05 1.09144075e-05\n",
      " 1.09174580e-05 1.09816874e-05 1.09880566e-05 1.09886960e-05\n",
      " 1.10053606e-05 1.10157653e-05 1.10234687e-05 1.10250985e-05\n",
      " 1.10285573e-05 1.10298524e-05 1.10314822e-05 1.10597985e-05\n",
      " 1.10845594e-05 1.10849705e-05 1.10893270e-05 1.11078925e-05\n",
      " 1.11147801e-05 1.11603040e-05 1.11630179e-05 1.11831132e-05\n",
      " 1.11854715e-05 1.11870922e-05 1.12060461e-05 1.12098187e-05\n",
      " 1.12107700e-05 1.12397702e-05 1.12519965e-05 1.12638500e-05\n",
      " 1.12874641e-05 1.13422357e-05 1.13495498e-05 1.13503947e-05\n",
      " 1.13521373e-05 1.13651577e-05 1.13754368e-05 1.13996985e-05\n",
      " 1.14185759e-05 1.14297764e-05 1.14298209e-05 1.14465856e-05\n",
      " 1.14472950e-05 1.14923623e-05 1.14927780e-05 1.15211778e-05\n",
      " 1.15310486e-05]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: []\n",
      "Total Number of Eagles the model believes are not eagles 3\n",
      "Total Number of images the model believes are not eagles 939\n",
      "Change ratio: 0.0325\n",
      "Iteration 6...\n",
      "\u001b[1m  5/163\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 42ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 21:45:46.022273: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step\n",
      "Adjusted 161 indices, avg confidence: 0.0000\n",
      "Changed Indices: [np.int64(537), np.int64(1083), np.int64(1087), np.int64(1109), np.int64(1325), np.int64(1337), np.int64(1355), np.int64(1404), np.int64(1417), np.int64(1445), np.int64(1512), np.int64(1537), np.int64(1579), np.int64(1648), np.int64(1678), np.int64(1700), np.int64(1745), np.int64(1761), np.int64(1765), np.int64(1769), np.int64(1781), np.int64(1837), np.int64(1843), np.int64(1903), np.int64(1939), np.int64(1955), np.int64(1957), np.int64(1985), np.int64(2001), np.int64(2044), np.int64(2072), np.int64(2085), np.int64(2095), np.int64(2135), np.int64(2189), np.int64(2197), np.int64(2201), np.int64(2209), np.int64(2256), np.int64(2279), np.int64(2291), np.int64(2317), np.int64(2332), np.int64(2333), np.int64(2342), np.int64(2370), np.int64(2386), np.int64(2409), np.int64(2439), np.int64(2463), np.int64(2486), np.int64(2496), np.int64(2500), np.int64(2524), np.int64(2532), np.int64(2537), np.int64(2539), np.int64(2546), np.int64(2558), np.int64(2593), np.int64(2667), np.int64(2693), np.int64(2730), np.int64(2741), np.int64(2811), np.int64(2832), np.int64(2848), np.int64(2878), np.int64(2888), np.int64(2896), np.int64(2938), np.int64(2955), np.int64(2996), np.int64(2998), np.int64(3003), np.int64(3024), np.int64(3064), np.int64(3156), np.int64(3184), np.int64(3190), np.int64(3237), np.int64(3270), np.int64(3290), np.int64(3306), np.int64(3311), np.int64(3319), np.int64(3326), np.int64(3346), np.int64(3381), np.int64(3397), np.int64(3398), np.int64(3430), np.int64(3454), np.int64(3456), np.int64(3492), np.int64(3494), np.int64(3520), np.int64(3550), np.int64(3588), np.int64(3645), np.int64(3705), np.int64(3714), np.int64(3759), np.int64(3772), np.int64(3785), np.int64(3792), np.int64(3797), np.int64(3830), np.int64(3831), np.int64(3840), np.int64(3871), np.int64(3886), np.int64(3915), np.int64(4005), np.int64(4045), np.int64(4084), np.int64(4109), np.int64(4115), np.int64(4117), np.int64(4185), np.int64(4198), np.int64(4204), np.int64(4216), np.int64(4256), np.int64(4271), np.int64(4275), np.int64(4293), np.int64(4305), np.int64(4313), np.int64(4409), np.int64(4429), np.int64(4445), np.int64(4473), np.int64(4554), np.int64(4560), np.int64(4580), np.int64(4587), np.int64(4627), np.int64(4628), np.int64(4667), np.int64(4702), np.int64(4704), np.int64(4723), np.int64(4728), np.int64(4753), np.int64(4762), np.int64(4769), np.int64(4915), np.int64(4919), np.int64(4927), np.int64(4928), np.int64(4935), np.int64(4973), np.int64(5001), np.int64(5024), np.int64(5030), np.int64(5060), np.int64(5124), np.int64(5141), np.int64(5142), np.int64(5187)]\n",
      "Predictions for Changed Indices: [1.14848017e-05 1.17420705e-05 1.18649805e-05 1.18711259e-05\n",
      " 1.20921404e-05 1.21496587e-05 1.21828080e-05 1.22123784e-05\n",
      " 1.23041100e-05 1.24156268e-05 1.24449780e-05 1.24815151e-05\n",
      " 1.26060686e-05 1.26141867e-05 1.27177154e-05 1.27315488e-05\n",
      " 1.27516469e-05 1.27566582e-05 1.28238789e-05 1.28343636e-05\n",
      " 1.28504316e-05 1.28649253e-05 1.28974543e-05 1.29808332e-05\n",
      " 1.31835141e-05 1.32047153e-05 1.32283103e-05 1.32287132e-05\n",
      " 1.32390369e-05 1.32406039e-05 1.32583682e-05 1.32778541e-05\n",
      " 1.32804635e-05 1.33035082e-05 1.33368658e-05 1.33658205e-05\n",
      " 1.33814574e-05 1.33978519e-05 1.34147031e-05 1.34265038e-05\n",
      " 1.34350084e-05 1.34821130e-05 1.34930715e-05 1.34997645e-05\n",
      " 1.35185746e-05 1.35444334e-05 1.35865048e-05 1.36018161e-05\n",
      " 1.36239769e-05 1.36733997e-05 1.36745857e-05 1.37399193e-05\n",
      " 1.37468151e-05 1.37501829e-05 1.38028799e-05 1.38062642e-05\n",
      " 1.38152200e-05 1.38423484e-05 1.38559644e-05 1.38833711e-05\n",
      " 1.38846945e-05 1.38927871e-05 1.39538406e-05 1.39714566e-05\n",
      " 1.40121938e-05 1.40419443e-05 1.40849161e-05 1.41078635e-05\n",
      " 1.41249611e-05 1.41525479e-05 1.41613909e-05 1.41704968e-05\n",
      " 1.41789042e-05 1.42010704e-05 1.42209665e-05 1.42364888e-05\n",
      " 1.42427489e-05 1.42584195e-05 1.42722956e-05 1.42724848e-05\n",
      " 1.43072357e-05 1.43179923e-05 1.43721109e-05 1.43732632e-05\n",
      " 1.44201358e-05 1.44365631e-05 1.44437527e-05 1.44528185e-05\n",
      " 1.44617543e-05 1.44637543e-05 1.44873866e-05 1.45016102e-05\n",
      " 1.45031872e-05 1.45235617e-05 1.45271488e-05 1.45307522e-05\n",
      " 1.45367112e-05 1.45421891e-05 1.45684926e-05 1.45956519e-05\n",
      " 1.46178563e-05 1.46206294e-05 1.46372731e-05 1.46544808e-05\n",
      " 1.46574994e-05 1.46619323e-05 1.47108831e-05 1.47140809e-05\n",
      " 1.47165938e-05 1.47265200e-05 1.47516657e-05 1.47665723e-05\n",
      " 1.48020581e-05 1.48054332e-05 1.48285635e-05 1.48533045e-05\n",
      " 1.48627560e-05 1.48856489e-05 1.49128182e-05 1.49142397e-05\n",
      " 1.49297375e-05 1.49375992e-05 1.49578964e-05 1.49712678e-05\n",
      " 1.49927018e-05 1.49946882e-05 1.49997531e-05 1.50032711e-05\n",
      " 1.50254937e-05 1.50279011e-05 1.50318419e-05 1.50481937e-05\n",
      " 1.50482228e-05 1.50581127e-05 1.50600818e-05 1.50625801e-05\n",
      " 1.50715332e-05 1.50810210e-05 1.50835112e-05 1.50934675e-05\n",
      " 1.50955984e-05 1.51076356e-05 1.51308786e-05 1.51328995e-05\n",
      " 1.51750410e-05 1.51929398e-05 1.51998083e-05 1.52008661e-05\n",
      " 1.52085668e-05 1.52224093e-05 1.52308030e-05 1.52537104e-05\n",
      " 1.52593420e-05 1.52687880e-05 1.52836765e-05 1.52848716e-05\n",
      " 1.53297024e-05 1.53329929e-05 1.53336950e-05 1.53809997e-05\n",
      " 1.53838755e-05]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [np.int64(1087), np.int64(537), np.int64(1109), np.int64(1083)]\n",
      "Total Number of Eagles the model believes are not eagles 7\n",
      "Total Number of images the model believes are not eagles 1100\n",
      "Change ratio: 0.0310\n",
      "Iteration 7...\n",
      "\u001b[1m  5/163\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 42ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 21:46:19.058811: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step\n",
      "Adjusted 153 indices, avg confidence: 0.0000\n",
      "Changed Indices: [np.int64(698), np.int64(1306), np.int64(1379), np.int64(1403), np.int64(1414), np.int64(1415), np.int64(1428), np.int64(1459), np.int64(1462), np.int64(1501), np.int64(1535), np.int64(1558), np.int64(1567), np.int64(1585), np.int64(1600), np.int64(1646), np.int64(1753), np.int64(1777), np.int64(1779), np.int64(1787), np.int64(1810), np.int64(1823), np.int64(1829), np.int64(1870), np.int64(1882), np.int64(1892), np.int64(1938), np.int64(1950), np.int64(1960), np.int64(1969), np.int64(2018), np.int64(2037), np.int64(2086), np.int64(2091), np.int64(2109), np.int64(2132), np.int64(2150), np.int64(2260), np.int64(2266), np.int64(2269), np.int64(2302), np.int64(2353), np.int64(2368), np.int64(2373), np.int64(2390), np.int64(2394), np.int64(2403), np.int64(2404), np.int64(2416), np.int64(2431), np.int64(2497), np.int64(2509), np.int64(2523), np.int64(2542), np.int64(2573), np.int64(2583), np.int64(2592), np.int64(2629), np.int64(2631), np.int64(2656), np.int64(2726), np.int64(2733), np.int64(2745), np.int64(2753), np.int64(2757), np.int64(2775), np.int64(2786), np.int64(2801), np.int64(2826), np.int64(2835), np.int64(2857), np.int64(2941), np.int64(2947), np.int64(2953), np.int64(3018), np.int64(3021), np.int64(3081), np.int64(3083), np.int64(3120), np.int64(3153), np.int64(3154), np.int64(3164), np.int64(3165), np.int64(3210), np.int64(3236), np.int64(3269), np.int64(3272), np.int64(3294), np.int64(3299), np.int64(3322), np.int64(3389), np.int64(3411), np.int64(3412), np.int64(3418), np.int64(3448), np.int64(3462), np.int64(3478), np.int64(3539), np.int64(3603), np.int64(3743), np.int64(3745), np.int64(3773), np.int64(3787), np.int64(3853), np.int64(3883), np.int64(3905), np.int64(3921), np.int64(3932), np.int64(3974), np.int64(4062), np.int64(4067), np.int64(4111), np.int64(4164), np.int64(4203), np.int64(4217), np.int64(4261), np.int64(4280), np.int64(4312), np.int64(4319), np.int64(4329), np.int64(4341), np.int64(4347), np.int64(4351), np.int64(4410), np.int64(4416), np.int64(4437), np.int64(4490), np.int64(4537), np.int64(4578), np.int64(4592), np.int64(4632), np.int64(4654), np.int64(4688), np.int64(4691), np.int64(4726), np.int64(4800), np.int64(4828), np.int64(4833), np.int64(4835), np.int64(4838), np.int64(4839), np.int64(4878), np.int64(4920), np.int64(4932), np.int64(4953), np.int64(5000), np.int64(5064), np.int64(5107), np.int64(5119), np.int64(5149), np.int64(5157), np.int64(5170), np.int64(5177)]\n",
      "Predictions for Changed Indices: [8.78103765e-06 1.01535898e-05 1.02010308e-05 1.02348740e-05\n",
      " 1.02764698e-05 1.02900231e-05 1.03100911e-05 1.03491702e-05\n",
      " 1.03582643e-05 1.03987559e-05 1.04417650e-05 1.04541496e-05\n",
      " 1.04558949e-05 1.05248137e-05 1.05358195e-05 1.05471390e-05\n",
      " 1.05540321e-05 1.05966928e-05 1.06081388e-05 1.07125052e-05\n",
      " 1.07174101e-05 1.07423994e-05 1.07434753e-05 1.07552223e-05\n",
      " 1.07811529e-05 1.07816668e-05 1.08004378e-05 1.08031372e-05\n",
      " 1.08214290e-05 1.08348222e-05 1.08390705e-05 1.08625918e-05\n",
      " 1.09004995e-05 1.09036391e-05 1.09098082e-05 1.09186440e-05\n",
      " 1.09208731e-05 1.09411694e-05 1.09668363e-05 1.09774564e-05\n",
      " 1.09825251e-05 1.09831844e-05 1.09986140e-05 1.10055389e-05\n",
      " 1.10254341e-05 1.10346291e-05 1.10511837e-05 1.10602305e-05\n",
      " 1.10698429e-05 1.10729798e-05 1.10743949e-05 1.10789888e-05\n",
      " 1.11173131e-05 1.11251620e-05 1.11315094e-05 1.11955142e-05\n",
      " 1.12317130e-05 1.12327198e-05 1.12393955e-05 1.12557964e-05\n",
      " 1.12705966e-05 1.12853859e-05 1.12917696e-05 1.13075785e-05\n",
      " 1.13210654e-05 1.13525057e-05 1.13557426e-05 1.13602382e-05\n",
      " 1.13652450e-05 1.13875294e-05 1.13910701e-05 1.14167469e-05\n",
      " 1.14297318e-05 1.14502973e-05 1.14604136e-05 1.14654858e-05\n",
      " 1.14895665e-05 1.14895893e-05 1.14941040e-05 1.15090870e-05\n",
      " 1.15115572e-05 1.15184203e-05 1.15573939e-05 1.15611856e-05\n",
      " 1.15751309e-05 1.15927296e-05 1.15949069e-05 1.16318324e-05\n",
      " 1.16441179e-05 1.16621986e-05 1.16642568e-05 1.16666924e-05\n",
      " 1.16964147e-05 1.16970177e-05 1.16981792e-05 1.17264817e-05\n",
      " 1.17476829e-05 1.17487361e-05 1.17587333e-05 1.17728714e-05\n",
      " 1.17875434e-05 1.17876789e-05 1.17892305e-05 1.17964164e-05\n",
      " 1.18010175e-05 1.18244270e-05 1.18324142e-05 1.18336093e-05\n",
      " 1.18768439e-05 1.18919743e-05 1.18937096e-05 1.19028791e-05\n",
      " 1.19034348e-05 1.19243741e-05 1.19285260e-05 1.19459546e-05\n",
      " 1.19612305e-05 1.19944252e-05 1.20046570e-05 1.20049881e-05\n",
      " 1.20078394e-05 1.20204659e-05 1.20222539e-05 1.20263348e-05\n",
      " 1.20382001e-05 1.20385675e-05 1.20519844e-05 1.20539044e-05\n",
      " 1.20586747e-05 1.20675913e-05 1.20765599e-05 1.20806153e-05\n",
      " 1.20841178e-05 1.21036319e-05 1.21085623e-05 1.21165085e-05\n",
      " 1.21418407e-05 1.21630374e-05 1.21825515e-05 1.22042620e-05\n",
      " 1.22145912e-05 1.22317379e-05 1.22546699e-05 1.22613328e-05\n",
      " 1.23066920e-05 1.23170239e-05 1.23469899e-05 1.23476502e-05\n",
      " 1.23536456e-05 1.23680147e-05 1.23752707e-05 1.23762029e-05\n",
      " 1.24125145e-05]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [np.int64(698)]\n",
      "Total Number of Eagles the model believes are not eagles 8\n",
      "Total Number of images the model believes are not eagles 1253\n",
      "Change ratio: 0.0294\n",
      "Iteration 8...\n",
      "\u001b[1m  5/163\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 42ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 21:46:51.889847: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step\n",
      "Adjusted 145 indices, avg confidence: 0.0000\n",
      "Changed Indices: [np.int64(1177), np.int64(1327), np.int64(1341), np.int64(1363), np.int64(1373), np.int64(1391), np.int64(1397), np.int64(1425), np.int64(1448), np.int64(1450), np.int64(1551), np.int64(1576), np.int64(1636), np.int64(1642), np.int64(1673), np.int64(1706), np.int64(1722), np.int64(1796), np.int64(1802), np.int64(1876), np.int64(1906), np.int64(1911), np.int64(1932), np.int64(1958), np.int64(1993), np.int64(2010), np.int64(2022), np.int64(2076), np.int64(2078), np.int64(2107), np.int64(2165), np.int64(2167), np.int64(2176), np.int64(2188), np.int64(2194), np.int64(2216), np.int64(2224), np.int64(2278), np.int64(2282), np.int64(2303), np.int64(2443), np.int64(2448), np.int64(2462), np.int64(2473), np.int64(2475), np.int64(2501), np.int64(2502), np.int64(2520), np.int64(2527), np.int64(2545), np.int64(2585), np.int64(2612), np.int64(2614), np.int64(2641), np.int64(2659), np.int64(2671), np.int64(2694), np.int64(2706), np.int64(2717), np.int64(2740), np.int64(2782), np.int64(2791), np.int64(2792), np.int64(2880), np.int64(2933), np.int64(2944), np.int64(2992), np.int64(2994), np.int64(3002), np.int64(3043), np.int64(3046), np.int64(3105), np.int64(3115), np.int64(3142), np.int64(3171), np.int64(3176), np.int64(3177), np.int64(3186), np.int64(3211), np.int64(3230), np.int64(3255), np.int64(3279), np.int64(3289), np.int64(3295), np.int64(3339), np.int64(3342), np.int64(3400), np.int64(3410), np.int64(3459), np.int64(3507), np.int64(3595), np.int64(3744), np.int64(3790), np.int64(3814), np.int64(3880), np.int64(3922), np.int64(3956), np.int64(3973), np.int64(3991), np.int64(3998), np.int64(4006), np.int64(4021), np.int64(4030), np.int64(4049), np.int64(4107), np.int64(4147), np.int64(4159), np.int64(4208), np.int64(4214), np.int64(4236), np.int64(4252), np.int64(4366), np.int64(4421), np.int64(4440), np.int64(4441), np.int64(4444), np.int64(4452), np.int64(4456), np.int64(4482), np.int64(4497), np.int64(4502), np.int64(4510), np.int64(4520), np.int64(4575), np.int64(4597), np.int64(4624), np.int64(4661), np.int64(4692), np.int64(4714), np.int64(4722), np.int64(4729), np.int64(4773), np.int64(4781), np.int64(4820), np.int64(4913), np.int64(4957), np.int64(5027), np.int64(5042), np.int64(5048), np.int64(5056), np.int64(5061), np.int64(5063), np.int64(5095), np.int64(5115), np.int64(5197)]\n",
      "Predictions for Changed Indices: [7.0286819e-06 7.1067334e-06 7.1765153e-06 7.6293318e-06 7.6298129e-06\n",
      " 7.6800970e-06 7.6839360e-06 7.6923743e-06 7.7486629e-06 7.7961759e-06\n",
      " 7.7992772e-06 7.8256144e-06 7.8350231e-06 7.8647136e-06 7.9336724e-06\n",
      " 7.9850442e-06 7.9990159e-06 8.0050668e-06 8.0146910e-06 8.0499594e-06\n",
      " 8.0753171e-06 8.0825903e-06 8.1395374e-06 8.1906965e-06 8.2022416e-06\n",
      " 8.2714305e-06 8.2800088e-06 8.2991164e-06 8.3369550e-06 8.3520026e-06\n",
      " 8.3847881e-06 8.3874265e-06 8.4057156e-06 8.4119874e-06 8.4292842e-06\n",
      " 8.4404655e-06 8.4483418e-06 8.4602098e-06 8.4802750e-06 8.4834137e-06\n",
      " 8.5587108e-06 8.5947659e-06 8.5973725e-06 8.6331293e-06 8.6521850e-06\n",
      " 8.6612908e-06 8.6784967e-06 8.6898099e-06 8.7261251e-06 8.7288627e-06\n",
      " 8.7386416e-06 8.7391172e-06 8.7617227e-06 8.7678418e-06 8.7776798e-06\n",
      " 8.7904882e-06 8.8077750e-06 8.8246325e-06 8.8263077e-06 8.8263496e-06\n",
      " 8.8278312e-06 8.8289844e-06 8.8302904e-06 8.8310308e-06 8.8363558e-06\n",
      " 8.8551169e-06 8.8946927e-06 8.9153473e-06 8.9397308e-06 8.9411633e-06\n",
      " 8.9535351e-06 8.9688319e-06 8.9747537e-06 8.9832647e-06 8.9933965e-06\n",
      " 8.9971718e-06 9.0080830e-06 9.0147869e-06 9.0148906e-06 9.0218737e-06\n",
      " 9.0439089e-06 9.0708327e-06 9.0708754e-06 9.0745443e-06 9.0790718e-06\n",
      " 9.0855856e-06 9.0935164e-06 9.0936992e-06 9.0972899e-06 9.1331476e-06\n",
      " 9.1376523e-06 9.1700567e-06 9.1719285e-06 9.1793927e-06 9.1904467e-06\n",
      " 9.1967595e-06 9.2032078e-06 9.2265136e-06 9.2343662e-06 9.2527898e-06\n",
      " 9.2636938e-06 9.2664595e-06 9.2664686e-06 9.2702257e-06 9.2718165e-06\n",
      " 9.2957926e-06 9.3001818e-06 9.3172703e-06 9.3217495e-06 9.3311692e-06\n",
      " 9.3344797e-06 9.3443750e-06 9.3521048e-06 9.3597682e-06 9.3767258e-06\n",
      " 9.4242760e-06 9.4250754e-06 9.4441593e-06 9.4703328e-06 9.4746320e-06\n",
      " 9.4840516e-06 9.4854631e-06 9.4992047e-06 9.5036903e-06 9.5149626e-06\n",
      " 9.5151263e-06 9.5242867e-06 9.5261030e-06 9.5278747e-06 9.5420146e-06\n",
      " 9.5529685e-06 9.5586274e-06 9.5607693e-06 9.5772693e-06 9.5912346e-06\n",
      " 9.5913083e-06 9.6078975e-06 9.6360309e-06 9.6526956e-06 9.6554222e-06\n",
      " 9.6623298e-06 9.6633166e-06 9.6679896e-06 9.6834729e-06 9.6857548e-06]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [np.int64(1177)]\n",
      "Total Number of Eagles the model believes are not eagles 9\n",
      "Total Number of images the model believes are not eagles 1398\n",
      "Change ratio: 0.0279\n",
      "Iteration 9...\n",
      "\u001b[1m  5/163\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 43ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 21:47:24.775223: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step\n",
      "Adjusted 138 indices, avg confidence: 0.0000\n",
      "Changed Indices: [np.int64(298), np.int64(684), np.int64(1312), np.int64(1348), np.int64(1380), np.int64(1412), np.int64(1426), np.int64(1455), np.int64(1472), np.int64(1499), np.int64(1519), np.int64(1522), np.int64(1602), np.int64(1618), np.int64(1626), np.int64(1691), np.int64(1702), np.int64(1712), np.int64(1729), np.int64(1791), np.int64(1827), np.int64(1838), np.int64(1877), np.int64(1887), np.int64(1902), np.int64(1910), np.int64(1973), np.int64(1990), np.int64(2067), np.int64(2163), np.int64(2268), np.int64(2276), np.int64(2288), np.int64(2295), np.int64(2319), np.int64(2355), np.int64(2385), np.int64(2387), np.int64(2401), np.int64(2413), np.int64(2456), np.int64(2562), np.int64(2568), np.int64(2618), np.int64(2669), np.int64(2678), np.int64(2682), np.int64(2714), np.int64(2721), np.int64(2768), np.int64(2788), np.int64(2802), np.int64(2820), np.int64(2858), np.int64(2870), np.int64(2914), np.int64(2920), np.int64(2957), np.int64(2980), np.int64(3070), np.int64(3084), np.int64(3092), np.int64(3110), np.int64(3125), np.int64(3138), np.int64(3175), np.int64(3183), np.int64(3334), np.int64(3336), np.int64(3372), np.int64(3374), np.int64(3395), np.int64(3419), np.int64(3446), np.int64(3470), np.int64(3474), np.int64(3484), np.int64(3551), np.int64(3558), np.int64(3563), np.int64(3615), np.int64(3626), np.int64(3628), np.int64(3644), np.int64(3650), np.int64(3652), np.int64(3677), np.int64(3686), np.int64(3718), np.int64(3732), np.int64(3755), np.int64(3800), np.int64(3829), np.int64(3884), np.int64(3930), np.int64(3946), np.int64(3963), np.int64(4012), np.int64(4040), np.int64(4057), np.int64(4103), np.int64(4143), np.int64(4188), np.int64(4237), np.int64(4255), np.int64(4291), np.int64(4292), np.int64(4316), np.int64(4331), np.int64(4386), np.int64(4390), np.int64(4418), np.int64(4432), np.int64(4462), np.int64(4563), np.int64(4593), np.int64(4625), np.int64(4638), np.int64(4673), np.int64(4677), np.int64(4699), np.int64(4700), np.int64(4732), np.int64(4777), np.int64(4815), np.int64(4850), np.int64(4910), np.int64(4911), np.int64(4938), np.int64(4940), np.int64(4968), np.int64(4978), np.int64(5017), np.int64(5071), np.int64(5072), np.int64(5078), np.int64(5080), np.int64(5193)]\n",
      "Predictions for Changed Indices: [1.13968845e-05 1.15183320e-05 1.15353496e-05 1.15650673e-05\n",
      " 1.17540021e-05 1.17796881e-05 1.17957534e-05 1.18017952e-05\n",
      " 1.19118722e-05 1.19482802e-05 1.20036721e-05 1.21586772e-05\n",
      " 1.21757112e-05 1.21868397e-05 1.21917928e-05 1.22090933e-05\n",
      " 1.22416805e-05 1.22450674e-05 1.22800093e-05 1.22870961e-05\n",
      " 1.23091568e-05 1.23094378e-05 1.23215350e-05 1.23373493e-05\n",
      " 1.23492864e-05 1.23496166e-05 1.23506998e-05 1.23518657e-05\n",
      " 1.23862992e-05 1.23998316e-05 1.24103490e-05 1.24189437e-05\n",
      " 1.24295475e-05 1.24363414e-05 1.24428070e-05 1.24870521e-05\n",
      " 1.25090064e-05 1.25125980e-05 1.25145789e-05 1.25151882e-05\n",
      " 1.25262204e-05 1.25543247e-05 1.25650204e-05 1.25665429e-05\n",
      " 1.25900779e-05 1.25985334e-05 1.26570849e-05 1.26606583e-05\n",
      " 1.26664672e-05 1.26919795e-05 1.26955510e-05 1.27122712e-05\n",
      " 1.27393578e-05 1.27426874e-05 1.27489966e-05 1.27970989e-05\n",
      " 1.27972708e-05 1.27979047e-05 1.27998819e-05 1.28012134e-05\n",
      " 1.28116790e-05 1.28135980e-05 1.28198435e-05 1.28365791e-05\n",
      " 1.28402044e-05 1.28404972e-05 1.28468910e-05 1.28593201e-05\n",
      " 1.28607307e-05 1.28747815e-05 1.28780721e-05 1.28841166e-05\n",
      " 1.28862048e-05 1.28896590e-05 1.29069285e-05 1.29317059e-05\n",
      " 1.29433292e-05 1.29652199e-05 1.29666287e-05 1.29718483e-05\n",
      " 1.29812779e-05 1.30043254e-05 1.30137669e-05 1.30343587e-05\n",
      " 1.30430517e-05 1.30510898e-05 1.30534181e-05 1.30657099e-05\n",
      " 1.30974468e-05 1.31014194e-05 1.31082179e-05 1.31292109e-05\n",
      " 1.31448578e-05 1.31452980e-05 1.31694396e-05 1.31824563e-05\n",
      " 1.31889965e-05 1.31903680e-05 1.32108880e-05 1.32203013e-05\n",
      " 1.32260902e-05 1.32355663e-05 1.32578507e-05 1.32624027e-05\n",
      " 1.32883306e-05 1.32970272e-05 1.33042704e-05 1.33163157e-05\n",
      " 1.33484336e-05 1.33836393e-05 1.33848635e-05 1.33876983e-05\n",
      " 1.33926151e-05 1.34433531e-05 1.34475831e-05 1.34487373e-05\n",
      " 1.34646898e-05 1.34651900e-05 1.34855454e-05 1.34892380e-05\n",
      " 1.34950660e-05 1.35300661e-05 1.35336149e-05 1.35352784e-05\n",
      " 1.35471983e-05 1.35476903e-05 1.35606551e-05 1.35829814e-05\n",
      " 1.35901719e-05 1.36177541e-05 1.36313984e-05 1.36376129e-05\n",
      " 1.36525250e-05 1.36750423e-05 1.36772860e-05 1.36851922e-05\n",
      " 1.36953240e-05 1.37060115e-05]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [np.int64(684), np.int64(298)]\n",
      "Total Number of Eagles the model believes are not eagles 11\n",
      "Total Number of images the model believes are not eagles 1536\n",
      "Change ratio: 0.0265\n",
      "Iteration 10...\n",
      "\u001b[1m  5/163\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 42ms/step "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 21:47:57.458237: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step\n",
      "Adjusted 131 indices, avg confidence: 0.0000\n",
      "Changed Indices: [np.int64(36), np.int64(91), np.int64(918), np.int64(1300), np.int64(1377), np.int64(1408), np.int64(1437), np.int64(1559), np.int64(1586), np.int64(1616), np.int64(1617), np.int64(1693), np.int64(1704), np.int64(1727), np.int64(1782), np.int64(1799), np.int64(1836), np.int64(1844), np.int64(1890), np.int64(1937), np.int64(1949), np.int64(1952), np.int64(2056), np.int64(2116), np.int64(2124), np.int64(2136), np.int64(2170), np.int64(2172), np.int64(2179), np.int64(2206), np.int64(2227), np.int64(2264), np.int64(2287), np.int64(2300), np.int64(2312), np.int64(2351), np.int64(2361), np.int64(2378), np.int64(2407), np.int64(2424), np.int64(2434), np.int64(2504), np.int64(2510), np.int64(2571), np.int64(2603), np.int64(2633), np.int64(2662), np.int64(2674), np.int64(2676), np.int64(2677), np.int64(2699), np.int64(2708), np.int64(2711), np.int64(2719), np.int64(2727), np.int64(2738), np.int64(2751), np.int64(2752), np.int64(2764), np.int64(2765), np.int64(2855), np.int64(2865), np.int64(2882), np.int64(2956), np.int64(2962), np.int64(2978), np.int64(3056), np.int64(3113), np.int64(3144), np.int64(3209), np.int64(3216), np.int64(3375), np.int64(3401), np.int64(3402), np.int64(3404), np.int64(3513), np.int64(3523), np.int64(3562), np.int64(3594), np.int64(3599), np.int64(3697), np.int64(3711), np.int64(3735), np.int64(3779), np.int64(3793), np.int64(3825), np.int64(3838), np.int64(3890), np.int64(3939), np.int64(3944), np.int64(4022), np.int64(4075), np.int64(4101), np.int64(4235), np.int64(4263), np.int64(4268), np.int64(4357), np.int64(4388), np.int64(4425), np.int64(4427), np.int64(4449), np.int64(4504), np.int64(4519), np.int64(4538), np.int64(4557), np.int64(4566), np.int64(4582), np.int64(4615), np.int64(4621), np.int64(4711), np.int64(4713), np.int64(4719), np.int64(4768), np.int64(4795), np.int64(4832), np.int64(4859), np.int64(4868), np.int64(4871), np.int64(4882), np.int64(4893), np.int64(4925), np.int64(4997), np.int64(5033), np.int64(5053), np.int64(5057), np.int64(5075), np.int64(5085), np.int64(5096), np.int64(5104), np.int64(5118), np.int64(5125)]\n",
      "Predictions for Changed Indices: [9.90501940e-06 1.11469117e-05 1.24427370e-05 1.29364171e-05\n",
      " 1.30190801e-05 1.33531057e-05 1.34752618e-05 1.36843837e-05\n",
      " 1.39387985e-05 1.41674427e-05 1.41906321e-05 1.41915798e-05\n",
      " 1.42268937e-05 1.42596155e-05 1.42608260e-05 1.43096240e-05\n",
      " 1.44090691e-05 1.44584856e-05 1.45116410e-05 1.46015964e-05\n",
      " 1.46166421e-05 1.48119998e-05 1.48211002e-05 1.48961017e-05\n",
      " 1.50337064e-05 1.51649283e-05 1.52169941e-05 1.52677549e-05\n",
      " 1.52790435e-05 1.52917091e-05 1.53251567e-05 1.53735946e-05\n",
      " 1.53763522e-05 1.54125391e-05 1.55189246e-05 1.56381921e-05\n",
      " 1.57048780e-05 1.57079048e-05 1.57913219e-05 1.58212897e-05\n",
      " 1.58214407e-05 1.58417934e-05 1.58448765e-05 1.58947732e-05\n",
      " 1.59012779e-05 1.59091342e-05 1.59216434e-05 1.59235551e-05\n",
      " 1.59285664e-05 1.59485699e-05 1.59572410e-05 1.59756964e-05\n",
      " 1.60003055e-05 1.60298132e-05 1.60325035e-05 1.60655018e-05\n",
      " 1.60892814e-05 1.61009011e-05 1.61262724e-05 1.61263488e-05\n",
      " 1.61876105e-05 1.62361739e-05 1.62427405e-05 1.62526721e-05\n",
      " 1.62535871e-05 1.62802226e-05 1.62888427e-05 1.62978085e-05\n",
      " 1.63038239e-05 1.63090008e-05 1.63126733e-05 1.63128898e-05\n",
      " 1.63206714e-05 1.63467466e-05 1.63878904e-05 1.64133853e-05\n",
      " 1.64362555e-05 1.64636749e-05 1.64756930e-05 1.64764460e-05\n",
      " 1.65022193e-05 1.65194451e-05 1.65440870e-05 1.65449073e-05\n",
      " 1.65631409e-05 1.66169193e-05 1.66239715e-05 1.66529935e-05\n",
      " 1.66626978e-05 1.66747486e-05 1.67263825e-05 1.67390044e-05\n",
      " 1.67483613e-05 1.67537128e-05 1.67613198e-05 1.67634626e-05\n",
      " 1.67781764e-05 1.68105926e-05 1.68312054e-05 1.68316092e-05\n",
      " 1.68423503e-05 1.68499810e-05 1.68621664e-05 1.68878178e-05\n",
      " 1.69042687e-05 1.69270643e-05 1.69273535e-05 1.69297437e-05\n",
      " 1.69331670e-05 1.69339892e-05 1.69590076e-05 1.69703326e-05\n",
      " 1.69872856e-05 1.69908180e-05 1.69937830e-05 1.70004132e-05\n",
      " 1.70169587e-05 1.70174444e-05 1.70242765e-05 1.70507446e-05\n",
      " 1.70598869e-05 1.70785406e-05 1.71281918e-05 1.71350839e-05\n",
      " 1.71656211e-05 1.71820830e-05 1.71909651e-05 1.72017244e-05\n",
      " 1.72261680e-05 1.72375221e-05 1.72441323e-05]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [np.int64(918), np.int64(91), np.int64(36)]\n",
      "Total Number of Eagles the model believes are not eagles 14\n",
      "Total Number of images the model believes are not eagles 1667\n",
      "Change ratio: 0.0252\n"
     ]
    }
   ],
   "source": [
    "n_percentage = 5  # % of least confident eagle predictions to adjust\n",
    "max_iterations = 10 # has biggest numbers from above output  \n",
    "convergence_tolerance = 0  # stop if change ratio is this\n",
    "high_conf_threshold = 0.8  # threshold for confident eagle flips\n",
    "confidence_init = confidence_init = np.array([.35 if x<.5 else 1 for x in label_train]).reshape(5200,1)\n",
    "\n",
    "num_incorrectly_modified = 0\n",
    "num_modified = 0\n",
    "for iteration in range(max_iterations):\n",
    "    print(f\"Iteration {iteration + 1}...\")\n",
    "\n",
    "    # step 1: train model\n",
    "    history = model.fit(\n",
    "        [images_train, confidence_init],\n",
    "        label_train,\n",
    "        batch_size=100, # ok this might seem crazy but im wondering if w batch=32 it wasn't encountering enough wrong labels \n",
    "        epochs=1,\n",
    "        # validation_data=([images_val, np.ones((len(images_val), 1))], label_val), # this was a line from chat, replaced w ours instead below\n",
    "        validation_data=([images_val, label_val.reshape(-1,1)], label_val),\n",
    "        callbacks=callbacks,\n",
    "        shuffle = True,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # step 2: predict probabilities\n",
    "    preds = model.predict([images_train, confidence_init]).flatten()\n",
    "\n",
    "    # step 3: identify least confident eagle predictions\n",
    "    low_confidence_indices = np.where((label_train == 0) & (preds < 0.5) & (preds != 0))[0] # grabbing indices where label_train is 0 (noneagle), focusing in on the misclassified\n",
    "    filtered_indices = low_confidence_indices[confidence_init.flatten()[low_confidence_indices]!= 0]\n",
    "    sorted_indices = filtered_indices[np.argsort(preds[filtered_indices])] # sorts the preds low to high\n",
    "    to_adjust = sorted_indices[:int(len(sorted_indices) * (n_percentage / 100))] # only grabbing 5% rn of the bottom\n",
    "    \n",
    "\n",
    "    # step 4: update confidence for least confident predictions\n",
    "    if len(to_adjust) > 0:\n",
    "        confidence_init[to_adjust] = 0  # reduce confidence to 0 for the indices we picked by %\n",
    "        avg_confidence = np.mean(preds[to_adjust])\n",
    "        print(f\"Adjusted {len(to_adjust)} indices, avg confidence: {avg_confidence:.4f}\")\n",
    "    else:\n",
    "        print(\"No indices to adjust in this iteration.\")\n",
    "        \n",
    "    print(f\"Changed Indices: {sorted(to_adjust)}\")\n",
    "    print(f\"Predictions for Changed Indices: {preds[to_adjust]}\")\n",
    "    wrongly_switched = [x for x in changed_indices if x in to_adjust]\n",
    "    print(\"FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES:\", wrongly_switched)\n",
    "    num_incorrectly_modified += len(wrongly_switched)\n",
    "    num_modified += len(to_adjust)\n",
    "    print(\"Total Number of Eagles the model believes are not eagles\", num_incorrectly_modified)\n",
    "    print(\"Total Number of images the model believes are not eagles\", num_modified)\n",
    "\n",
    "\n",
    "    # step 5: check for convergence\n",
    "    if len(to_adjust) > 0:\n",
    "            change_ratio = len(to_adjust) / len(label_train)  \n",
    "    else:\n",
    "            change_ratio=0\n",
    "    print(f\"Change ratio: {change_ratio:.4f}\")\n",
    "\n",
    "    if change_ratio < convergence_tolerance:\n",
    "        print(\"Convergence reached.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "deb04844-8b14-4c62-836d-b47fadb743f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many actual eagles in top 10% is: 101\n",
      "How many total are in the top 10% is: 416\n"
     ]
    }
   ],
   "source": [
    "def top_n_percent_indices(predictions, n_percent):\n",
    "\n",
    "    target_indices = np.where(label_train == 0)[0]\n",
    "    filtered_preds = predictions[target_indices]\n",
    "    \n",
    "    # calculate the number of top elements to select\n",
    "    num_top_elements = int(np.ceil(len(filtered_preds) * n_percent / 100))\n",
    "    \n",
    "    # get the indices of the sorted values (descending order)\n",
    "    sorted_indices = np.argsort(filtered_preds)[::-1]\n",
    "    \n",
    "    # select the top n_percent indices\n",
    "    top_indices = sorted_indices[:num_top_elements]\n",
    "\n",
    "    top_original_indices = target_indices[top_indices]\n",
    "    \n",
    "    return top_original_indices\n",
    "n_percent = 10\n",
    "high_confidence_indices = top_n_percent_indices(preds,n_percent)\n",
    "high_confidence_indices\n",
    "actually_eagles = [x for x in high_confidence_indices if x in changed_indices]\n",
    "# print(\"The least confident not-eagles that are actually eagles:\", actually_eagles)\n",
    "print(f\"How many actual eagles in top {n_percent}% is: {len(actually_eagles)}\")\n",
    "print(f\"How many total are in the top {n_percent}% is: {len(high_confidence_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b058189-0d36-42ac-b896-628b8cd73b2d",
   "metadata": {},
   "source": [
    "The best we can seem to do here is roughly approach a ratio of ~25% being in a top n% - If I have to guess why this is, it would be that as you do more runs or more epochs or take a larger/smaller percent of the top or bottom, the total number of eagles that get the \"0\" confidence stays. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81785304-cb24-4373-9777-0bcdec02e170",
   "metadata": {},
   "source": [
    "To try and achieve a higher ratio, we ended up testing 48 different combinations. The results can be seen in the data frame below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e048db37-9de7-4bdd-8ff5-3711d0b61411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to <_io.TextIOWrapper name='eagles_analysis.csv' mode='w' encoding='UTF-8'>\n",
      "    n_percentage  iterations  n_percent  \\\n",
      "0              5          10          5   \n",
      "1              5          10         10   \n",
      "2              5          10         15   \n",
      "3              5          10         20   \n",
      "4              5          15          5   \n",
      "5              5          15         10   \n",
      "6              5          15         15   \n",
      "7              5          15         20   \n",
      "8             10          10          5   \n",
      "9             10          10         10   \n",
      "10            10          10         15   \n",
      "11            10          10         20   \n",
      "12            10          15          5   \n",
      "13            10          15         10   \n",
      "14            10          15         15   \n",
      "15            10          15         20   \n",
      "16            15          10          5   \n",
      "17            15          10         10   \n",
      "18            15          10         15   \n",
      "19            15          10         20   \n",
      "20            15          15          5   \n",
      "21            15          15         10   \n",
      "22            15          15         15   \n",
      "23            15          15         20   \n",
      "24            20          10          5   \n",
      "25            20          10         10   \n",
      "26            20          10         15   \n",
      "27            20          10         20   \n",
      "28            20          15          5   \n",
      "29            20          15         10   \n",
      "30            20          15         15   \n",
      "31            20          15         20   \n",
      "32             5           5          5   \n",
      "33             5           5         10   \n",
      "34             5           5         15   \n",
      "35             5           5         20   \n",
      "36            10           5          5   \n",
      "37            10           5         10   \n",
      "38            10           5         15   \n",
      "39            10           5         20   \n",
      "40            15           5          5   \n",
      "41            15           5         10   \n",
      "42            15           5         15   \n",
      "43            15           5         20   \n",
      "44            20           5          5   \n",
      "45            20           5         10   \n",
      "46            20           5         15   \n",
      "47            20           5         20   \n",
      "\n",
      "    how many actual eagles in top {n_percent}  \\\n",
      "0                                          60   \n",
      "1                                          97   \n",
      "2                                         124   \n",
      "3                                         152   \n",
      "4                                          59   \n",
      "5                                          97   \n",
      "6                                         114   \n",
      "7                                         139   \n",
      "8                                          54   \n",
      "9                                          88   \n",
      "10                                        118   \n",
      "11                                        137   \n",
      "12                                         32   \n",
      "13                                         65   \n",
      "14                                         95   \n",
      "15                                        119   \n",
      "16                                         51   \n",
      "17                                         94   \n",
      "18                                        115   \n",
      "19                                        125   \n",
      "20                                         33   \n",
      "21                                         87   \n",
      "22                                         93   \n",
      "23                                        100   \n",
      "24                                         41   \n",
      "25                                         77   \n",
      "26                                         99   \n",
      "27                                        111   \n",
      "28                                         46   \n",
      "29                                         54   \n",
      "30                                         66   \n",
      "31                                         73   \n",
      "32                                         55   \n",
      "33                                         91   \n",
      "34                                        121   \n",
      "35                                        146   \n",
      "36                                         52   \n",
      "37                                         81   \n",
      "38                                        111   \n",
      "39                                        131   \n",
      "40                                         46   \n",
      "41                                         83   \n",
      "42                                        104   \n",
      "43                                        127   \n",
      "44                                         40   \n",
      "45                                         70   \n",
      "46                                         91   \n",
      "47                                        109   \n",
      "\n",
      "    how many total are in the top {n_percent}   ratio  \n",
      "0                                         208  28.85%  \n",
      "1                                         416  23.32%  \n",
      "2                                         624  19.87%  \n",
      "3                                         832  18.27%  \n",
      "4                                         208  28.37%  \n",
      "5                                         416  23.32%  \n",
      "6                                         624  18.27%  \n",
      "7                                         832  16.71%  \n",
      "8                                         208  25.96%  \n",
      "9                                         416  21.15%  \n",
      "10                                        624  18.91%  \n",
      "11                                        832  16.47%  \n",
      "12                                        208  15.38%  \n",
      "13                                        416  15.63%  \n",
      "14                                        624  15.22%  \n",
      "15                                        832   14.3%  \n",
      "16                                        208  24.52%  \n",
      "17                                        416   22.6%  \n",
      "18                                        624  18.43%  \n",
      "19                                        832  15.02%  \n",
      "20                                        208  15.87%  \n",
      "21                                        416  20.91%  \n",
      "22                                        624   14.9%  \n",
      "23                                        832  12.02%  \n",
      "24                                        208  19.71%  \n",
      "25                                        416  18.51%  \n",
      "26                                        624  15.87%  \n",
      "27                                        832  13.34%  \n",
      "28                                        208  22.12%  \n",
      "29                                        416  12.98%  \n",
      "30                                        624  10.58%  \n",
      "31                                        832   8.77%  \n",
      "32                                        208  26.44%  \n",
      "33                                        416  21.88%  \n",
      "34                                        624  19.39%  \n",
      "35                                        832  17.55%  \n",
      "36                                        208     25%  \n",
      "37                                        416  19.47%  \n",
      "38                                        624  17.79%  \n",
      "39                                        832  15.75%  \n",
      "40                                        208  22.12%  \n",
      "41                                        416  19.95%  \n",
      "42                                        624  16.67%  \n",
      "43                                        832  15.26%  \n",
      "44                                        208  19.23%  \n",
      "45                                        416  16.83%  \n",
      "46                                        624  14.58%  \n",
      "47                                        832   13.1%  \n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# taken from the excel\n",
    "header = [\n",
    "    \"n_percentage\", \"iterations\", \"n_percent\",\n",
    "    \"how many actual eagles in top {n_percent}\",\n",
    "    \"how many total are in the top {n_percent}\", \"ratio\"\n",
    "]\n",
    "rows = [\n",
    "    [5, 10, 5, 60, 208, \"28.85%\"],\n",
    "    [5, 10, 10, 97, 416, \"23.32%\"],\n",
    "    [5, 10, 15, 124, 624, \"19.87%\"],\n",
    "    [5, 10, 20, 152, 832, \"18.27%\"],\n",
    "    [5, 15, 5, 59, 208, \"28.37%\"],\n",
    "    [5, 15, 10, 97, 416, \"23.32%\"],\n",
    "    [5, 15, 15, 114, 624, \"18.27%\"],\n",
    "    [5, 15, 20, 139, 832, \"16.71%\"],\n",
    "    [10, 10, 5, 54, 208, \"25.96%\"],\n",
    "    [10, 10, 10, 88, 416, \"21.15%\"],\n",
    "    [10, 10, 15, 118, 624, \"18.91%\"],\n",
    "    [10, 10, 20, 137, 832, \"16.47%\"],\n",
    "    [10, 15, 5, 32, 208, \"15.38%\"],\n",
    "    [10, 15, 10, 65, 416, \"15.63%\"],\n",
    "    [10, 15, 15, 95, 624, \"15.22%\"],\n",
    "    [10, 15, 20, 119, 832, \"14.3%\"],\n",
    "    [15, 10, 5, 51, 208, \"24.52%\"],\n",
    "    [15, 10, 10, 94, 416, \"22.6%\"],\n",
    "    [15, 10, 15, 115, 624, \"18.43%\"],\n",
    "    [15, 10, 20, 125, 832, \"15.02%\"],\n",
    "    [15, 15, 5, 33, 208, \"15.87%\"],\n",
    "    [15, 15, 10, 87, 416, \"20.91%\"],\n",
    "    [15, 15, 15, 93, 624, \"14.9%\"],\n",
    "    [15, 15, 20, 100, 832, \"12.02%\"],\n",
    "    [20, 10, 5, 41, 208, \"19.71%\"],\n",
    "    [20, 10, 10, 77, 416, \"18.51%\"],\n",
    "    [20, 10, 15, 99, 624, \"15.87%\"],\n",
    "    [20, 10, 20, 111, 832, \"13.34%\"],\n",
    "    [20, 15, 5, 46, 208, \"22.12%\"],\n",
    "    [20, 15, 10, 54, 416, \"12.98%\"],\n",
    "    [20, 15, 15, 66, 624, \"10.58%\"],\n",
    "    [20, 15, 20, 73, 832, \"8.77%\"],\n",
    "    [5, 5, 5, 55, 208, \"26.44%\"],\n",
    "    [5, 5, 10, 91, 416, \"21.88%\"],\n",
    "    [5, 5, 15, 121, 624, \"19.39%\"],\n",
    "    [5, 5, 20, 146, 832, \"17.55%\"],\n",
    "    [10, 5, 5, 52, 208, \"25%\"],\n",
    "    [10, 5, 10, 81, 416, \"19.47%\"],\n",
    "    [10, 5, 15, 111, 624, \"17.79%\"],\n",
    "    [10, 5, 20, 131, 832, \"15.75%\"],\n",
    "    [15, 5, 5, 46, 208, \"22.12%\"],\n",
    "    [15, 5, 10, 83, 416, \"19.95%\"],\n",
    "    [15, 5, 15, 104, 624, \"16.67%\"],\n",
    "    [15, 5, 20, 127, 832, \"15.26%\"],\n",
    "    [20, 5, 5, 40, 208, \"19.23%\"],\n",
    "    [20, 5, 10, 70, 416, \"16.83%\"],\n",
    "    [20, 5, 15, 91, 624, \"14.58%\"],\n",
    "    [20, 5, 20, 109, 832, \"13.1%\"],\n",
    "]\n",
    "\n",
    "# write data to a CSV file\n",
    "filename = \"eagles_analysis.csv\"\n",
    "with open(filename, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(rows)\n",
    "print(f\"Data successfully written to {file}\")\n",
    "\n",
    "# df\n",
    "best_params_df = pd.read_csv('eagles_analysis.csv') \n",
    "print(best_params_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adacf22-c6e5-412b-bff9-05cc226e5b90",
   "metadata": {},
   "source": [
    "# Best parameters result \n",
    "### n_percentage = 5, max_iterations = 10, n_percent = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "540183b1-651e-4892-8469-e8f8f8283d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1...\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step\n",
      "Adjusted 208 indices, avg confidence: 0.1062\n",
      "Changed Indices: [15, 91, 306, 329, 533, 564, 578, 666, 684, 803, 837, 1089, 1109, 1148, 1217, 1300, 1302, 1321, 1330, 1334, 1349, 1355, 1430, 1454, 1463, 1528, 1559, 1562, 1636, 1665, 1666, 1689, 1800, 1810, 1812, 1815, 1821, 1830, 1842, 1870, 1880, 1882, 1922, 1932, 1937, 1951, 1999, 2017, 2037, 2047, 2075, 2095, 2108, 2122, 2126, 2145, 2170, 2171, 2209, 2224, 2287, 2325, 2338, 2340, 2352, 2367, 2383, 2395, 2410, 2417, 2433, 2443, 2447, 2459, 2475, 2479, 2498, 2499, 2516, 2528, 2556, 2570, 2611, 2617, 2637, 2661, 2715, 2740, 2741, 2743, 2744, 2760, 2768, 2777, 2778, 2798, 2805, 2811, 2825, 2829, 2855, 2891, 2969, 3026, 3034, 3104, 3113, 3130, 3145, 3154, 3202, 3231, 3315, 3338, 3344, 3356, 3362, 3377, 3387, 3395, 3400, 3433, 3461, 3477, 3497, 3538, 3552, 3563, 3582, 3634, 3677, 3700, 3701, 3714, 3728, 3755, 3757, 3761, 3775, 3797, 3799, 3812, 3841, 3875, 3885, 3890, 3936, 3956, 3969, 4002, 4005, 4021, 4031, 4050, 4075, 4099, 4101, 4114, 4118, 4129, 4144, 4184, 4185, 4192, 4204, 4230, 4267, 4274, 4284, 4363, 4372, 4394, 4414, 4456, 4482, 4488, 4500, 4510, 4551, 4624, 4628, 4637, 4650, 4693, 4708, 4728, 4750, 4751, 4780, 4783, 4787, 4813, 4818, 4819, 4828, 4858, 4912, 4923, 4934, 4982, 4993, 5012, 5040, 5062, 5086, 5102, 5104, 5185]\n",
      "Predictions for Changed Indices: [0.01694514 0.02581174 0.03015035 0.03300071 0.03388599 0.04537833\n",
      " 0.04701019 0.04803725 0.04805419 0.05066967 0.05273295 0.05282561\n",
      " 0.05326954 0.06165349 0.06372709 0.06669655 0.06800205 0.06875116\n",
      " 0.07024726 0.07264149 0.07274039 0.07327169 0.07488477 0.07693076\n",
      " 0.07711196 0.07865817 0.08046716 0.08098479 0.08251212 0.08262155\n",
      " 0.08265212 0.08408795 0.0855613  0.0862864  0.08796231 0.08808159\n",
      " 0.08823882 0.08826631 0.08910097 0.09004895 0.09093753 0.09130768\n",
      " 0.09136763 0.0917757  0.09225572 0.09245602 0.09299512 0.09305998\n",
      " 0.09317248 0.09340738 0.09406317 0.09428128 0.094509   0.09458201\n",
      " 0.09469765 0.09607753 0.09632154 0.09727502 0.09747934 0.09779434\n",
      " 0.0983194  0.09907688 0.09978459 0.10012276 0.10016242 0.10022436\n",
      " 0.10041648 0.10050136 0.10064126 0.10070224 0.10170723 0.10184214\n",
      " 0.1023476  0.10263528 0.10334724 0.10338446 0.10363513 0.10408244\n",
      " 0.10427424 0.10440146 0.10448683 0.10489774 0.10503545 0.10509702\n",
      " 0.10522483 0.10618834 0.10626163 0.1077011  0.10812458 0.10812898\n",
      " 0.10868193 0.10911397 0.10922356 0.10930751 0.10969567 0.11059725\n",
      " 0.11103959 0.11107259 0.11121429 0.11156849 0.11173557 0.11176103\n",
      " 0.11212541 0.11264417 0.11265066 0.11273711 0.11295527 0.11301992\n",
      " 0.11314373 0.11335908 0.11352994 0.11461386 0.11563197 0.1163648\n",
      " 0.11646467 0.11665092 0.11670659 0.11688075 0.11729527 0.11789494\n",
      " 0.11794881 0.11807671 0.1182407  0.11837427 0.11839064 0.11840547\n",
      " 0.118543   0.11892862 0.11923861 0.11939144 0.11942102 0.1194596\n",
      " 0.11948208 0.11976007 0.11997168 0.12032025 0.12056229 0.12114426\n",
      " 0.12147068 0.12167819 0.12185467 0.12193356 0.12201471 0.12213869\n",
      " 0.12294275 0.12296835 0.12300605 0.12308559 0.12327173 0.12338836\n",
      " 0.12353009 0.12412424 0.12423677 0.12427217 0.12452724 0.12473174\n",
      " 0.12481193 0.12486929 0.12507    0.12511498 0.1252261  0.12531127\n",
      " 0.12555087 0.12565325 0.12574627 0.1262091  0.1264639  0.12685367\n",
      " 0.12748718 0.12783411 0.12813988 0.12826984 0.12832548 0.12847146\n",
      " 0.12849161 0.12863348 0.1286697  0.1287985  0.12895367 0.12902267\n",
      " 0.12902947 0.12905891 0.12924287 0.12933712 0.12935239 0.12952623\n",
      " 0.12959343 0.12969176 0.12973943 0.1299245  0.13004014 0.13032712\n",
      " 0.13037135 0.13088238 0.13098834 0.13110451 0.13123034 0.13154379\n",
      " 0.13160264 0.13161145 0.13181224 0.1318788  0.13204049 0.1321625\n",
      " 0.13267878 0.13273467 0.1329479  0.13330159]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [564, 578, 837, 666, 15, 803, 533, 684, 91, 1217, 1109, 329, 1089, 306, 1148]\n",
      "Total Number of Eagles the model believes are not eagles 15\n",
      "Total Number of images the model believes are not eagles 208\n",
      "Change ratio: 0.0400\n",
      "Iteration 2...\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step\n",
      "Adjusted 197 indices, avg confidence: 0.0174\n",
      "Changed Indices: [731, 1314, 1332, 1371, 1372, 1388, 1400, 1401, 1414, 1418, 1432, 1436, 1442, 1462, 1487, 1545, 1549, 1587, 1622, 1623, 1639, 1642, 1646, 1658, 1676, 1682, 1712, 1715, 1716, 1718, 1722, 1739, 1768, 1795, 1823, 1862, 1873, 1876, 1890, 1899, 1971, 1977, 1986, 2008, 2025, 2128, 2165, 2173, 2246, 2269, 2272, 2286, 2393, 2403, 2416, 2431, 2435, 2446, 2478, 2482, 2506, 2512, 2520, 2540, 2547, 2553, 2602, 2619, 2657, 2670, 2696, 2703, 2704, 2713, 2720, 2742, 2754, 2755, 2761, 2807, 2844, 2857, 2859, 2908, 2940, 2945, 2960, 2964, 2972, 2999, 3023, 3025, 3066, 3132, 3133, 3137, 3153, 3184, 3189, 3200, 3215, 3233, 3235, 3316, 3322, 3339, 3341, 3343, 3360, 3373, 3385, 3389, 3403, 3411, 3418, 3422, 3489, 3492, 3495, 3507, 3508, 3522, 3565, 3573, 3623, 3649, 3706, 3796, 3806, 3813, 3851, 3879, 3891, 3981, 3987, 4011, 4032, 4055, 4089, 4093, 4175, 4202, 4236, 4240, 4243, 4260, 4272, 4288, 4289, 4367, 4404, 4409, 4433, 4442, 4514, 4537, 4550, 4567, 4582, 4586, 4595, 4601, 4636, 4651, 4654, 4687, 4724, 4734, 4748, 4774, 4786, 4791, 4794, 4812, 4825, 4827, 4833, 4840, 4877, 4878, 4889, 4917, 4949, 4958, 4996, 5004, 5021, 5026, 5033, 5047, 5071, 5097, 5109, 5123, 5141, 5156, 5180]\n",
      "Predictions for Changed Indices: [0.00595493 0.00653023 0.00718314 0.00758456 0.00763679 0.00783446\n",
      " 0.00801572 0.0083593  0.00863269 0.0086824  0.00886441 0.00895267\n",
      " 0.00912047 0.00945803 0.00969048 0.00970523 0.00982813 0.01015597\n",
      " 0.01039908 0.01049065 0.0106128  0.01096114 0.0110389  0.0112616\n",
      " 0.01126736 0.0112747  0.0114218  0.01158858 0.01164809 0.012037\n",
      " 0.01204094 0.0123223  0.0123823  0.01241581 0.01243774 0.01269963\n",
      " 0.01291748 0.01332608 0.01340727 0.01342913 0.01344577 0.01348325\n",
      " 0.01350452 0.0135159  0.01372252 0.01373063 0.01384982 0.013974\n",
      " 0.01430328 0.01432388 0.01448458 0.01469713 0.0149074  0.014913\n",
      " 0.01492183 0.0149219  0.01492637 0.01502825 0.01510083 0.01513604\n",
      " 0.01517617 0.0152837  0.01532033 0.01535519 0.0154016  0.01548298\n",
      " 0.01550053 0.01563836 0.01569405 0.0157703  0.01615146 0.01619217\n",
      " 0.01638807 0.01640024 0.01645587 0.01651207 0.01663344 0.01664579\n",
      " 0.01668058 0.01668392 0.01675008 0.01675943 0.01685216 0.0169363\n",
      " 0.01696978 0.01707196 0.01732874 0.017363   0.01746822 0.01761185\n",
      " 0.01770551 0.01772477 0.01774156 0.01782487 0.01782838 0.0179214\n",
      " 0.01793968 0.01797663 0.01810182 0.01818921 0.01828764 0.01832766\n",
      " 0.0184453  0.0184945  0.01853048 0.01858257 0.01872178 0.01890378\n",
      " 0.01898085 0.01901563 0.01904251 0.01905647 0.01908609 0.01920925\n",
      " 0.01923451 0.01923838 0.01931731 0.01940234 0.01949091 0.01956882\n",
      " 0.01961538 0.01970877 0.01982515 0.01982942 0.01994844 0.02002971\n",
      " 0.02009083 0.02013652 0.02019426 0.0202483  0.02026032 0.02028989\n",
      " 0.02034784 0.0203839  0.02041162 0.02048211 0.02049265 0.02051478\n",
      " 0.0205388  0.02056623 0.02061792 0.02067632 0.02081329 0.02081632\n",
      " 0.02084809 0.02087205 0.02089842 0.02115558 0.02115611 0.02117551\n",
      " 0.02121217 0.02124386 0.02124696 0.02134307 0.02146673 0.0214904\n",
      " 0.02154644 0.02164698 0.02167758 0.02184498 0.02186869 0.0219591\n",
      " 0.02201362 0.0220662  0.02216541 0.02219674 0.02232044 0.02239377\n",
      " 0.02241095 0.02246763 0.02251263 0.02257086 0.02260757 0.02261622\n",
      " 0.02263243 0.02270422 0.02275578 0.02281655 0.02284192 0.02291723\n",
      " 0.02292226 0.02298039 0.02317281 0.02320013 0.02325038 0.02330944\n",
      " 0.02331083 0.02333068 0.02334154 0.02334674 0.02348013 0.023483\n",
      " 0.02357831 0.02361921 0.02363139 0.02365906 0.0238612 ]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [731]\n",
      "Total Number of Eagles the model believes are not eagles 16\n",
      "Total Number of images the model believes are not eagles 405\n",
      "Change ratio: 0.0379\n",
      "Iteration 3...\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step\n",
      "Adjusted 187 indices, avg confidence: 0.0101\n",
      "Changed Indices: [1309, 1374, 1403, 1423, 1441, 1451, 1476, 1484, 1501, 1502, 1525, 1548, 1570, 1580, 1596, 1602, 1609, 1611, 1626, 1685, 1695, 1696, 1744, 1755, 1769, 1796, 1826, 1833, 1835, 1840, 1863, 1897, 1927, 1938, 1943, 1960, 1974, 1994, 1997, 2016, 2035, 2045, 2079, 2100, 2115, 2168, 2180, 2185, 2194, 2225, 2229, 2239, 2303, 2307, 2315, 2337, 2347, 2348, 2353, 2364, 2386, 2404, 2442, 2448, 2464, 2493, 2503, 2538, 2558, 2575, 2579, 2608, 2631, 2809, 2876, 2893, 2896, 2901, 2913, 2915, 2959, 3020, 3059, 3099, 3157, 3175, 3232, 3262, 3303, 3330, 3348, 3364, 3376, 3380, 3416, 3434, 3455, 3478, 3502, 3539, 3547, 3550, 3567, 3581, 3618, 3638, 3682, 3696, 3704, 3737, 3744, 3767, 3776, 3789, 3801, 3803, 3805, 3837, 3860, 3887, 3908, 3923, 3925, 3937, 3942, 3957, 3958, 3986, 3991, 4025, 4027, 4061, 4065, 4083, 4143, 4207, 4219, 4222, 4242, 4251, 4265, 4300, 4310, 4316, 4328, 4337, 4342, 4382, 4387, 4402, 4424, 4430, 4492, 4524, 4568, 4575, 4578, 4597, 4656, 4726, 4745, 4785, 4793, 4829, 4835, 4844, 4856, 4857, 4863, 4920, 4928, 4950, 4957, 4959, 4980, 4997, 5005, 5008, 5010, 5018, 5043, 5061, 5064, 5072, 5078, 5138, 5144]\n",
      "Predictions for Changed Indices: [0.0051722  0.00619251 0.00627469 0.00632313 0.00633271 0.00640819\n",
      " 0.00646329 0.00646663 0.00653443 0.00693934 0.00700953 0.00709956\n",
      " 0.00728293 0.00741591 0.00743471 0.00744446 0.00754907 0.00757063\n",
      " 0.00767724 0.0077411  0.00775574 0.00780719 0.0078305  0.00784028\n",
      " 0.007844   0.00788923 0.00794489 0.00799517 0.00810827 0.00820758\n",
      " 0.00829228 0.00837808 0.0084452  0.00846589 0.00847901 0.008533\n",
      " 0.00854873 0.00858869 0.00861689 0.00871524 0.00873152 0.00874799\n",
      " 0.00882121 0.00882711 0.00892391 0.00904584 0.00906103 0.00913236\n",
      " 0.00914264 0.00916988 0.00917368 0.00918583 0.00925371 0.00926516\n",
      " 0.00927885 0.0095327  0.00953763 0.00955208 0.00964473 0.00965539\n",
      " 0.00965626 0.00972508 0.00973768 0.00974278 0.00975208 0.00979482\n",
      " 0.00981206 0.00984975 0.00987013 0.00987127 0.00991423 0.00991455\n",
      " 0.00991903 0.00992269 0.00996757 0.00999225 0.01001157 0.01002502\n",
      " 0.01003355 0.01005637 0.01008013 0.01008447 0.01010393 0.01010786\n",
      " 0.01017081 0.01020429 0.01020476 0.01022162 0.01023085 0.01029231\n",
      " 0.01031049 0.01032167 0.01033939 0.01034157 0.01042122 0.01045572\n",
      " 0.01047345 0.01051502 0.01054225 0.01054483 0.01056026 0.01060163\n",
      " 0.01062264 0.01064409 0.01071206 0.0107193  0.01074187 0.01075886\n",
      " 0.01076067 0.0107618  0.01076828 0.01077021 0.01077144 0.01077659\n",
      " 0.0107828  0.01083502 0.01084563 0.01084985 0.01087043 0.01087743\n",
      " 0.01096537 0.01098594 0.01099676 0.01103716 0.011047   0.01106771\n",
      " 0.01107077 0.01108302 0.01112945 0.01115751 0.01119989 0.01120429\n",
      " 0.01120805 0.0112237  0.01126001 0.01127166 0.01128064 0.0113019\n",
      " 0.01136889 0.01139769 0.01139869 0.01140724 0.01141071 0.01142649\n",
      " 0.011435   0.01143978 0.01144553 0.01147951 0.01151664 0.01153345\n",
      " 0.01154742 0.01156073 0.01157012 0.01157627 0.01160262 0.0116241\n",
      " 0.01163327 0.01163474 0.01167512 0.01167522 0.01167985 0.01171999\n",
      " 0.01172862 0.01173779 0.01176818 0.01176841 0.01182964 0.01183071\n",
      " 0.01186034 0.01187762 0.01190619 0.01191614 0.01194973 0.01195052\n",
      " 0.01197659 0.01198181 0.01198393 0.01200755 0.01200975 0.01201591\n",
      " 0.0120324  0.01205059 0.0120561  0.01205744 0.01206138 0.01213036\n",
      " 0.01213454]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: []\n",
      "Total Number of Eagles the model believes are not eagles 16\n",
      "Total Number of images the model believes are not eagles 592\n",
      "Change ratio: 0.0360\n",
      "Iteration 4...\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step\n",
      "Adjusted 178 indices, avg confidence: 0.0057\n",
      "Changed Indices: [1305, 1307, 1315, 1320, 1341, 1369, 1373, 1376, 1379, 1383, 1407, 1422, 1424, 1435, 1599, 1607, 1621, 1640, 1672, 1702, 1753, 1777, 1780, 1781, 1809, 1825, 1836, 1861, 1875, 1881, 1895, 1903, 1907, 1934, 1953, 1970, 1993, 2018, 2031, 2074, 2094, 2106, 2119, 2131, 2147, 2190, 2210, 2296, 2328, 2359, 2389, 2390, 2401, 2408, 2409, 2426, 2458, 2488, 2496, 2508, 2550, 2554, 2577, 2580, 2592, 2600, 2612, 2651, 2668, 2682, 2699, 2733, 2759, 2774, 2786, 2791, 2792, 2799, 2812, 2842, 2868, 2877, 2885, 2939, 2953, 2982, 2993, 3002, 3018, 3068, 3075, 3076, 3080, 3101, 3115, 3119, 3196, 3221, 3225, 3228, 3306, 3354, 3386, 3391, 3406, 3451, 3475, 3487, 3534, 3549, 3562, 3592, 3608, 3613, 3617, 3625, 3653, 3674, 3786, 3829, 3830, 3846, 3870, 3917, 3979, 4028, 4034, 4045, 4048, 4080, 4108, 4112, 4137, 4150, 4160, 4164, 4213, 4294, 4330, 4359, 4410, 4416, 4422, 4437, 4444, 4475, 4502, 4505, 4509, 4535, 4536, 4577, 4592, 4593, 4607, 4615, 4638, 4681, 4688, 4692, 4778, 4781, 4782, 4853, 4854, 4940, 5000, 5017, 5019, 5079, 5106, 5112, 5116, 5128, 5137, 5147, 5158, 5165]\n",
      "Predictions for Changed Indices: [0.0040756  0.00422464 0.00438622 0.00443879 0.0045471  0.0045851\n",
      " 0.00483971 0.00491425 0.0049293  0.00494421 0.00494744 0.00501194\n",
      " 0.00503718 0.00505129 0.00507903 0.00509007 0.00509837 0.00511429\n",
      " 0.00513347 0.00515427 0.00516316 0.00518518 0.00518747 0.00518866\n",
      " 0.0051931  0.00519852 0.00522634 0.00522659 0.00524669 0.00524727\n",
      " 0.005258   0.00527829 0.00528049 0.0052929  0.00529293 0.00529486\n",
      " 0.00530399 0.00535095 0.00535189 0.00535317 0.00536852 0.00537011\n",
      " 0.00538022 0.00538757 0.00539421 0.00539597 0.00541263 0.00541327\n",
      " 0.00542988 0.00543163 0.00546349 0.00546734 0.00547474 0.00549271\n",
      " 0.0054942  0.00551877 0.00552595 0.00553873 0.00554766 0.00555643\n",
      " 0.00556515 0.00558373 0.00561811 0.00562188 0.00563131 0.00564647\n",
      " 0.00565355 0.00567856 0.00569724 0.00569955 0.00570526 0.00571564\n",
      " 0.00573228 0.0057353  0.00574443 0.00575131 0.00575655 0.0057618\n",
      " 0.00577283 0.00578045 0.00579201 0.00580152 0.0058031  0.0058157\n",
      " 0.00581937 0.0058343  0.0058353  0.00585712 0.00586461 0.00587121\n",
      " 0.00587721 0.0058773  0.00588762 0.00588972 0.00589554 0.00590055\n",
      " 0.00590665 0.00590906 0.00591313 0.00591567 0.00591771 0.00591777\n",
      " 0.00593219 0.00594148 0.0059425  0.00595981 0.0059622  0.00596871\n",
      " 0.00598212 0.00598605 0.0059912  0.00599554 0.0060072  0.00600734\n",
      " 0.00601087 0.00601566 0.00602039 0.00602445 0.00602931 0.00603845\n",
      " 0.00605236 0.00606556 0.00606572 0.00606575 0.00607413 0.0060793\n",
      " 0.0060813  0.00608989 0.00609805 0.0060999  0.00610333 0.00610679\n",
      " 0.0061107  0.00611878 0.00612569 0.00613152 0.00613391 0.00613851\n",
      " 0.00614323 0.00614642 0.00614643 0.00615258 0.00615486 0.00615945\n",
      " 0.00617905 0.00618    0.00619096 0.0061946  0.00620078 0.00620979\n",
      " 0.00621756 0.0062184  0.00621903 0.00621903 0.00623353 0.00623527\n",
      " 0.00624114 0.00624754 0.006248   0.0062503  0.00625286 0.00625322\n",
      " 0.00626548 0.00627362 0.00627947 0.00628198 0.00628482 0.0062925\n",
      " 0.00629276 0.00630163 0.0063096  0.00631115 0.00631934 0.00632199\n",
      " 0.00632316 0.00633549 0.00634038 0.00634355]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: []\n",
      "Total Number of Eagles the model believes are not eagles 16\n",
      "Total Number of images the model believes are not eagles 770\n",
      "Change ratio: 0.0342\n",
      "Iteration 5...\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step\n",
      "Adjusted 169 indices, avg confidence: 0.0037\n",
      "Changed Indices: [1317, 1327, 1416, 1440, 1456, 1535, 1572, 1594, 1618, 1631, 1643, 1670, 1688, 1745, 1786, 1891, 1911, 1963, 1965, 1979, 1983, 2038, 2044, 2046, 2061, 2076, 2078, 2083, 2086, 2103, 2114, 2144, 2198, 2219, 2249, 2280, 2299, 2309, 2317, 2327, 2332, 2357, 2372, 2397, 2505, 2509, 2542, 2551, 2563, 2630, 2655, 2660, 2680, 2697, 2734, 2823, 2835, 2836, 2898, 2912, 2914, 2923, 2983, 2988, 2991, 3011, 3028, 3039, 3049, 3062, 3063, 3086, 3087, 3117, 3209, 3213, 3261, 3268, 3307, 3333, 3336, 3353, 3402, 3413, 3420, 3438, 3481, 3510, 3523, 3530, 3537, 3587, 3684, 3691, 3707, 3762, 3783, 3800, 3804, 3853, 3856, 3864, 3867, 3876, 3921, 3966, 4026, 4036, 4094, 4097, 4105, 4122, 4188, 4193, 4196, 4209, 4246, 4261, 4263, 4268, 4283, 4319, 4320, 4351, 4355, 4374, 4390, 4411, 4417, 4440, 4446, 4450, 4455, 4462, 4512, 4518, 4544, 4603, 4608, 4613, 4632, 4661, 4743, 4816, 4845, 4848, 4852, 4859, 4876, 4880, 4883, 4887, 4910, 4911, 4913, 4952, 4955, 4969, 5006, 5073, 5107, 5115, 5157, 5161, 5162, 5169, 5178, 5181, 5187]\n",
      "Predictions for Changed Indices: [0.0028448  0.00292208 0.002945   0.00297597 0.00304417 0.00309189\n",
      " 0.00317536 0.0032094  0.0032365  0.00323797 0.00324103 0.00324279\n",
      " 0.0032532  0.00326005 0.00326382 0.003264   0.00327387 0.00328788\n",
      " 0.00328962 0.00330182 0.00331787 0.00334182 0.00334376 0.0033464\n",
      " 0.0033546  0.00335546 0.00337136 0.00338715 0.0034016  0.00340623\n",
      " 0.0034157  0.00342633 0.00342805 0.00344117 0.00345369 0.00346832\n",
      " 0.00349117 0.00349497 0.0034989  0.00350686 0.0035183  0.00352092\n",
      " 0.00352546 0.00352803 0.00353636 0.00355221 0.00355648 0.00355757\n",
      " 0.00356149 0.00356801 0.00357154 0.00357461 0.00357545 0.00358483\n",
      " 0.00358745 0.00358889 0.0035897  0.0035913  0.00359356 0.00359648\n",
      " 0.00359928 0.00360044 0.00360973 0.00361622 0.0036193  0.00363222\n",
      " 0.00363466 0.00364483 0.00364632 0.00364687 0.00364833 0.00365004\n",
      " 0.00365162 0.0036572  0.00367128 0.00367222 0.00367849 0.00369043\n",
      " 0.00369984 0.00370179 0.00370607 0.0037102  0.00371324 0.00371361\n",
      " 0.00371472 0.00372182 0.0037229  0.00372593 0.00372743 0.00372874\n",
      " 0.00373107 0.00374441 0.0037457  0.00374748 0.00375774 0.00376754\n",
      " 0.00376945 0.00377325 0.00377396 0.00377581 0.00377718 0.00378425\n",
      " 0.00380096 0.00381004 0.00381701 0.00381839 0.00382109 0.0038301\n",
      " 0.0038316  0.0038318  0.00383605 0.00383718 0.0038375  0.00383946\n",
      " 0.00384192 0.00384287 0.00385817 0.0038603  0.00387627 0.00387883\n",
      " 0.00388066 0.00389021 0.00389229 0.00389923 0.00390315 0.00390522\n",
      " 0.00390959 0.00391646 0.00393284 0.00393355 0.00393906 0.00394085\n",
      " 0.00394686 0.00394738 0.00395003 0.00395958 0.00396379 0.00397048\n",
      " 0.00397891 0.00398009 0.00398078 0.00398337 0.00398502 0.00399188\n",
      " 0.00399195 0.0039968  0.00400885 0.00400952 0.00401683 0.00401929\n",
      " 0.00402307 0.00402712 0.00403038 0.00403247 0.00403674 0.0040402\n",
      " 0.00404062 0.00404548 0.0040559  0.00405947 0.00406091 0.00406481\n",
      " 0.00407137 0.00407201 0.00407796 0.00407965 0.00407986 0.00408057\n",
      " 0.00408131]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: []\n",
      "Total Number of Eagles the model believes are not eagles 16\n",
      "Total Number of images the model believes are not eagles 939\n",
      "Change ratio: 0.0325\n",
      "Iteration 6...\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step\n",
      "Adjusted 161 indices, avg confidence: 0.0036\n",
      "Changed Indices: [123, 810, 1201, 1339, 1346, 1348, 1377, 1380, 1412, 1415, 1443, 1512, 1534, 1536, 1558, 1593, 1615, 1619, 1637, 1657, 1659, 1660, 1679, 1683, 1705, 1757, 1785, 1828, 1831, 1892, 1913, 1920, 1928, 1950, 1954, 1958, 1981, 2006, 2048, 2084, 2138, 2139, 2146, 2166, 2203, 2226, 2247, 2262, 2297, 2311, 2342, 2387, 2471, 2485, 2527, 2545, 2549, 2666, 2707, 2762, 2787, 2802, 2830, 2845, 2867, 2884, 2894, 2992, 3008, 3042, 3044, 3058, 3093, 3122, 3129, 3149, 3159, 3183, 3186, 3204, 3207, 3290, 3310, 3347, 3374, 3390, 3409, 3410, 3446, 3472, 3486, 3509, 3527, 3535, 3544, 3577, 3591, 3600, 3652, 3675, 3731, 3739, 3754, 3769, 3807, 3818, 3825, 3863, 3872, 3880, 3950, 4051, 4053, 4062, 4081, 4084, 4107, 4159, 4199, 4200, 4205, 4275, 4356, 4399, 4419, 4443, 4458, 4465, 4477, 4487, 4491, 4501, 4519, 4531, 4539, 4566, 4655, 4662, 4668, 4672, 4677, 4694, 4701, 4704, 4709, 4715, 4763, 4768, 4867, 4874, 4893, 4900, 4964, 4978, 5014, 5057, 5077, 5134, 5135, 5139, 5142]\n",
      "Predictions for Changed Indices: [0.00313637 0.00316291 0.00323593 0.00325064 0.0032957  0.00331603\n",
      " 0.00333772 0.00334846 0.00335222 0.00335319 0.00336644 0.00337506\n",
      " 0.00337819 0.00340285 0.00340839 0.00342332 0.00342655 0.00342939\n",
      " 0.00344234 0.00344758 0.00344768 0.00345004 0.00345158 0.00345417\n",
      " 0.00347357 0.00347675 0.00347893 0.00348023 0.00349474 0.00350492\n",
      " 0.00350863 0.0035106  0.00351359 0.0035145  0.00351494 0.00351549\n",
      " 0.00351856 0.00352425 0.00352595 0.00353307 0.00353478 0.00353823\n",
      " 0.00353829 0.00354269 0.00354823 0.00355113 0.00356058 0.00356667\n",
      " 0.00356987 0.00357125 0.00358375 0.00358632 0.00359119 0.00359156\n",
      " 0.00359674 0.00359764 0.00359847 0.00360075 0.00360623 0.00360938\n",
      " 0.00361016 0.00361205 0.00361332 0.00361729 0.00361995 0.00362248\n",
      " 0.00362583 0.00362791 0.00362925 0.00363073 0.0036336  0.00363494\n",
      " 0.00363646 0.00363685 0.00363828 0.00364048 0.00364124 0.00364243\n",
      " 0.00364413 0.0036445  0.00364696 0.00365698 0.00366145 0.00366184\n",
      " 0.00366448 0.00367034 0.00367183 0.00367462 0.00367469 0.00367488\n",
      " 0.0036788  0.00368646 0.00368879 0.0036903  0.00369045 0.00369674\n",
      " 0.00369674 0.00369688 0.00369726 0.00370276 0.00371325 0.00371833\n",
      " 0.00372031 0.00372077 0.00372612 0.00372726 0.00372996 0.00373163\n",
      " 0.00373256 0.00373729 0.00374044 0.00374155 0.00374609 0.00374837\n",
      " 0.00375919 0.00376117 0.00376221 0.00376296 0.00376336 0.00376669\n",
      " 0.00376768 0.00377539 0.00377706 0.00378175 0.0037824  0.00378354\n",
      " 0.00378464 0.00378513 0.00378641 0.00378881 0.00379109 0.00379294\n",
      " 0.00379615 0.00381009 0.00381198 0.0038127  0.00381905 0.00382013\n",
      " 0.00383101 0.0038318  0.00383244 0.00383331 0.00383701 0.00383727\n",
      " 0.00383731 0.0038392  0.00384061 0.0038454  0.00384617 0.00385045\n",
      " 0.00385125 0.00385381 0.00385544 0.00386082 0.00386706 0.00386994\n",
      " 0.0038719  0.00387216 0.00387222 0.00387444 0.00388047]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [123, 810, 1201]\n",
      "Total Number of Eagles the model believes are not eagles 19\n",
      "Total Number of images the model believes are not eagles 1100\n",
      "Change ratio: 0.0310\n",
      "Iteration 7...\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step\n",
      "Adjusted 153 indices, avg confidence: 0.0020\n",
      "Changed Indices: [169, 204, 859, 1303, 1308, 1347, 1437, 1448, 1480, 1488, 1507, 1518, 1526, 1547, 1575, 1578, 1585, 1608, 1610, 1649, 1669, 1681, 1724, 1773, 1776, 1822, 1857, 1864, 1871, 1939, 1946, 1973, 1975, 1990, 2030, 2055, 2093, 2129, 2135, 2136, 2200, 2250, 2289, 2308, 2313, 2375, 2422, 2438, 2523, 2539, 2557, 2568, 2578, 2584, 2618, 2629, 2674, 2694, 2695, 2706, 2727, 2753, 2806, 2813, 2871, 2926, 2933, 2947, 2956, 3009, 3017, 3021, 3031, 3070, 3083, 3120, 3124, 3143, 3156, 3165, 3180, 3198, 3224, 3236, 3264, 3267, 3270, 3272, 3281, 3294, 3340, 3357, 3427, 3441, 3448, 3490, 3521, 3626, 3627, 3644, 3705, 3770, 3866, 3881, 3892, 3902, 3933, 3964, 3965, 3973, 4017, 4040, 4115, 4128, 4145, 4146, 4147, 4155, 4217, 4232, 4262, 4276, 4287, 4297, 4305, 4376, 4396, 4403, 4499, 4503, 4508, 4530, 4558, 4627, 4643, 4702, 4735, 4767, 4795, 4797, 4799, 4803, 4870, 4891, 4926, 4931, 4939, 4960, 5002, 5070, 5082, 5090, 5170]\n",
      "Predictions for Changed Indices: [0.00178898 0.00179061 0.00181402 0.0018193  0.00182061 0.0018289\n",
      " 0.00182939 0.00185734 0.00186383 0.00187154 0.0018747  0.00188721\n",
      " 0.00189649 0.00190474 0.0019066  0.00190697 0.00191265 0.00191746\n",
      " 0.00192772 0.00193402 0.00194757 0.00195732 0.00196144 0.00196188\n",
      " 0.00196301 0.00196346 0.00196474 0.00196877 0.00197047 0.00197266\n",
      " 0.00197368 0.00197481 0.00197838 0.00197982 0.00198    0.00198281\n",
      " 0.00198469 0.00198647 0.00198948 0.001992   0.00199629 0.00199762\n",
      " 0.00199989 0.00200146 0.00200175 0.00200534 0.00200537 0.00200878\n",
      " 0.00200907 0.00201214 0.00201275 0.0020141  0.00201598 0.00201709\n",
      " 0.00201974 0.00202212 0.00202279 0.00202415 0.00202429 0.00202601\n",
      " 0.00202721 0.00202726 0.00202799 0.00203157 0.00203375 0.00203674\n",
      " 0.00203828 0.00203896 0.0020416  0.00204176 0.00204328 0.00204375\n",
      " 0.00204399 0.00204874 0.0020495  0.00205055 0.00205113 0.002055\n",
      " 0.00205657 0.00205686 0.00205816 0.00206019 0.00206118 0.00206229\n",
      " 0.00206433 0.00206569 0.00206634 0.00206651 0.00206792 0.00207057\n",
      " 0.00207198 0.00207425 0.00207916 0.00208266 0.00208355 0.00208498\n",
      " 0.0020851  0.00208528 0.00209047 0.00209187 0.00209225 0.00209395\n",
      " 0.00209437 0.00209511 0.00209857 0.0020986  0.00209991 0.00210105\n",
      " 0.00210201 0.00210385 0.00210416 0.00210594 0.00210626 0.00210853\n",
      " 0.0021109  0.00211234 0.00211325 0.0021143  0.00211596 0.00211655\n",
      " 0.00211697 0.00211819 0.00211847 0.00212328 0.00212351 0.00212354\n",
      " 0.00212392 0.00212417 0.00212589 0.00212796 0.00212955 0.00212998\n",
      " 0.00213026 0.00213184 0.00213245 0.00213426 0.00213537 0.00213639\n",
      " 0.0021368  0.00214201 0.00214326 0.00214371 0.00214488 0.00214535\n",
      " 0.00214642 0.00214697 0.00214721 0.0021481  0.00214888 0.00214941\n",
      " 0.00214952 0.00214992 0.00215195]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [204, 859, 169]\n",
      "Total Number of Eagles the model believes are not eagles 22\n",
      "Total Number of images the model believes are not eagles 1253\n",
      "Change ratio: 0.0294\n",
      "Iteration 8...\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step\n",
      "Adjusted 145 indices, avg confidence: 0.0020\n",
      "Changed Indices: [95, 1311, 1337, 1370, 1420, 1445, 1450, 1470, 1491, 1541, 1582, 1600, 1620, 1711, 1719, 1758, 1775, 1783, 1793, 1817, 1839, 1900, 1905, 1931, 1949, 1955, 1962, 1985, 2015, 2027, 2089, 2109, 2150, 2176, 2237, 2242, 2260, 2270, 2277, 2291, 2318, 2322, 2358, 2360, 2373, 2391, 2412, 2441, 2457, 2481, 2497, 2502, 2522, 2531, 2537, 2653, 2665, 2714, 2775, 2782, 2793, 2927, 2938, 2957, 2980, 2998, 3036, 3048, 3053, 3056, 3081, 3089, 3092, 3116, 3118, 3128, 3138, 3174, 3181, 3194, 3208, 3220, 3246, 3248, 3255, 3269, 3273, 3279, 3295, 3297, 3299, 3324, 3365, 3460, 3467, 3488, 3528, 3585, 3601, 3603, 3716, 3745, 3792, 3815, 3831, 3843, 3907, 3910, 3948, 3998, 4003, 4012, 4018, 4029, 4068, 4079, 4116, 4156, 4186, 4208, 4250, 4253, 4291, 4325, 4335, 4398, 4466, 4486, 4495, 4496, 4557, 4563, 4564, 4741, 4761, 4771, 4796, 4806, 4839, 4974, 5113, 5119, 5132, 5133, 5148]\n",
      "Predictions for Changed Indices: [0.00181575 0.0018182  0.0018435  0.00184917 0.00185646 0.0018758\n",
      " 0.00187694 0.00188904 0.00189612 0.0018963  0.00189659 0.00189809\n",
      " 0.0019053  0.00190551 0.0019074  0.00190929 0.00193548 0.00193834\n",
      " 0.0019389  0.00193993 0.00194555 0.00194832 0.00195204 0.00195251\n",
      " 0.00195275 0.00195341 0.00195456 0.0019553  0.00195547 0.00195847\n",
      " 0.00195934 0.0019601  0.00196839 0.00196857 0.00197108 0.00197279\n",
      " 0.00197426 0.00197482 0.00197561 0.0019757  0.00197772 0.0019789\n",
      " 0.00197911 0.00198366 0.00198498 0.00198635 0.00198793 0.00198867\n",
      " 0.00199177 0.00199426 0.00199504 0.00199504 0.00199692 0.00199782\n",
      " 0.00200169 0.00200256 0.00200358 0.00200364 0.00200591 0.0020072\n",
      " 0.00200903 0.00200982 0.00201156 0.00201187 0.00201413 0.00201734\n",
      " 0.00201902 0.00201921 0.00202023 0.00202083 0.00202124 0.00202214\n",
      " 0.00202271 0.00202286 0.0020231  0.00202376 0.00202791 0.00202841\n",
      " 0.00203061 0.00203094 0.00203356 0.00203415 0.00203431 0.00203494\n",
      " 0.00203522 0.00203695 0.00203772 0.0020379  0.00203811 0.00203888\n",
      " 0.00204013 0.00204059 0.00204132 0.00204308 0.00204487 0.0020476\n",
      " 0.00204946 0.00204983 0.0020506  0.00205065 0.00205092 0.00205162\n",
      " 0.00205306 0.00205504 0.00205705 0.00205753 0.00205859 0.00206034\n",
      " 0.00206285 0.00206331 0.00206536 0.00206542 0.00206645 0.00206688\n",
      " 0.00206723 0.00206729 0.0020677  0.002068   0.00206834 0.00206865\n",
      " 0.00206972 0.00207363 0.00207436 0.00207457 0.00207469 0.00207572\n",
      " 0.00207714 0.00208335 0.00208392 0.00208498 0.00208558 0.00209101\n",
      " 0.00209126 0.00209306 0.00209408 0.00209439 0.00209824 0.00210158\n",
      " 0.00210414 0.00210438 0.00210559 0.0021072  0.00210751 0.00210786\n",
      " 0.00210848]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [95]\n",
      "Total Number of Eagles the model believes are not eagles 23\n",
      "Total Number of images the model believes are not eagles 1398\n",
      "Change ratio: 0.0279\n",
      "Iteration 9...\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step\n",
      "Adjusted 138 indices, avg confidence: 0.0015\n",
      "Changed Indices: [877, 1301, 1363, 1426, 1475, 1544, 1661, 1671, 1678, 1750, 1814, 1837, 1879, 1887, 1925, 1929, 1930, 1964, 1976, 1980, 1988, 2001, 2004, 2014, 2022, 2036, 2050, 2056, 2110, 2127, 2132, 2156, 2181, 2199, 2202, 2207, 2213, 2222, 2241, 2254, 2355, 2368, 2370, 2380, 2384, 2400, 2402, 2449, 2462, 2483, 2486, 2555, 2597, 2621, 2623, 2648, 2662, 2721, 2748, 2757, 2828, 2840, 2879, 2919, 2920, 2952, 2954, 3052, 3125, 3251, 3274, 3283, 3302, 3305, 3311, 3314, 3337, 3398, 3426, 3491, 3511, 3548, 3556, 3605, 3643, 3664, 3678, 3679, 3683, 3835, 3836, 3839, 3840, 3845, 3874, 3888, 3899, 3914, 4000, 4020, 4038, 4082, 4088, 4096, 4106, 4117, 4139, 4151, 4157, 4158, 4245, 4259, 4298, 4329, 4365, 4379, 4405, 4413, 4460, 4479, 4538, 4556, 4569, 4574, 4594, 4612, 4641, 4663, 4680, 4798, 4824, 4903, 4915, 5027, 5114, 5167, 5188, 5190]\n",
      "Predictions for Changed Indices: [0.00126841 0.00138839 0.00139251 0.00139697 0.00139776 0.00141265\n",
      " 0.001413   0.00141995 0.00142051 0.00142423 0.00143248 0.00143385\n",
      " 0.00143612 0.00143894 0.0014445  0.00144591 0.00144681 0.00144726\n",
      " 0.00144916 0.00145092 0.00145386 0.00145462 0.00145504 0.00145667\n",
      " 0.00145946 0.0014607  0.00146744 0.00146786 0.00146799 0.00146892\n",
      " 0.00146897 0.00147077 0.00147152 0.00147289 0.00147323 0.00147349\n",
      " 0.00147752 0.00148154 0.00148301 0.00148608 0.00148794 0.00148804\n",
      " 0.00148881 0.00149398 0.00149443 0.00149554 0.00149622 0.00149822\n",
      " 0.00150101 0.00150135 0.00150212 0.00150256 0.00150287 0.00150352\n",
      " 0.00150422 0.00150449 0.00150532 0.00150556 0.0015062  0.00150641\n",
      " 0.0015079  0.00150815 0.00150829 0.0015108  0.0015112  0.00151204\n",
      " 0.00151216 0.0015123  0.00151399 0.00151534 0.00151541 0.00151786\n",
      " 0.00151818 0.00151834 0.00152058 0.00152089 0.00152129 0.00152181\n",
      " 0.00152304 0.00152602 0.00152614 0.00152687 0.00152696 0.00152793\n",
      " 0.00152811 0.00152846 0.00153065 0.00153173 0.00153211 0.00153407\n",
      " 0.0015341  0.00153447 0.00153633 0.00153653 0.00153683 0.00153772\n",
      " 0.00153869 0.00153922 0.00153968 0.00154001 0.00154003 0.00154132\n",
      " 0.00154146 0.00154149 0.0015432  0.00154331 0.00154486 0.0015449\n",
      " 0.00154604 0.00154617 0.00154648 0.00154666 0.00154668 0.00154706\n",
      " 0.00154802 0.00154803 0.00154921 0.00155046 0.00155064 0.0015519\n",
      " 0.00155191 0.00155203 0.00155346 0.00155374 0.00155521 0.00155542\n",
      " 0.00155624 0.00155664 0.00155687 0.00155899 0.00155953 0.00155984\n",
      " 0.00155997 0.00156073 0.00156073 0.00156125 0.00156245 0.00156271]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [877]\n",
      "Total Number of Eagles the model believes are not eagles 24\n",
      "Total Number of images the model believes are not eagles 1536\n",
      "Change ratio: 0.0265\n",
      "Iteration 10...\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step\n",
      "Adjusted 131 indices, avg confidence: 0.0015\n",
      "Changed Indices: [106, 160, 298, 811, 947, 1083, 1360, 1421, 1429, 1431, 1496, 1500, 1565, 1579, 1590, 1606, 1699, 1703, 1734, 1748, 1832, 1841, 1869, 1893, 1961, 2032, 2034, 2039, 2041, 2068, 2071, 2102, 2105, 2112, 2142, 2188, 2205, 2215, 2218, 2232, 2266, 2279, 2290, 2316, 2320, 2335, 2378, 2411, 2535, 2559, 2589, 2625, 2628, 2669, 2686, 2691, 2708, 2769, 2803, 2882, 3013, 3043, 3072, 3074, 3077, 3105, 3141, 3257, 3394, 3404, 3442, 3447, 3452, 3454, 3464, 3496, 3500, 3506, 3513, 3606, 3621, 3645, 3669, 3671, 3685, 3686, 3709, 3715, 3756, 3765, 3781, 3869, 3903, 3916, 3930, 3949, 3959, 3972, 4063, 4078, 4176, 4285, 4318, 4338, 4366, 4386, 4401, 4452, 4467, 4470, 4473, 4542, 4547, 4580, 4583, 4591, 4604, 4691, 4732, 4777, 4811, 4836, 4838, 4894, 5039, 5042, 5052, 5124, 5164, 5186, 5189]\n",
      "Predictions for Changed Indices: [0.00134232 0.001348   0.00136021 0.00136597 0.00136865 0.00137384\n",
      " 0.00137697 0.00138143 0.00138613 0.00138716 0.00139284 0.00139624\n",
      " 0.00139937 0.00140245 0.00140445 0.00140608 0.00140741 0.00140775\n",
      " 0.00140906 0.00140952 0.00141436 0.00141473 0.00141733 0.0014201\n",
      " 0.00142074 0.00142141 0.00142444 0.00142484 0.00142569 0.0014266\n",
      " 0.00142723 0.00142915 0.00143235 0.00143433 0.00143443 0.0014354\n",
      " 0.00143705 0.00143787 0.00143957 0.00144084 0.00144089 0.00144221\n",
      " 0.00144227 0.00144259 0.00144292 0.00144325 0.00144354 0.00144367\n",
      " 0.00144372 0.00144458 0.00144492 0.00144509 0.00144653 0.00144902\n",
      " 0.00145105 0.00145157 0.00145231 0.00145317 0.0014541  0.00145431\n",
      " 0.00145608 0.00145673 0.00145728 0.0014585  0.00146089 0.00146335\n",
      " 0.00146525 0.00146554 0.00146778 0.00146791 0.0014683  0.00146871\n",
      " 0.00147069 0.00147158 0.0014718  0.0014724  0.00147319 0.00147666\n",
      " 0.00147674 0.00147837 0.00147917 0.0014819  0.00148428 0.00148455\n",
      " 0.00148516 0.00148521 0.00148563 0.0014857  0.00148597 0.00148605\n",
      " 0.00148654 0.00148965 0.00148967 0.00149041 0.00149107 0.00149177\n",
      " 0.00149297 0.00149363 0.00149457 0.00149707 0.00149748 0.00149832\n",
      " 0.00150145 0.0015032  0.00150402 0.0015053  0.00150775 0.0015089\n",
      " 0.00150901 0.00150982 0.00150982 0.00151027 0.00151048 0.00151076\n",
      " 0.00151187 0.00151355 0.00151407 0.00151424 0.00151474 0.00151477\n",
      " 0.00151482 0.00151548 0.00151647 0.00151752 0.00151761 0.00151781\n",
      " 0.00151899 0.00152024 0.00152042 0.00152172 0.00152232]\n",
      "FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES: [160, 106, 298, 947, 1083, 811]\n",
      "Total Number of Eagles the model believes are not eagles 30\n",
      "Total Number of images the model believes are not eagles 1667\n",
      "Change ratio: 0.0252\n"
     ]
    }
   ],
   "source": [
    "n_percentage = 5  # % of least confident eagle predictions to adjust\n",
    "max_iterations = 10 \n",
    "convergence_tolerance = 0  # stop if change ratio is this\n",
    "high_conf_threshold = 0.8  # threshold for confident eagle flips\n",
    "confidence_init = confidence_init = np.array([.35 if x<.5 else 1 for x in label_train]).reshape(5200,1)\n",
    "\n",
    "num_incorrectly_modified = 0\n",
    "num_modified = 0\n",
    "for iteration in range(max_iterations):\n",
    "    print(f\"Iteration {iteration + 1}...\")\n",
    "\n",
    "    # step 1: train model\n",
    "    history = model.fit(\n",
    "        [images_train, confidence_init],\n",
    "        label_train,\n",
    "        batch_size=100, # ok this might seem crazy but im wondering if w batch=32 it wasn't encountering enough wrong labels \n",
    "        epochs=1,\n",
    "        # validation_data=([images_val, np.ones((len(images_val), 1))], label_val), # this was a line from chat, replaced w ours instead below\n",
    "        validation_data=([images_val, label_val.reshape(-1,1)], label_val),\n",
    "        callbacks=callbacks,\n",
    "        shuffle = True,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # step 2: predict probabilities\n",
    "    preds = model.predict([images_train, confidence_init]).flatten()\n",
    "\n",
    "    # step 3: identify least confident eagle predictions\n",
    "    low_confidence_indices = np.where((label_train == 0) & (preds < 0.5) & (preds != 0))[0] # grabbing indices where label_train is 0 (noneagle), focusing in on the misclassified\n",
    "    filtered_indices = low_confidence_indices[confidence_init.flatten()[low_confidence_indices]!= 0]\n",
    "    sorted_indices = filtered_indices[np.argsort(preds[filtered_indices])] # sorts the preds low to high\n",
    "    to_adjust = sorted_indices[:int(len(sorted_indices) * (n_percentage / 100))] # only grabbing 5% rn of the bottom\n",
    "    \n",
    "\n",
    "    # step 4: update confidence for least confident predictions\n",
    "    if len(to_adjust) > 0:\n",
    "        confidence_init[to_adjust] = 0  # reduce confidence to 0 for the indices we picked by %\n",
    "        avg_confidence = np.mean(preds[to_adjust])\n",
    "        print(f\"Adjusted {len(to_adjust)} indices, avg confidence: {avg_confidence:.4f}\")\n",
    "    else:\n",
    "        print(\"No indices to adjust in this iteration.\")\n",
    "        \n",
    "    print(f\"Changed Indices: {sorted(to_adjust)}\")\n",
    "    print(f\"Predictions for Changed Indices: {preds[to_adjust]}\")\n",
    "    wrongly_switched = [x for x in changed_indices if x in to_adjust]\n",
    "    print(\"FOR CHECKING -- THE EAGLES WHICH THE MODEL GOT EVEN MORE CONFIDENT WASN'T EAGLES:\", wrongly_switched)\n",
    "    num_incorrectly_modified += len(wrongly_switched)\n",
    "    num_modified += len(to_adjust)\n",
    "    print(\"Total Number of Eagles the model believes are not eagles\", num_incorrectly_modified)\n",
    "    print(\"Total Number of images the model believes are not eagles\", num_modified)\n",
    "\n",
    "\n",
    "    # step 5: check for convergence\n",
    "    if len(to_adjust) > 0:\n",
    "            change_ratio = len(to_adjust) / len(label_train)  \n",
    "    else:\n",
    "            change_ratio=0\n",
    "    print(f\"Change ratio: {change_ratio:.4f}\")\n",
    "\n",
    "    if change_ratio < convergence_tolerance:\n",
    "        print(\"Convergence reached.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1d5f8a43-623a-4e17-9786-85b18742f7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many actual eagles in top 5% is: 60\n",
      "How many total are in the top 5% is: 208\n"
     ]
    }
   ],
   "source": [
    "def top_n_percent_indices(predictions, n_percent):\n",
    "\n",
    "    target_indices = np.where(label_train == 0)[0]\n",
    "    filtered_preds = predictions[target_indices]\n",
    "    \n",
    "    # calculate the number of top elements to select\n",
    "    num_top_elements = int(np.ceil(len(filtered_preds) * n_percent / 100))\n",
    "    \n",
    "    # get the indices of the sorted values (descending order)\n",
    "    sorted_indices = np.argsort(filtered_preds)[::-1]\n",
    "    \n",
    "    # select the top n_percent indices\n",
    "    top_indices = sorted_indices[:num_top_elements]\n",
    "\n",
    "    top_original_indices = target_indices[top_indices]\n",
    "    \n",
    "    return top_original_indices\n",
    "n_percent = 5\n",
    "high_confidence_indices = top_n_percent_indices(preds,n_percent)\n",
    "high_confidence_indices\n",
    "actually_eagles = [x for x in high_confidence_indices if x in changed_indices]\n",
    "# print(\"The least confident not-eagles that are actually eagles:\", actually_eagles)\n",
    "print(f\"How many actual eagles in top {n_percent}% is: {len(actually_eagles)}\")\n",
    "print(f\"How many total are in the top {n_percent}% is: {len(high_confidence_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bd6f29-a6fa-4902-bebe-63b1fecdd0a4",
   "metadata": {},
   "source": [
    "# Visualizing the best parameter's results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d05968-84e3-4216-89ad-8d1e702ce961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reminder: \n",
    "# true counts of eagle:not eagle = 1300:3900\n",
    "# adjusted counts post misclassification: 1040:4160\n",
    "\n",
    "# want to see how things change location from iter to iteration and how counts imrpove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9216df0-2f5a-463b-af10-698e0a379ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a273c4ed-0c87-46f6-8f6c-d763fa64e0c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b988f3c-a1b6-4798-8f4a-bbc6878d2172",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### just to test distributions / debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38406e69-705d-44df-837e-d79871e2f2fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f0cea15-01b9-416e-9a34-e2a4e47b57cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly labeled eagles: 1040/1040\n",
      "Correctly labeled noneagles: 4160/4160\n"
     ]
    }
   ],
   "source": [
    "# Compare relabeled classes to original ground truth\n",
    "original_labels = new_labels  \n",
    "correct_eagles = np.sum((label_train == 1) & (original_labels == 1))  # True eagles as eagles\n",
    "correct_noneagles = np.sum((label_train == 0) & (original_labels == 0))  # True noneagles as noneagles\n",
    "\n",
    "print(f\"Correctly labeled eagles: {correct_eagles}/{np.sum(original_labels == 1)}\")\n",
    "print(f\"Correctly labeled noneagles: {correct_noneagles}/{np.sum(original_labels == 0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8b2ea307-66f6-4820-af0f-e7387923936e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean confidence for eagles: 0.0297\n",
      "Mean confidence for noneagles: 0.0079\n"
     ]
    }
   ],
   "source": [
    "eagle_confidence = preds[label_train == 1]\n",
    "noneagle_confidence = preds[label_train == 0]\n",
    "\n",
    "print(f\"Mean confidence for eagles: {eagle_confidence.mean():.4f}\")\n",
    "print(f\"Mean confidence for noneagles: {noneagle_confidence.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "091e90f8-5dd7-45d0-904b-b4f1e03d1997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-statistic: 285.7584, P-value: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "t_stat, p_val = ttest_ind(eagle_confidence, noneagle_confidence, equal_var=False)\n",
    "print(f\"T-statistic: {t_stat:.4f}, P-value: {p_val:.4e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "92a903ff-2813-4184-bbc5-479a20d2dbe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(eagle_confidence, bins=20, alpha=0.7, label='Eagles (1)')\n",
    "plt.hist(noneagle_confidence, bins=20, alpha=0.7, label='Noneagles (0)')\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Confidence Distribution')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8fe5eac0-43f7-49ec-a853-c79292cd062d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Label=0, Confidence=0.4747\n",
      "Sample 1: Label=1, Confidence=0.5125\n",
      "Sample 2: Label=1, Confidence=0.5125\n",
      "Sample 3: Label=1, Confidence=0.5125\n",
      "Sample 4: Label=1, Confidence=0.5125\n",
      "Sample 5: Label=1, Confidence=0.5125\n",
      "Sample 6: Label=1, Confidence=0.5125\n",
      "Sample 7: Label=0, Confidence=0.4747\n",
      "Sample 8: Label=1, Confidence=0.5125\n",
      "Sample 9: Label=1, Confidence=0.5125\n"
     ]
    }
   ],
   "source": [
    "# Get indices of ambiguous samples\n",
    "ambiguous_indices = np.where((preds > 0.4) & (preds < 0.6))[0]\n",
    "\n",
    "# Inspect some ambiguous samples (requires image visualization logic)\n",
    "for idx in ambiguous_indices[:10]:  # First 10 ambiguous cases\n",
    "    print(f\"Sample {idx}: Label={label_train[idx]}, Confidence={preds[idx]:.4f}\")\n",
    "    # Add your image visualization code here if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372577b8-e20f-4931-9411-7cdd8356c626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d91cb0-eff3-4deb-8493-b6a45b535c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
